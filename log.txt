13.04 15:30-16:30
Teraz cel to zrobić wstępny plan badania. Zacząłbym od wyszukania customer segmentation po słowach kluczowych w scholarze i sprawdzenia ogólnej dostępnej teorii/metod itd. 

Traktować ten zbiór jako symulacja prawdziwego e-commerce i nie przejmować się specyficzną strukturą - udawać że to jest zwykły sklep internetowy.

Notatki z kotlera:
- Segmentacja geograficzna
- Segmentacja demograficzna (wiek, płeć, dochód itd.)
- Gender
- Income
-social class, lifestyle

Do brazylii jest sporo danych demograficznych spatial - sprawdzić czy są jakieś interesujące, na pewno płci, analfabetyzm, jakieś zbinowane dane o wieku. Chyba nie ma zarobków.

17.04 18:30 - 19:30
market  segmentation - Dividing  a market  into distinct groups of buyers with different needs,characteristics  or behaviour, who might require  separate produces or marketing mixes

STP 

Local marketing - targetowanie po geografii

ACORN   (A Classification Of Residential Neighbourhoods) - po population census, często wykorzystywane w segmentacji

Multistage segmentation - drzewko

Wymienione podejścia: conjoint analysis, Automatic Interaction Detection

Kryteria dobrej segmentacji:
Measurability
Accessibility

O klastrowaniu był jakiś syf


29.04
feature selection pomysły:
- Train test split, zrobienie klastrowania na train, predykcja te teście, sprawdzenie ile pokrywa się ze wcześniejszym sposobem
- jakaś miara dobroci klastrowania - silhouette na przykład. Jest niewrażliwa na ilość klastrów i też na ilość wymiarów
- po prostu PCA - ale to brzydkie, bo zostają wszystkie zmienne
- entropy based - tu: http://www.public.asu.edu/~huanliu/papers/pakdd00clu.pdf
Nie czaję tej metody jeszcze ale ma spoko wytłumaczenie teoretyczne dlaczego feature selection jest ważne

- jest złoto papier z tego roku


Znaleziska:
- silohouette jest średnie, bo ze zwiększaniem ilości zmiennych zwiększa się wariancja do złapania
- convex kmeans - dla każdego itema jest kilka setów features (prawie jak to co chcę). 
- Feature weighting - jest papier o gradient descent 
- fajnie wykorzystać k modes do categorical
- ogólne wrażenie że kmeans to syf z większą ilością zmiennych
- poszukać metod klastrowania które są bardziej elastyczne
- oparte na modelu ml - losowo przypisz labele do danych, puść svm, części źle zakwalifikowanych zmień labele, puść jeszcze raz


- interpretowalność w jakiś sposób
- dbscan brzmi dobrze, bo można zdefiniować minimalną ilość klientów
- z drugiej strony hierarchical brzmi dobrze bo jest trochę interpretowalne


01.05
Jest 95% one-timerów. Z prespekywy zarabiania można powiedzież że to 1 segment. Pomysł:
Ograniczyć stałych klientów do 1 zamówienia i zrobić predykcję co sprawia że decydują się na ponowne zakupy. A segmentację taką z algorytmu zrobić tylko na tych którzy już zrobili więcej zakupów.

Zrobiłem wstępne klastrowanie. Obserwacje:
- po wyskalowaniu danych wychodzi o wiele gorzej
- Część features jest taka sama w każdym klastrze
- silhouette spada po dodaniu całkowicie nieistotnej zmiennej - random
- ale rośnie (mocno) jeżeli doda się nieistotną zmienną niewyskalowaną - pewnie szuka błędnego klastrowania

Co przetestować dalej:
- Zrobić klastrowanie na 2 zmiennych i porysować
- Sprawdzić k-medians (wyeliminować problem zmiennych = 1)
- Sprawdzić metody z drzewem - wydaje się że zmienne binarne powinny ładnie siadać
- Zaimplementować feature weights - znormalizować wagi do 1 i zrobić grid search 
- Sprawdzić paczkę do feature importance 
- Zrobić stability analysis - puścić na kilku podzbiorach albo bootstrap
- Potestować dbscan - jest trochę pewniejszy pod względem łapania klastrów niż kmeans 
- feature weight self-adjustment mechanism


03.05
Sprawdziłem klastrowanie na 2 zmiennych. Wychodzi ładna kulka - żadnych klastrów nie widać. Co z tym zrobić?
1. Zostawić na razie klastrowanie, skupić się na ocenie czynników które wpływają na różnice pomiędzy oosbami które kupiły więcej niż raz i one timerami
2. Olać zejście do customera - pójść w stronę rynków geograficznych

06.09 
Zaczynam znowu prace nad magisterką. Co jest do zrobienia przed wysłaniem?

Porządna EDA
Przetestować na podstawie RFM - wybrać dobre okna czasowe
Przetestować ant colony 
Jeżeli oba powyższe nie wyjdą to iść w stronę rynków geograficznych - przetestować basic



Jaki jest cel segmentacji klientów?
Dopasowanie asortymentu i reklam pod dany segment
Dopasowanie strategii pod dany segment

cel rfm?
wybranie klientów z dużym potencjałem do dalszego inwestowania w tą grupę

Znaleziska:
Lokalizacja klientów jest bardzo dobrze klastrowalna - silhouette > 0.7 dla 2 klastrów
RFM jest w ogóle nie klastrowalne - rozkład monetary value jest skośny i tyle, a rozkład czasu od ostatniego kupna jednostajny - żadnych więcej regularności

RFM nie powinno być używane do klastrowania - to jest bez sensu. To jest zupełnie inny rodzaj rozdziału klientów na grupy. 

08.09
Sprawdziłem jeszcze klastrowanie danych RFM po zlogarytmowaniu zmiennych - i w końcu osiągam dobre wartości silhouette. W związku z tym zostawiam na tą chwilę sprytniejsze algorytmy do klastrowania. 

Cel pracy to zwiększenie profitów z klientów. 2 sposoby:
- jest bardzo mało klientów kupujących więcej niż raz. Można zbadać tych, którzy kupują więcej, znaleźć ich charakterystyki po 1 zakupie, a następnie klastrować klientów już po 1 kupnie, żeby zogniskować działania marketingowe. 
- po prostu przeanalizować klientów którzy już są i zobaczyć ich charakterystyki - w celu lepszego zrozumienia klientów i dopasowania oferty/promocji pod najbardziej dochodowych klientów.

Co jest do zrobienia:
- opisanie podejścia RFM (rezultaty klastrowania, według którego papera)
- 


Segmentacja tych którzy kupili wielokrotnie, 

21.10
DONE:
Pobranie danych z SIBRA 
Spatial join tych danych z ZIP-codes 
Testy z PCA - wygląda obiecująco

#' Avaliable information:
#' - microregion code - to join with other datasets
#' - total_pop - population
#' - age_... - percentage of people in given age
#' - perc_rural, perc_urban - percentage of people living in urban and rural areas
#' - inc... - percentage of people having income at least ... times the minimum wage
#' - no_immigrants - percentage of immigrants

EDA of Market Basket
Research about clustering/dimension reduction of market basket data
couple of options possible, UMAP, MDS (it's bad because of the complexity), t-SNE, PCA on correlation matrix (SVD)


TODO:
Research o metrykach (categorical/mixed)
analiza koszyka
	- EDA produktów w koszyku ok 
	- research o Market Basket ok
	- Klastrowanie jak klastrowanie dokumentów
doklejenie geo danych 
	- pobranie ok
	- przekształcenie do postaci mikororegionu ok
	- wykminienie joina ok
	- analiza tych danych ok
	- join w jakiś sposób ok
	- wybranie sposobu na dołączenie danych - PCA? ile komponentów? I czy w ogóle? Czy rotated? Do dogadania jak będzie już benchmark

Analiza reviews
	- Kurs o NLP - analiza sentymentu, topic modeling
	- Odpalić chmurę - do deep learningu ok
	- Przetłumaczyć recenzje (google api?)
	


Na ten tydzień - basket analysis:
Cel - zredukowanie wymiarowości do kilku wymiarów
Metody do przetestowania:
k-modes - powinno być straightforward, ale potencjalnie może dać dużo klastrów (później one-hot encoding)
UMAP - w pythonie jest zaimplementowana odległość dla danych kategorycznych, w R nie ma
t-SNE - tak samo, w pythonie
PCA proste

Setting:
1. Oddzielnie 1 produkt i więcej
2. Różne wagi macierzy - wartość produktów w koszyku, 0/1



Pytania:
jakie wybrać metryki binarne?
klastrowanie danych z wieloma zerami 

Podsumowanie seminarium 23.10:
Olać customera, skupić się na spatial cechach - PCA, wizualizacje

TODO:
- EDA 
- sPCA

EDA założenia:
Pominąć wymiar customera - traktować każde zamówienie jako oddzielnego
Zagregować rzeczy do zip code 3-cyfrowego

Ustalenia:
- do danych geo nie da się wiele dokleić - tylko średnie zamówienie, liczba zamówień - trochę mało ciekawe
- nie ma sensu robić pca na 4 zmiennych ^^
- patterny są widoczne - ale w sumie nic to nie zmienia, prawdopodobnie odpowiadają zamożności klienta - jest więcej zamówień w bogatszych dzielnicach
- 

Podsumowanie modelowania 03.11
Podstawowy model na łatwo dostępnych danych:
    payment_value,
    review_score,
    if_second_order,
    geolocation_lat,
    geolocation_lng,
    no_items,
    sum_freight.
Do tego upsampling (1% jedynek), metoda GBM. Na podstawie pierwszego zamówienia stara się przewidzieć czy dojdzie do kolejnego. Daje AUC w granicach 0.6 na zbiorze testowym.

To wygląda na dobry trop. Mimo że performance nie jest jakiś super, to nawet nie chodzi o dobre przewidzenie, bo to ma być tylko input do strategii marketingowej. I chodzi o określenie top x% klientów. Pytanie czy wysyłać do top x, czy do takich w połowie rankingu? Pytanie czy ci z p=0.99 i tak kupią. Do tego trzeba by było się dowiedzieć jak działają klienci na reklamę - raczej nie do sprawdzenia. 
Strategię można przygotować na podstawie rezultatów klastrowania. 

11.11
Zrobiłem rolling predictions - bardzo słabo wypada dla tylko 1 miesiąca. Testuję jeszcze dodawanie kolejnego okresu, aby zasymulować comiesięczne dodawanie danych tak jak normalnie w firmie. GBM zaczyna mieć sens dopiero od pewnej liczby obserwacji. Kiedy uruchamiam dla całego datasetu to zdecydowanie lepiej wypada niż LR, ale kiedy datastet to tylko 1 miesiąc to overfituje i LR jest lepsze. Może ciekawą rzeczą w pracy byłoby switching models/sprawdzanie od którego momentu bardziej elastyczne modele zaczynają wygrywać nad basic podejściem

Poza tym zrobiłem resampling/bootstrap - spójne rezultaty dla całego datasetu, bardzo niespójne dla pojedynczych okresów - auc czasami nawet spadało poniżej 50%

Z papera o evolutionary algos in CRM sensownie pasuje do danych to co zacząłem robić, czyli churn prediction i segmentacja. 
Co do segmentacji to spoko wygląda "incremental ant colony clustering" - inna metoda niż kmeans, bardziej density-based. Poza tym umożliwia inkrementalne klastrowanie - czyli fajna rzecz na tych danych. Problem jest że nie ma implementacji w R a w pythonie jest biedna - na pewno nie jest zrobiona jako estymator w sklearn


13.11 Spotkanie ustalenia
- Pocisnąć klastrowanie tym mrówkowym żeby dostać kod w pythonie
- Po pierwszych wynikach zorbić konspektt





