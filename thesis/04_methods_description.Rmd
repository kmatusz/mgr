---
title: "Methods description"
bibliography: resources/bib.bibtex
output: html_document
# output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Processing parts of the dataset

TODO: zastanowić się nad diagramem gdzie będzie opisane jak były przetwarzane dane

### Order items

For each purchase of the customer there is avaliable information about the items that particular customer bought. All items in the shop are assigned to one of .. categories. There is a couple of challenges that should be adressed before adding this information to the dataset:

1. For each purchase, the customer can buy products from different categories
2. Most of the purchases (..%) contain only one item - this means that one-hot-encoding matrix would be extremely sparse
3. Some of the categories are underrepresented - there are ... categories for which only ... purchases were made. This can pose generalization problems.
4. The number of categories is very big, and because of that model training would be extremely slow

To mitigate the problem of too many categories, I have decided to bin the least popular ones as a distinct category "other". I have chosen 13 (??) most popular ones, which account for .. of all observations. Then, I have used one-hot-encoding^[Because of the fact that in one order there can be multiple product categories, it is not guaranteed that there will be one "1" value per each row as in classical one-hot-encoding method.] approach to create numeric representation. Category "other" was set as a base level. 

### Geographic data - long/lat

TODO: Znaleźć cytowanie o wkładaniu wyników klastrowania do modelu zamiast surowych koordynat

To be able to decode the coordinates differently, I have used a DBSCAN clustering technique (TODO: cytowanie). 

DBSCAN is particularly well-suited for geolocation point data. First, it is density-based, and because of that has a natural interpretation. When the point belongs to any cluster it means that the customer is living in a densely-populated area, while the points decoded by DBSCAN as noise are the customers living in more isolated places. Second, one doesn't have to set the number of clusters in advance. Third, it doesn't require the clusters to be spherical, which is not a condition that comes naturally in the geolocation settings. 

DBSCAN has 2 parameters to be decided before running the algorithm. These are the minimal number of points laying close to each other that are needed to constitute a cluster (*k*), and maximal distance, at which one considers the poits to lay close to each oter (*epsilon*). 
A typical rule-of-thumb for deciding k and epsilon parameters is to first set k, and then plot k-nearest-neighbors distances. Epsilon should be then decided based on *elbow point*, where the line is bending. However, when the features are geographical coordinates, epsilon is actually a physical distance between two locations. That is why based on expert knowledge from the company one can set what should be more reasonable criteria for constituting clusters. 

In my work I have decided somehow arbitrarly that minimal number of customers in the cluster is 100, and the maximum  distance between the customers in one cluster is 50 kilometers. For the location of Brazil on the geoide, this transfers roughly to epsilon=0.2. However, this kind of decision in a real company setting can (and should be) consulted with the company's domain experts in marketing. 

### Demographic data - sidra

I have processed the demographic part of the dataset using Principal Components Analysis. There are couple of reasons for that. First, this part of the dataset is relatively high dimensional (34 variables). At the same time, one would expect that the information can be somehow compressed, because lots of the variables represent very similar concept (for example there are 20 variables encoding age structure). Second, trainig Machine Learning models on a smaller, compressed dataset is more resource-efficient and at the same time was shown to improve the modeling results (TODO: cytowania z prac w których robili PCA). 

One decision regarding PCA transformation is whether to use a standard version, or the one with rotated loadings (TODO: cytowanie). The trade-off between these two methods is that rotated loadings allows for interpretation of the loadings, but is less optimal in a sense that the variance along each loading is not maximized. I have decided that a standard one would be more suitable in case of this study, because the explainability of the input variables to the model is not as important as correctly representing the features in lower-dimensional space and thus preserving as much valuable information as possible for the modeling phase.

### Text reviews (TODO)

TODO: Ogarnąć strukturę

Olist e-commerce store is operating only in Brazil. That is why most of the reviews are written in Portuguese. I have used Google Translate API to change the language of them to English. This is to facilitate not only understanding the reviews, but also the NLP tools avaliable for english language are more up-to-date and advanced.

3 algorithms for topic modeling that I have tested were:

- Latent Dirchlet Allocation - because it is a go-to standard for topic recognition
- Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture - as this method is an improvement over LDA, meant especially for short texts. This is the case from this study, as most of the reviews are just couple of words long
- Attention-Based Aspect Extraction - this method is also meant for short texts, and at the same time it uses the most modern, state-of-the-art NLP techniques.

To improve the results in all 3 methods I have removed stopwords and punctuation from the text. Also, to limit the number of words in the vocabulary I have used lemmatization technique. This was done by WordNet lemmatizer. To further improve the results, I have used Part-of-Speech tagger, and passed the tags of words to the lemmatizer. This way the algorithm can change the form of the word on a more informed basis, and thus apply lemmatization to more words. 

Second step of the preprocessing was converting lemmatized reviews into vector format. In case of LDA, count-vectorizing approach was applied. the words which appeared in less than 0.1% of reviews were dropped from the dataset. In the case of Gibbs Sampling the same preprocessing is done internally by the function. 

In both of these cases after vectorization one should obtain a matrix with n rows and k columns, where n is number of observations in the original dataset, while k - size of the vocabulary. 

Very different preprocessing was required in the case of Attention-Based Aspect Extraction. The neural network requires simply lemmatized reviews in textual format as the output. Then, one of the layers of the network is meant to embed the currently preprocessed word. These embeddings are not learnt during the network training, they should be trained beforehand instead. The authors of the paper propose Word2vec technique for learning embeddings. Following their guidelines I have used this method, setting the dimensionality of the vector space to 200. I have also applied the word window of 10. 

After applying word2vec on this dataset, I have obtained the matrix with m rows and 200 columns, where m stands for number of words in the dataset, and 200 is the dimensionality of the vector space chosen as a hyperparameter. 
One should bear in mind that count-vectorization works on review level, while word2vec - on words level. This means that after applying word2vec model to 1 review, one would obtain the same number of outputs as the number of words in the review. This is why it is impossible to use word2vec to preprocess the dataset and then use LDA or Gibbs sampling without some way to convert couple of vectors into one. 

Unfortunately, the evaluation of topic extraction is a hard task. The only reasonable one is human inspection. That is why after running every model I have verified the obtained topic for coherency and distinctiveness.

TODO: Opis wyboru parametrów modeli z poniższej listy

- LDA
  - remove stopwords and punctuation
  - use wordnet lemmatizer [@wordnet] with POS tagging (NLTK)
  - use count-vectorize approach with removing words which appear in less than 0.1% reviews
  - use LDA with varying number of components 3, 5, 10, 15
  - manually inspect inferred topics
  
- Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture (movie group process)
  - same as in LDA but this one does not require vectorizing the dataset
  - 30 topics
  - Citing python gsdmm package documentation:
  
    "Alpha controls the probability that a student will join a table that is currently empty.
    When alpha is 0, no one will join an empty table.
    Beta controls the student's affinity for other students with similar interests. A low beta means
    that students desire to sit with students of similar interests. A high beta means they are less
    concerned with affinity and are more influenced by the popularity of a table"
  - checking every combination of alpha, beta chosen from the values: [0.01, 0.1, 0.5, 0.9]
  - manual inspection
  
- Attention-Based Aspect Extraction [@he2017aspect]
  - Based on the implementation by authors of the article 
  - remove stopwords and punctuation
  - use wordnet lemmatizer [@wordnet]
  - converting to vectorized form using Word2Vect model with final dimensionality of 200 and window of 10
  - Training attention model
  - manual assessment of reviews topics
  
For the analysis of textual reviews I have used python programming language. 

- python [@van1995python]
  - NLTK [@bird2009natural], spacy [@spacy] itd
  
## Modeling approach

### General 

I have used a simple train-test split of the dataset, with 70% of the observations belonging to the trainig dataset. On the training dataset, I have searched for optimal hyperparameters using 3-fold cross-validation on training dataset. I have defined search space simply as a grid of all possible combinations of the hyperaparameters. 

One important problem with this dataset is its very high target classes imbalance. Only 3% of the customers have decided to buy for the second time. To handle this issue I have used upsampling of the minority class on the training dataset to obtain equal class proportions. Also, the choice of an appropriate metric to optimize is very important in imbalanced dataset, as some metrics (like accuracy) are very biased in these cases. That is why I have decided to optimize Area-Under-Curve metric, as it weights the performance on the minority and majority classes equally.

I have used Logistic Regression and XGBoost models. The reason is that Logistic Regression is relatively simple and explainable, and was used in the task of churn modeling in previous studies (TODO: cytowania do tego paragrafu). On the other hand, XGBoost model was shown to be very well-suited for all kinds of modeling using tabular data. It can also learn non-linearities and interactions between the variables on its own, contrary to LR where such features should be introduced to the model manually. 

### Feature selection 

#### General

TODO: Introduction, opisać listy 1 zdaniem

Sets of features:

- basic information - value of the purchase, geolocation in raw format lat/lng, value of the package, number of items in the package, review score (6 variables)
- demographic features for the region from which the customer is - age structure, percentage of population in urban area, income structure (35 variables)
- demographic features transformed using PCA - (10 variables/components)
- indicator whether the customer is in an agglomeration area obtained from DBSCAN on location data (1 variable)
- product categories that the customer has bought in the purchase (15 dummy variables)
- main topic that the customer has mentioned in the review (15 dummy variables)

Models tested:

- basic features
- demographic + basic
- demographic with PCA + basic
- agglomeration + basic
- product categories + basic
- review topic + basic
- all variables - demographic features transformed with PCA

I have not run the model containing all variables with demographic features without PCA preprocessing. There are 2 reasons for that - one is that number of variables in this set is very big, what poses performance reasons - model training simply would take a very long time. Assuming that the PCA set of features would give better score can be also supported by the fact that the model with only included PCA demographic variables is performing better than the full set of variables.  

#### Boruta

To remove the human judgement about which predictors should be used, I have also used a Boruta algorithm for feature selection. It is widely popular among machine learning practitioners (??? TODO: znaleźć cytowania oryginalnej pracy). The algorithm belongs to category of wrapper feature selection algorithms, and Random Forest algorithm is usually used as an underlying algorithm. It works as follows. At first, all features from the original dataset are randomly permuted. In this way one obtains a dataset with close-to-zero predictive power. Then, the resulting features are added to the original dataset and Random Forest model is trained. 

Random Forest model have a built in feature importance measure, that is usually Mean Decrease Impurity. After running random forest model, for each original features MDI is compared against all MDI scores for shadow features. If for any original variable the score is less than the one from any of shadow features, the variable gets a "hit". 

Above procedure is repeated for preassigned number of iterations. Finally, important features that should make it to the final model are the ones that obtain less hits than preassigned n. 

After gaining knowledge about the variables that should make it to the model, I have trained XGBoost classifier using these features. The rest of the fitting procedure (cross-validation, up-sampling, hyper-parameters etc.) stayed the same as in the rest of the approaches.  

One should have in mind that Boruta algorithm is very time-consuming. The minimal number of runs recommended by the method authors is 100, and one run consists of fitting a Random Forest model to the whole dataset with doubled number of features (because of added shadow features). In the case of this analysis, model computation took about 12 hours on a medium-class modern laptop. Although other wrapper algorithms also require an iterative fitting the model, they usually start with fitting the model to one variable, in the next iteration to 2, and so on up to k features. On the other hand Boruta algorithm in each iteration fits the model to 2*k features (original and shadow features).



