---
title: "Methods description"
bibliography: resources/bib.bibtex
# output: html_document
output:
  bookdown::word_document2:
    reference_docx: resources/styles_template.docx
    number_sections: true
params:
  echo: False
---

```{r include=FALSE}
knitr::opts_chunk$set(echo=params$echo)
```

TODO: *W części 04 też przyda się flow chart co będzie robione, na jakich danych i na jakie pytania odpowie.*


## Introduction

3 Parts:

- Modeling approach
- Used variables and applied preprocessing
- Variables selection methoid

## Modeling methods

I have used a simple train-test split of the dataset, with 70% of the observations belonging to the training dataset. On the training dataset, I have searched for optimal hyperparameters using 3-fold cross-validation on training dataset. I have defined search space simply as a grid of all possible combinations of the hyperparameters. 

One important problem with this dataset is its very high target classes imbalance. Only 3% of the customers have decided to buy for the second time. To handle this issue I have used upsampling of the minority class on the training dataset to obtain equal class proportions. Also, the choice of an appropriate metric to optimize is very important in imbalanced dataset, as some metrics (like accuracy) are very biased in these cases. That is why I have decided to optimize Area-Under-Curve metric, as it weights the performance on the minority and majority classes equally.

I have used Logistic Regression and XGBoost models. The reason is that Logistic Regression is relatively simple and explainable, and was used in the task of churn modeling in previous studies [@nie2011credit; @dalvi2016analysis]. On the other hand, XGBoost model was shown to give superior performance in all kinds of modeling using tabular data, also in the context of churn prediction [@gregory2018predicting]. It can also learn non-linearities and interactions between the variables on its own, contrary to LR where such features should be introduced to the model manually. 


## Variables used

In this study I have separated 3 groups of variables analyzed:

- behavioural (first transaction) features
- location features
- perception features.

On diagram .. a summary of preprocessing applied to all the parts of the dataset is presented. All the tables on the left-hand-side are coming directly from Olist (4 tables) and SIDRA sources (1 table - demographic data). Purple table is the primary one, the features from this table were combined with all the remaining sets of variables. The final tables after preprocessing of each of the parts of the dataset are shown in gray. In the modeling phase, I have used simple join of the basic table, and the remaining ones, separately from each other (e.g. basic information + order items, basic information + DBSCAN cluster etc.).

![](resources/data_processing_diagram.png)


### Behavioural features

Behavioural predictors can be defined as the variables quantifying previous actions of the customer. In most of the cases this narrows down to the data about previous transactions and previous interactions with the company. Behavioural information about customer's interactions with the company was shown to be an important predictor in churn prediction (for overview see @schmittlein1994customer). 

The widest range of behavioural variables up in churn prediction setting up to date was used by @buckinx2005customer. Besides 7 variables meant for encompassing frequency and monetary value, they also included variables indicating total spending divided by categories of the products available in e-commerce shop. They found that all 3 categories of variables are statistically significant and bring improvement to model's predictions. In particular, they found that bigger customer spending leads to the customer's desire to keep being a company's customer. Besides that, the categories that the customer has been buying in the previous purchases also have been shown to influence the customer's decision to stay. This is in line with findings from the previous studies [@athanassopoulos2000customer; @mozer2000predicting]. One possible explanation of churning based on categories bought suggested by @mozer2000predicting is that the satisfaction of purchasing a particular category is low - no matter if because of high price or low quality of the product bought.

In this study, from the category of behavioural variables, I have included information about monetary value of the first purchase, delivery cost, number of items bought and categories of items that the customer bought. The specification of the behavioural variables used is slightly different from previous studies. Namely, I have excluded Recency and Frequency from the set of predictors. The reason is that in this study I am interested in predicting customer loyalty just after first purchase. Because of that, these variables that need some time passed since the purchase can't be calculated or used.

In the case of company analyzed in this study, the products sold belong to 74 distinct categories. At the same time, top 15 categories account for 80% of all purchases. Because of potential problems with generalization and also slower model training, I have decided to bin the least popular ones as a new category "other". Then, I have used one-hot-encoding approach to create numeric representation, with "other" category set as a base level. ^[Because of the fact that in one order there can be multiple product categories, it is not guaranteed that there will be one "1" value per each row as in classical one-hot-encoding method.]

To assess the validity of previous studies' findings about behavioural variables in e-commerce retail context, I have tested 2 hypotheses. (1) The amount of money spent on the first purchase positively influences customer probability of buying for the second time. (2) categories bought by the customer can influence the customer probability to stay with the company.

### Location features

#### Ways of including spatial dimension to the churn prediction

@lee2013neighborhood argues that customer location and its neighbourhood is an important factor to consider in CRM analyses even in e-commerce settings.

There are multiple ways to include spatial dimension in modeling. In this study, I have analyzed 3 broad approaches, that were used in previous studies:

- directly including location variables (geographical coordinates, zip code, region dummies etc.)
- analyzing neighbourhood that the customer resides in (demographical statistics about the region)
- classifying customers by living in an urban or rural area

#### Direct inclusion of spatial variables

To the best of authors knowledge, no studies on churn prediction conducted before included raw geographic coordinates in the model formulation. Rather, usually dummy variables indicating the administrative regions were used. There is no consensus whether such data can improve the predictions. @verbeke2012new argued that "the number of times a customer called the help desk will most probably be a better predictor of churn behavior than the zip code". On the other hand, @buckinx2005customer showed that such dummies were significant in the case of Neural Network model, but not in Random Forest. Also, @long2019new found that these dummies are significant. In these case however, a different spatial extent was analyzed - the regions variables indicated countries rather than postcodes.

@de2019impact used geolocation data in the context of churn prediction for an insurance company. They took different approach to operationalizing customer location. Instead of including dummies indicating customer's region, they calculated distance between the customer and the closest insurance agent. Such variable was significant.

In this study, I have simply included longitude/latitude data about each customer directly to the model formulation. This is to check, if propensity to churn can be explained by customer location.

#### Rural vs. urban

Generally, there is a consensus among researchers that there is a difference in customer behaviours between rural and urban areas [@sun2004consumption]. In particular, couple of studies in FMCG sector have found that rural customers tend to be more loyal to the previously chosen company [@jha2003understanding; @sharma2021impact]. The potential reason for such finding provided by the authors is smaller choice of other options in the rural shops compared to urban ones. However, up to date there were no studies that were meant to assess the differences between customer loyalty in urban and rural areas, but aimed at e-commerce sector. The findings from FMCG sector does not have to translate directly, as in online setting the customers are generally not limited by the availability of the brand in their area. 

A hypothesis worth checking is if tendency to churn is dependent on whether the customer is living in a densely populated area.

There are 2 possible ways to conclude if particular customer is living in an urban or rural area. One is simply checking if the customer's coordinates are inside city's administrative boundaries. Such approach does not guarantee that this customer is really living in densely populated area - because of the fact that administrative boundaries do not have to reflect actual boundaries, for example because of fast suburbanisation spilling to previously village areas.

Other way is inferring the population density in the area from the data. This way, one gets more reality-reflecting densely populated areas classifications. As was shown before in dataset review (??), number of the customers per each microregion highly correlates with population density in this area. Because of that, it can be argued that also in smaller scale of analysis than microregions such correlation will be also evident. This leads to a conclusion that the company's customers' locations can be used as a proxy for population density, so it can be used for classifying densely and sparsely populated areas. 

In this study I have used Density-Based Spatial Clustering with Noise (DBSCAN) algorithm for the task of rural vs. urban areas classification. This clustering algorithm besides assignment to particular cluster can also detect noise points. Because of that the assignments have a natural interpretation. When the point belongs to any cluster it means that the customer is living in a densely-populated area, while the points decoded by DBSCAN as noise are the customers living in more isolated places.

DBSCAN has 2 parameters to be decided before running the algorithm. These are the minimal number of points laying close to each other that are needed to constitute a cluster (*k*), and maximal distance, at which one considers the points to lay close to each other (*epsilon*).
A typical rule-of-thumb for deciding k and epsilon parameters is to first set k, and then plot k-nearest-neighbors distances. Epsilon should be then decided based on *elbow point*, where the line is bending. However, when the features are geographical coordinates, epsilon is actually a physical distance between two locations. That is why based on expert knowledge from the company one can set what should be more reasonable criteria for constituting clusters. 

In my work I have decided somehow arbitrarily that minimal number of customers in the cluster is 100, and the maximum  distance between the customers in one cluster is 50 kilometers. For the location of Brazil on the geoide, this transfers roughly to epsilon=0.2. However, this kind of decision in a real company setting can (and should be) consulted with the company's domain experts in marketing. 

#### Geodemographics

Geodemographics is the "analysis of people by where they live" [@harris2005geodemographics]. In this paradigm, it is assumed that people living in the same area share similar characteristics, like their social status, income etc. Such type of data was used extensively in previous studies. However, as pointed by @singleton2014past, its usage was mostly in public sector areas, mainly public health and law enforcement. Publicly available research in usage of geodemographics in context of marketing, or specifically churn prediction is almost non-existent. This has its reasons in confidential nature of research done in individual companies [@webber2004targeting]. The only publicly available study was conducted by @zhao2005customer. They found that geodemographic features were significant in the churn prediction model.

A hypothesis I would like to check is if social structure of the customer's environment can serve as a valuable predictor of churn tendency.

In total, I have included 35 demographic features for the microregion from which the customer is - age structure, percentage of population in urban area, income structure, number of immigrants. These features were obtained from Brazilian statistical office SIDRA. Joining of the data coming from SIDRA and OLIST sources proved to be challenging. The details of such spatial join are presented in Appendix A. 

Geodemographic features in this study are relatively high dimensional (35 variables). At the same time, one would expect that the information can be somehow compressed, because lots of the variables represent very similar concept (for example there are 20 variables encoding only age structure). 

Because of that, I have decided to process this part of the dataset using Principal Components Analysis. This can potentially bring some improvements in the process of Machine Learning modeling, as training the model on a smaller, compressed dataset is more resource-efficient and at the same time was shown to improve the modeling results in some cases [@howley2005effect].

One decision regarding PCA transformation is whether to use a standard version, or the one with rotated loadings [@corner2009choosing]. The trade-off between these two methods is that rotated loadings allows for interpretation of the loadings, but is less optimal in a sense that the variance along each loading is not maximized. I have decided that a standard one would be more suitable in case of this study, because the explainability of the input variables to the model is not as important as correctly representing the features in lower-dimensional space and thus preserving as much valuable information as possible for the modeling phase.

### Perception

<!-- #### Definition of the category -->

Customer perception of the company is considered an important factor driving customer loyalty [@kracklauer2001mutual].
Unfortunately, customer satisfaction is an immeasurable variable. Different proxies can be however included in the model, and usually gathering such data requires conducting customer surveys. @churnthesis specifies possible dimensions of such survey: "overall satisfaction, quality of service, locational convenience and reputation of the company". 

In e-commerce settings, an industry standard is to provide a way for the customers to express their opinions about the purchase [@lucini2020text]. The company has to decide, in how structured way it would like to collect them. Text reviews can provide way richer information about the customer experience, as they are not limited to describing the experience in predefined dimensions. On the other hand, extracting meaningful information from sometimes millions of text reviews is a very challenging task to which no universally acclaimed solutions exist [@felbermayr2016role; @zhao2019predicting]. 

In the case of the dataset analyzed in this study, there are 2 proxies of customer perception avaliable. One is a customer review on a scale from 1 to 5. The other is a textual review of the purchase. Using numeric review in the modeling is straightforward and doesn't require further explanation. In the next sections I have presented ...

#### Ways of analyzing textual reviews

As stated before, text reviews can potentially serve as a rich source of information about customer satisfaction.
Although text mining for customer reviews in general is an active field of research, usage of such information in the context of churn prediction is way less covered. To the best of author's knowledge, only 2 studies used the data from textual reviews for churn prediction. @DECAIGNY20201563 have used text embedding approach, while @9325646 - simple tf-idf technique. 

@lucini2020text specifies 2 natural language processing areas that can be used to  extract insights from customer reviews, namely topic modeling and sentiment analysis. First one is meant to answer the question "what the review is about?", while the second - "what is the perception contained in this review?" . Combination of these two dimensions can help answer the question, which areas of customer experience are rated positively, and which need improvement.

In the case of this study, I have focused only on extracting the topic from the review. The reason is that an information about whether the experience of the customer was positive is already contained in numeric review. My hypothesis is that both the numeric review, as well as topic of the textual review can be useful predictors of customer loyalty.

<!-- @zeroual2018data -->
<!-- @athanassopoulos2000customer  -->
<!-- https://www.jmir.org/2021/1/e22184/ - topic modeling + RNN -->

#### Previous research in topic modeling

<!-- ##### LDA, dlaczego słabe -->

Undoubtly the most popular model for inferring the topic of a text is Latent Dirchlet Allocation [@blei2003latent]. The method is based on assumption that each document is a mixture of a small number of topics. At the same time, each topic can be characterized by a distribution of words frequency.

@hong2010empirical argues that short texts (as in the case of customer reviews) comprise of very small amount of topics, usually only one. Because of that, LDA should not be used in such settings as its assumptions are violated. This claim is supported by empirical study of short texts from Tweeter, in which LDA has failed to find informative topics.

<!-- ##### Movie group process -->

The drawbacks of LDA in the setting of short texts were addressed by @10.1145/2623330.2623715 . They used Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model, which is improvement over typical LDA. Main difference compared to the basic algorithm is an introduction of assumption, that each texts comprises of only one topic. The authors show that this algorithm provides superior performance compared to basic LDA technique in the context of short texts. 

<!-- ##### Aspect attention -->

More modern approaches to topic modeling were also developed recently. A milestone in the whole NLP field was inventing an efficient way to embed words in a vector space while preserving their meaning, namely word2vec [@mikolov2013efficient]. On a basis of this method, @he2017aspect presented an Attention-based Aspect Extraction^[Words "Aspect" and "Topic" are often used interchangably in NLP literature] model. At first, words embedding using Word2Vec model is created. After that, for each text in the corpus, attention weight for each word is computed using neural network with an attention layer. Then, embedding of the whole sentence is created by computing an average for all words embedding. The words are weighted by their attention weights. Last step of the procedure is creating encoder-decoder model for learning sentence aspect embedding. The reconstruction of the sentence is the linear combination of aspect embeddings, and aspect embeddings are learned by mapping sentence embedding to a lower dimensional space.

Another studies using embedding technique were conducted by @tulkens-van-cranenburgh-2020-embarrassingly and @ijcai2019-712. In both of the studies the algorithms presented outperformed LDA method in the task of short text topic modeling.

<!-- @tulkens-van-cranenburgh-2020-embarrassingly, who proposed a new type of Attention mechanism, meant especially for aspect recognition task. It's advantage over the one presented by [@he2017aspect] is that instead of a complex neural network, a way simpler approach based on Radial Basis Function kernel is used. Another work presenting new attention mechanism is by @ijcai2019-712 - they use a use a Encoder-Decoder framework with an *Semene Attention* mechanism. (skrócić do 1 zdania) -->

#### Text reviews preprocessing in this study

In this study, I have tried and evaluated 3 algorithms for topic modeling:

- Latent Dirchlet Allocation [@blei2003latent] - because it is a go-to standard for topic recognition.
- Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture [@10.1145/2623330.2623715] - as this method is an improvement over LDA, meant especially for short texts. This is true in this case, as most of the reviews are just a couple of words long.
- Attention-Based Aspect Extraction [@he2017aspect] - this method is also meant for short texts, and at the same time it uses the most modern, state-of-the-art NLP techniques. Besides that, in the original paper the authors worked in the similar domain of internet text reviews. 

Various preprocessing steps were needed to apply all 3 aforementioned algorithms:

- **Translation of the reviews from portuguese to english.** Olist e-commerce store is operating only in Brazil. That is why most of the reviews are written in Portuguese. I have used Google Translate API to change the language of them to English. This is to facilitate not only understanding the reviews, but also the NLP tools available for English language are more advanced than for other languages.
- **Removal of stopwords and punctuation.**
- **Lemmatization** using WordNet lemmatizer [@wordnet] combined with Part-of-Speech tagger. This step is needed to limit the number of words in the vocabulary. Thanks to Part-of-speech tagger,the lemmatizer can change the form of the word on a more informed basis, and thus apply correct lemmatization to more words. 

Later steps of the preprocessing were different for each of the algorithms.

For LDA and Gibbs Sampling, only **converting lemmatized reviews into vector format** was needed. In case of LDA, count-vectorizing approach was applied, with removing of words which appeared in less than 0.1% of reviews. In the case of Gibbs Sampling the same preprocessing is done internally by the training function from python package. In both of these cases after vectorization one should obtain a matrix with n rows and k columns, where n is number of observations in the original dataset, while k - size of the vocabulary. 

Very different preprocessing was required in the case of Attention-Based Aspect Extraction. The neural network architecture proposed by the authors requires simply lemmatized reviews in textual format as the output. Then, one of the layers of the network is meant to embed the currently preprocessed word. These embeddings are not learnt during the network training, they should be trained beforehand instead. The authors of the paper propose Word2vec technique [@mikolov2013efficient] for learning embeddings. Following their guidelines I have used this method, setting the dimensionality of the vector space to 200. I have also applied the word window of 10. After applying word2vec on this dataset, I have obtained the matrix with m rows and 200 columns, where m stands for number of words in the dataset, and 200 is the dimensionality of the vector space chosen as a hyperparameter. 

Concerning topic models training, I have searched for optimal hyperparameters for all 3 models based on grid search. For LDA, I have tested varying number of topics that the model has to learn (3, 5, 10 and 15). For GSDMM, there are 2 parameters that influence topics coherency in each "cluster". I have run the algorithm for all 16 combinations of both parameters chosen from the values 0.01, 0.1, 0.5 and 0.9. For Attention-Based Aspect Extraction, I have manipulated with the number of topics to learn, from the values 10, 15. Unfortunately, this last model takes a very long time to run (around 3 hours per one set of hyperparameters), I have limited the number of hyperparameters checked compared to LDA model. 

Unfortunately, the evaluation of topic extraction is a hard task, as no model-agnostic metrics that can be compared between different models exist. The only reasonable method is human inspection. That is why after running every model I have verified the obtained topic for coherency (whether reviews inside one topic are similar) and distinctiveness (whether there are visible differences between modeled topics).


<!-- Olist e-commerce store is operating only in Brazil. That is why most of the reviews are written in Portuguese. I have used Google Translate API to change the language of them to English. This is to facilitate not only understanding the reviews, but also the NLP tools available for English language are more up-to-date and advanced. -->

<!-- In all 3 methods I have removed stopwords and punctuation from the text. Also, to limit the number of words in the vocabulary I have used lemmatization technique. This was done by WordNet lemmatizer [@wordnet]. To further improve the results, I have used Part-of-Speech tagger, and passed the tags of words to the lemmatizer. This way the algorithm can change the form of the word on a more informed basis, and thus apply lemmatization to more words.  -->

<!-- Second step of the preprocessing was converting lemmatized reviews into vector format. In case of LDA, count-vectorizing approach was applied. The words which appeared in less than 0.1% of reviews were dropped from the dataset. In the case of Gibbs Sampling the same preprocessing is done internally by the training function from python package.  -->
<!-- In both of these cases after vectorization one should obtain a matrix with n rows and k columns, where n is number of observations in the original dataset, while k - size of the vocabulary.  -->

<!-- Very different preprocessing was required in the case of Attention-Based Aspect Extraction. The neural network requires simply lemmatized reviews in textual format as the output. Then, one of the layers of the network is meant to embed the currently preprocessed word. These embeddings are not learnt during the network training, they should be trained beforehand instead. The authors of the paper propose Word2vec technique [@mikolov2013efficient] for learning embeddings. Following their guidelines I have used this method, setting the dimensionality of the vector space to 200. I have also applied the word window of 10.  -->

<!-- After applying word2vec on this dataset, I have obtained the matrix with m rows and 200 columns, where m stands for number of words in the dataset, and 200 is the dimensionality of the vector space chosen as a hyperparameter.  -->
<!-- One should bear in mind that count-vectorization works on review level, while word2vec - on words level. This means that after applying word2vec model to 1 review, one would obtain the same number of outputs as the number of words in the review. This is why it is impossible to use word2vec to preprocess the dataset and then use LDA or Gibbs sampling without some way to convert couple of vectors into one.  -->

<!-- I have searched for optimal hyperparameters for all 3 models based on grid search. For LDA, I have tested varying number of topics that the model has to learn (3, 5, 10 and 15). For GSDMM, there are 2 parameters that influence topics coherency in each "cluster". I have run the algorithm for all 16 combinations of both parameters chosen from the values 0.01, 0.1, 0.5 and 0.9. For Attention-Based Aspect Extraction, I have manipulated with the number of topics to learn, from the values 10, 15. Unfortunately, this last model takes a very long time to run (around 3 hours per one set of hyperparameters), I have limited the number of hyperparameters checked compared to LDA model.  -->

<!-- Unfortunately, the evaluation of topic extraction is a hard task. The only reasonable method is human inspection. That is why after running every model I have verified the obtained topic for coherency (whether reviews inside one topic are similar) and distinctiveness (whether there are visible differences between modeled topics). -->

<!-- ## Reviews analysis -->

<!-- The algorithm used by the authors is called Movie Group Process. Short introduction to this algorithm is included below. -->

<!-- Imagine a movie discussion group. There are k tables, and the goal is to assign students to tables according to their similar movie taste. There are 2 preference parameters set for each student: -->

<!-- 1. Choose a table with students having similar movie taste. This is meant to introduce homogeneity of the clusters.  -->
<!-- 2. Choose a table with more students in this group. This rule is meant to improve completeness - so to the clusters have a reasonably high number of members. -->


<!-- In recent years completely new approaches to Natural Language Processing emerged, thanks to improvement in the area of Neural Network algorithms. Two approaches are especially important as they serve as a baseline for the most recent findings in aspect (topic) recognition area. These two are word vector representations and attention mechanism. A short introduction of these two methods is presented in the section below. -->

<!-- In 2013, word2vec [@mikolov2013efficient] was presented. The goal of this method is to learn a meaningful vector representation of each word in a corpus. Word2vec’s approach is to train a model that predicts all of the neighboring words for every occurrence of every word in an entire body of text (a corpus). -->

<!-- Intuitively, suppose that the model needs to learn embeddings for 3 words: "king", "queen", "orange". The points in the embedding space for the first two words should lay in the proximity, while "orange" should be further. Word2vec approach is to look at the probability, that given word should be placed in particular place in the sentence, given the neighboring words. Suppose we have an incomplete sentence "XXX were usually very rich in the past". Word2vec tries to predict what XXX should be. From the corpus it should understand, that "king" and "queen" are more probable than "orange", that is why puts the embeddings closer. -->

<!-- Creating word embeddings usually serve as a preprocessing phase for next analysis steps, as with the data in numeric form one can use all tools that conventional data analysis has to offer, not being limited anymore by the complicated nature of textual data. -->

<!-- Another concept very helpful in the aspect recognition domain is attention mechanism [@chorowski2015attention]. It is based on attention mechanism in psychology. When a human is trying to understand any content (visual, textual etc.) she is not using all content in the same extent, but only the relevant parts. For example, when a car driver is making a decision whether to cross an intersection, from all the visual signals that she obtains at the moment, the most important (and the only one looked at) is whether the light is red or green.  -->

<!-- This concept can be very useful in the area of aspect prediction, as usually only couple of words from the whole sentence show the topic of it.  -->


<!-- An important source of knowledge about e-commerce customers are textual reviews. They can serve as a rich source of feedback for what in the shop or product is liked and what needs change. Also, in the textual reviews one can get to know customer's opinions way better then using other types of feedback, for example 1-5 rating of a purchase. With these advantages, they come at the expense of increased complexity of such analysis. A big challenge is to extract meaningful information from this type of highly unstructured data. -->

<!-- Two most important types of text mining in text reviews is *sentiment prediction* and *topic mining* (in the context of reviews also often called *aspect mining*).  -->
<!-- Topic modeling is particularly challenging, as usually one does not have a annotated dataset with topics assigned to each text. That is why an unsupervised approach usually has to be used. -->


## Variables selection

TODO: Introduction dlaczego takie sety zmiennych a nie inne, opisać listy 1 zdaniem

Previous studies usually used RFM variables as basic ones, adding other types as additional ones.

### Sets of conceptual features (?? lepszy tytuł)

Sets of features:

- basic information - value of the purchase, geolocation in raw format lat/lng, value of the package, number of items in the package, review score (6 variables)
- demographic features for the region from which the customer is - age structure, percentage of population in urban area, income structure, number of immigrants (35 variables)
- demographic features transformed using PCA - (10 variables/components)
- indicator whether the customer is in an agglomeration area obtained from DBSCAN on location data (1 variable)
- product categories that the customer has bought in the purchase (15 dummy variables)
- main topic that the customer has mentioned in the review (15 dummy variables)

Models tested:

- basic features
- demographic + basic
- demographic with PCA + basic
- agglomeration + basic
- product categories + basic
- review topic + basic
- all variables - demographic features transformed with PCA

I have not run the model containing all variables with demographic features without PCA preprocessing. There are 2 reasons for that - one is that number of variables in this set is very big, what poses performance reasons - model training simply would take a very long time. Assuming that the PCA set of features would give better score can be also supported by the fact that the model with only included PCA demographic variables is performing better than the full set of variables.  

### Boruta

To remove the human judgement about which predictors should be used, I have also used a Boruta algorithm for feature selection [@kursa2010feature]. It is widely popular among machine learning practitioners [@kumar2017empirical]. The algorithm belongs to category of wrapper feature selection algorithms, and Random Forest algorithm is usually used as an underlying algorithm. It works as follows. At first, all features from the original dataset are randomly permuted. In this way one obtains a dataset with close-to-zero predictive power. Then, the resulting features are added to the original dataset and Random Forest model is trained. 

Random Forest model have a built in feature importance measure, that is usually Mean Decrease Impurity. After running random forest model, for each original features MDI is compared against all MDI scores for shadow features. If for any original variable the score is less than the one from any of shadow features, the variable gets a "hit". 

Above procedure is repeated for preassigned number of iterations. Finally, important features that should make it to the final model are the ones that obtain less hits than preassigned n. 

After gaining knowledge about the variables that should make it to the model, I have trained XGBoost classifier using these features. The rest of the fitting procedure (cross-validation, up-sampling, hyper-parameters etc.) stayed the same as in the rest of the approaches.  

One should have in mind that Boruta algorithm is very time-consuming. The minimal number of runs recommended by the method authors is 100, and one run consists of fitting a Random Forest model to the whole dataset with doubled number of features (because of added shadow features). In the case of this analysis, model computation took about 12 hours on a medium-class modern laptop. Although other wrapper algorithms also require an iterative fitting the model, they usually start with fitting the model to one variable, in the next iteration to 2, and so on up to k features. On the other hand Boruta algorithm in each iteration fits the model to 2*k features (original and shadow features).

<!-- ## Śmieci -->

<!-- https://www.sciencedirect.com/science/article/pii/S0377221703009184?casa_token=6GS1hqD0KcAAAAAA:4p02d0H6wlfEDACtjCUYfXB_5ABUmf2QqD9RILvjbNYHgLF6S3tnAT2VjqJXLzJFxI1oVK9GTQ -->

<!-- Dobrze wytłumaczone co może powodować które kategorie -->
