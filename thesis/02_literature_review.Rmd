---
title: "Literature review"
output: word_document
# output: html_document
bibliography: resources/bib.bibtex
params:
  echo: False
---

```{r include=FALSE}
knitr::opts_chunk$set(echo=params$echo)
```

TODO: *W części 02 z literature review dałabym ilustrację celów, metod i dotychczasowych badań (trochę jak w tym artykule o metodach w CRM), żeby uwypuklić o czym w ogóle mówimy.*

TODO: Coś dać o tym że XAI jest ważne dla rozpowszechniania używania modelu i predykcji - tutaj link do jakiegoś badania nt. kultury w organizacji po wprowadzeniu ML: https://www.bastagroup.nl/wp-content/uploads/2019/01/the-state-of-machine-learning-adoption-in-the-enterprise.pdf



## Customer churn

Paragrafy do dodania ogólnie:

- ogólnie o zjawisku churnu:
  - definicja churnu
  - zalety posiadania lojalnych klientóW
  - mechanizm churn i retention - większość researchu dla subscription based services, dla retail mało a dla online retail bardzo mało. Coś o tym że klienci w internecie są mniej lojalni (jeżeli to prawda)
  - if we can predict which customers are likely to leave, we can target them with some customer attraction tools
  
  
- Churn prediction methods

  - definicja churn jako zmiennej w modelowaniu
  - "churn problem can be decomposed as choice of ML model and choice of model of variable influences"
  - review algorytmów wykorzystywanych - zacytować kilka prac o tym czego używali, ale też napisać o no free lunch theorem i że powinno się zawsze sprawdzić na swoich danych. Do tego może coś o trade offie pomiędzy LR i XGBoost - o explainability
  - review zmiennych wykorzystywanych

### General about customer retention

#### Churn in CRM

Customer Relationship Management is defined as a process, in which the business manages its interactions with the customers using data integration from various sources and data analysis in the later stages [Digital CRM: Strategies and Emerging Trends: Building Customer Relationship in the Digital Era]. 
[@] provides 4 areas in which CRM approaches can be of use:

- Customer identification (aquisition) - determining who can be a potential customer?
- Customer attraction - how can one make this person a customer?
- Customer development - how can one make a customer more profitable?
- Customer retention - how can one make the customer stay with the company?

The last one is the main focus of this study. 

#### Why having loyal clients is important

Improving the loyalty of customer base is profitable to the company. This has its source in multiple factors, the most important one being cost of aquisition.  Multiple studies have shown that retaining customers costs less than attracting new ones [@dick1994customer; @gefen2002customer; @buckinx2005customer]. Moreover, there is some evidence that loyal customers are less sensitive to the competitor's actions regarding price changes [@achrol1999marketing; @choi2006customer]. 


#### Difference with tackling potential churners

There are 2 basic categories of approaches for the company to deal with customer churn. First category is "untargeted" approach. The company seeks to improve its product quality and relies on mass advertising to reduce the churn. The other way is "targeted" approach - the company tries to address aim their marketing campaigns at the customers that are more likely to churn [@burez2007crm]. This approach can be divided further, by the way in which targeted customers are chosen. The company can target only these ones that have already made a decision to resign from further relationship. For example, in contractual settings this can mean canceling the subscription or breaching the contract. 

The other way to approach churn problem is to try to predict, which customers are likely to churn in the near future. This has an advantage of having lower cost, as the customers that are about to leave are likely to have higher demands from the last-minute deal proposed to them [@tamaddoni2010modeling]. 

#### Non-contractual churn prediction

As pointed out by @tamaddoni2010modeling in their literature review, most of the studies concerning churn prediction were done in contractual settings. In other words, churn was defined as the client resigning from using the company's services more by canceling the subscription or breaching the contract. Such way to specify the churn is different from the businesses in which the customer doesn't have to inform the company about resigning. 

One problem that arises in non-contractual setting is the definition of churn. As there is no clear moment that the customer decides not to use the company's services anymore, it has to be specified by the researcher based on the goals that one has to achieve from the churn analysis. @churnthesis defined partial churners as the customers not making new purchases in the retail shop for the next 3 months. A very different approach was used by @buckinx2005customer. All the customers that had frequency of purchases below average were treated as "churners", since these customers were shown to provide little value to the company. 

#### The difference between one-shot and multiple purchases shopping

- do tej pory nie widziałem takiej pracy która o tym mówiła

### Customer churn prediction

#### Why churn prediction makes sense

If the company is able to successfully predict, which customers are most likely to leave, it can target them with retention-focused campaign. Contrary to targeting all of the customers with such campaign, focusing on the customers that are most likely to leave leads to reduction of the cost of the campaign. 

#### Why classification vs. ranking

Churn prediction fits well with the framework of classification, as the variable that one would like to predict is binary (churn-no churn). However, not only such binary prediction is valuable for later retention campaign efforts. As noted by @1255389, equally important is that the machine learning model can predict the likelihood of the customer leaving. After such prediction, the customers can be ranked from the most to the least likely to churn. 

This has two benefits. First, the company can decide what percentage of the customers to target in the retention campaign, and not be bound by how many customers the model will predict as potential churners. Second, the company can decide how strong the targeting should be based on the likelihood to leave. For example, based on cost-benefit analysis of variuos targeting approaches, one could decide that for top 10% of the most likely to leave customers the company should offer big discounts for the next purchase, while for top 30% - only send an encouraging email. 

#### Challenges to be addressed

Churn prediction task can be decomposed into 2 main important aspects that one has to tackle. First is the decision about specific Machine Learning model that gives the best performance. Second is deciding on the model formula - in other words, deciding about which variables should be included in the model and what should be the form of the relationship.

#### Review of previous churn studies for models used

In previous studies multiple machine learning algorithms for prediction were used in churn setting (for overview see @verbeke2011building). The two most widely used techniques are Logistic Regression (LR) and Decision Trees. 

An important feature of both of them is that they are relatively simple, and because of that the way they make predictions can be assessed by a qualified expert [@paruelo1997prediction].
However, these two methods often give suboptimal results compared to more advanced and recent approaches like Neural Networks or Random Forests [@murthy1998automatic; @churnthesis]. Moreover, this was shown not only in the case of churn prediction setting, but also in more general benchmarks that used multiple datasets and comparison metrics [@caruana2006empirical].

Recently, XGBoost algorithm [@chen2015xgboost] is gaining popularity in multiple domains in which one faces prediction tasks.
XGBoost main strengths are ability to infer non-linear relationships from the data, and relative speed, which allows the researcher to try out multiple hyperparameters and decide on the best ones [@chen2015xgboost]. Because of that, it is considered as a go-to standard for machine learning challenges, and very often solutions based on it achieve the best results in various competitions and benchmarks  [@xgblist]. In the context of churn prediction, XGBoost was used by @gregory2018predicting. It achieved superior performance compared to other techniques, like Logistic Regression and Random Forests.

#### Review of previous churn studies for variables/categories of variables used 20 min przekminienie jak sformułować i czy tutaj czy do methods

- focus on preprocessing


### Churn analysis do śmieci?
  



The most low-hanging fruit for the companies that want to start basing their business desicions on the data is usage of transaction-level data. That is because virtually every e-commerce shop is based on the the mechanism of user registration, and storing the client's purchasing history is an industry standard.

!!! The data only about when the customer made purchases and how much did he pay are very easily translated into the framework of Recency-Frequency-Monetary value. Multiple works (@aleksandrova2018application, @8284914) demonstrated that such data can serve as a good input to churn prediction machine learning model. In fact, most of the publications presented in this review is using RFM variables as one part of the dataset, while including more complex, engineered variables as the other part. 

!!! Because of the digital nature of e-commerce shopping, way more detailed and enriched data can be used in hope of finding more appropriate features. One of such features is per-session data - that is the information about how the user is navigating on the site. @yu2011extended used the data avaliable in the data warehouse of e-commerce company to predict churn. They combined per-transaction data, per-session data and customer data using Extract-Transform-Load tools and manual feature engineering. @8627369 tried to predict user churn on a per-session basis. The question they stated was "Will the customer unregister from the service during this session?". They used a very detailed per-session metadata like the day of the week, session number or number of purchases done up to the point. 

After the first purchase of the customer in the e-commerce shop, their exact adresses can be inferred with high probability. Usually the delivery adress would be to the home of a customer, or in worse cases to other place that the customer visits (like workplace etc.). @zhao2005customer used this kind of customer location data to enrich the dataset with basic spatial characteristics of the region, that is geographic situation and demographic variables.

The content of the web is very often avaliable in unstructured, textual form. Online retailers very often give their customers to write reviews about the purchase. Although such information is very hard to incorporate into churn prediction model, it can serve as a very rich source of insight. 
@DECAIGNY20201563 showed how such data can be incorporated into churn prediction modeling to obtain superior results. They have used text embedding approach as a feature extraction method.

@9325646 analysed responses to such questionaires and tried to predict, whether preprocessed textual reviews can serve as explanatory variables. After using tf-idf transformation they found out that they can. 

### Reviews analysis

TODO: do zastanowienia jakie powinny być więcej częsci w lit review, poprawić strukturę tekstu

An important source of knowledge about e-commerce customers are textual reviews. They can serve as a rich source of feedback for what in the shop or product is liked and what needs change. Also, in the textual reviews one can get to know customer's opinions way better then using other types of feedback, for example 1-5 rating of a purchase. With these advantages, they come at the expense of increased complexity of such analysis. A big challenge is to extract meaningful information from this type of highly unstructured data.

Two most important types of text mining in text reviews is *sentiment prediction* and *topic mining* (in the context of reviews also often called *aspect mining*). 
Topic modeling is particularly challenging, as usually one does not have a annotated dataset with topics assigned to each text. That is why an unsupervised approach usually has to be used.

A go-to model for inferring the topic of a text is Latent Dirchlet Allocation [@blei2003latent]. The method is based on assumption, that each document is a mixture of a small number of topics. At the same time, each topic can be characterized by a distribution of words frequency. 

Unfortunately, LDA approach was created with different purpose in mind. Typically reviews in the aspect of e-commerce are very short. @hong2010empirical showed that LDA is not able to find informative topics in Twitter posts. These posts are bound by the rules of the platform to be shorter than 280 characters long. Possible reason that LDA does not cope well is that assumption about a document being mixture of topics is false. Short texts probably comprise of very small amount of topics, usually only one. 

The drawbacks of LDA in setting of short texts were adressed by [@10.1145/2623330.2623715] . They used Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model, which is improvement over typical LDA. 
The algorithm used by the authors is called Movie Group Process. Short introduction to this algorithm is included below.

Imagine a movie discussion group. There are k tables, and the goal is to assign students to tables according to their similar movie taste. There are 2 preference parameters set for each student:

1. Choose a table with students having similar movie taste. This is meant to introduce homogeneity of the clusters. 
2. Choose a table with more students in this group. This rule is meant to improve completeness - so to the clusters have a reasonably high number of members.

The authors show that this algorithm provides superior performance to vanilla LDA not only when the texts are short, but also in general. 

In recent years completely new approaches to Natural Language Processing emerged, thanks to improvement in the area of Neural Network algorithms. Two approaches are especially important as they serve as a baseline for the most recent findings in aspect (topic) recognition area. These two are word vector representations and attention mechanism. A short introduction of these two methods is presented in the section below.

In 2013, word2vec [@mikolov2013efficient] was presented. The goal of this method is to learn a meaningful vector representation of each word in a corpus. Word2vec’s approach is to train a model that predicts all of the neighboring words for every occurrence of every word in an entire body of text (a corpus).

Intuitively, suppose that the model needs to learn embeddings for 3 words: "king", "queen", "orange". The points in the embedding space for the first two words should lay in the proximity, while "orange" should be further. Word2vec approach is to look at the probability, that given word should be placed in particular place in the sentence, given the neighboring words. Suppose we have an incomplete sentence "XXX were usually very rich in the past". Word2vec tries to predict what XXX should be. From the corpus it should understand, that "king" and "queen" are more probable than "orange", that is why puts the embeddings closer.

Creating word embeddings usually serve as a preprocessing phase for next analysis steps, as with the data in numeric form one can use all tools that conventional data analysis has to offer, not being limited anymore by the complicated nature of textual data.

Another concept very helpful in the aspect recognition domain is attention mechanism [@chorowski2015attention]. It is based on attention mechanism in psychology. When a human is trying to understand any content (visual, textual etc.) she is not using all content in the same extent, but only the relevant parts. For example, when a car driver is making a decision whether to cross an intersection, from all the visual signals that she obtains at the moment, the most important (and the only one looked at) is whether the light is red or green. 

This concept can be very useful in the area of aspect prediction, as usually only couple of words from the whole sentence show the topic of it. 

@he2017aspect presented an Attention-based Aspect Extraction model. At first, words embedding using Word2Vec model is created. After that, for each text in the corpus, attention weight for each word is computed using neural network with an attention layer. Then, embedding of the whole sentence is created by computing an average for all words embedding. The words are weighted by their attention weights. Last step of the procedure is creating encoder-decoder model for learning sentence aspect embedding. The reconstruction of the sentence is the linear combination of aspect embeddings, and aspect embeddings are learned by mapping sentence embedding to a lower dimensional space.

Another work worth mentioning is by @tulkens-van-cranenburgh-2020-embarrassingly, who proposed a new type of Attention mechanism, meant especially for aspect recognition task. It's advantage over the one presented by [@he2017aspect] is that instead of a complex neural network, a way simpler approach based on Radial Basis Function kernel is used. Another work presenting new attention mechanism is by @ijcai2019-712 - they use a use a Encoder-Decoder framework with an *Semene Attention* mechanism. 

Losowe papery:

Online reviews as a feedback mechanism for hotel CRM systems
Creating a geodemographic classification model within geo-marketing: the case of Eskişehir province

### XAI

While deciding on the type of Machine Learning algorithm, one usually faces the explainability-performance trade-off [@nanayakkara2018characterising]. More flexible models, like boosting or neural networks, usually present superior performance to more basic approaches. On the other hand, their predictions cannot be explained as easily as in the case of for example Decision Trees.


co możesz napisać?

- Dlaczego 

- AI jest black box
- co to XAI
- 
https://www.kdnuggets.com/2018/10/enterprise-explainable-ai.html


https://blog.goodaudience.com/holy-grail-of-ai-for-enterprise-explainable-ai-xai-6e630902f2a0

https://www.sciencedirect.com/science/article/pii/S1094996820300888?casa_token=Odeol3YA4U4AAAAA:NCkaJ-yDb53CB6JWnezV-MZpcI6kHb2D_XpTapz7DDih72a6U3N3Kcxn28IIiUqrmOnSnOhf5Q
challenge dla marketingu przy stosowaniu AI

https://arxiv.org/pdf/1810.00184.pdf

xai z perspektywy końcowego usera

Luźne opisy paperów:

https://www.preprints.org/manuscript/202106.0063/v1
Dokładnie ten sam dataset, preprint opublikowany 5.06 (sic!)

https://arxiv.org/pdf/1703.03869.pdf
Bardzo dokładny opis podejścia do churn prediction z użyciem DL

https://link.springer.com/chapter/10.1007/978-981-32-9563-6_11
[@jheng2019customer]
- retention prediction przez CNN

https://www.sciencedirect.com/science/article/pii/S0019850116301651?casa_token=YCUcElM8k_EAAAAA:-qJeOGXh7u2pQlqj-eyAo9k-eLgbc-m31QsDURsmpD2CEIyqtUzAjYGXUwkQRR4T0MrtkIbeWtaG

- sam paper średni ale ma dużo referencji

https://sci-hub.se/https://ieeexplore.ieee.org/abstract/document/7538581

- fancy metody imbalance
- używają datasetu takiego jak ja
- mega dobra dokładność
- ALE najpierw robią upsampling a później oceniają performance na CV - błąd! u mnie to różnica pommiędzy 0.62 a 0.78 AUC

http://flr-journal.org/index.php/mse/article/view/10816/11113

- wykorzystują social network userów

https://link.springer.com/article/10.1007/s10660-019-09383-2

- wykorzystują deep learning do imbalance - chyba lepiej będzie pasować do innej sekcji

https://www.emerald.com/insight/content/doi/10.1108/17515631011063767/full/html

dużo references 

A study on factors affecting the purchasing process of online shopping: a survey in China & Japan

na podstawie kwestionariusza ocena satysfakcji

https://d1wqtxts1xzle7.cloudfront.net/62198454/key-paper20200225-3623-15suux9.pdf?1582687757=&response-content-disposition=inline%3B+filename%3DWhat_Effects_Repurchase_Intention_of_Onl.pdf&Expires=1617187287&Signature=LfylLp7R2PXNPLFtVyCNdj~e4FhDBUz04-T152E7FSsNHjnqclWeFnnKf9C2fJskRN2q~sRx~CsXCbeuhn0zcrktL0lj8oN8GUxrWXpavIz1UaQuO~ayrylqfAH2XgIdwhDe~8FOoMNP9ZzaNz6lqYuy6DYaBNhP6G7N3sUo2spQ187dGOgRHgGafoS3Z7HZ2AgEUjgs1ldOsU1E7FXrP1delDpO7QYarp9h1euOUM6vCWCxlsDZYnRF6A-PIuQlgyP8QOyzMo2d487sDw0Jepwjrd69ocCrSMsi7dmu56Z00CUoXaUA3b~C9vyQrfYI9T1hzMcJYfQYri4lUWgblQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA

ocena repurchase probability na podstawie kwestionariusza 


https://www.tandfonline.com/doi/abs/10.1080/08874417.2011.11645518?casa_token=33mj-Wcpw8IAAAAA%3AQUNps1MzrKJLyKf_c0Vl6gRIzoqI8wU3PbavgeTiyiJlQxwpHMi3JLUMmmGr7ZX0C2uqsrTT-TBYIw8&

znowu kwestionariusz ale z modelem


https://link.springer.com/article/10.1007/s10660-015-9207-2

- predicting repurchase intention 
- na podstawie kwestionariusza
- ale prediction z fancy metodami


https://ieeexplore.ieee.org/abstract/document/9325646
[@9325646]

- repurchase jako kwestionariusz tak/nie 
- ale wykorzystują predykcję na podstawie reviews

[@ganesh2000understanding] definicja churn

A particularly prominent forecasting application in CRM is customer churn prediction (CCP), which is defined as a method of identifying customers who show a high inclination to abandon the company


https://www.sciencedirect.com/science/article/pii/S0169207019301499?casa_token=kopLN0D45dwAAAAA:pARTYFQ1-0aho11qk4RpZdFdBIb1S-cJVHPb1iaggq41zU7pI-heeNpG9uK5cGThM7IWfFAkeGqU
[@DECAIGNY20201563]

-ładnie opisane profity z posiadania lojalnych customerów
- dobry paper, dużo odniesień i wykorzystanie textual data


https://www.sciencedirect.com/science/article/pii/S0957417410006779?casa_token=0C1SeJigqT8AAAAA:GCfX81AUr9p3ZfrqwTPCb23r4Slx6YijCvIOJE5xTcrxgl1nge7gjwvQnCo4c_r5fp1zaSigKjve
[@yu2011extended]

- Jest o prawdziwym churnie a nie o retention
- Jest złożona baza danych
- minimalny wstęp o churn prediction

https://ieeexplore.ieee.org/abstract/document/8627369
[@8627369]

- używają danych o sesji w przeglądarce

https://cursa.ihmc.us/rid=1MYWPTN4Z-BBB2D6-30SB/Zhao_Churn_Prediction_SVM.pdf

[@zhao2005customer]
- Używa danych demograficznych
- jest o churn

https://ieeexplore.ieee.org/abstract/document/8284914
@8284914
- prawie nic nie ma ciekawego, tylko jako case 

https://ieeexplore.ieee.org/abstract/document/1255389
[@1255389]

- Jest o tym że nie chodzi o predykcję tylko o ranking


##### References

[@tulkens-van-cranenburgh-2020-embarrassingly] CAT 
attention [@chorowski2015attention]
word2vec [@mikolov2013efficient] 
Main: [@he2017aspect]
drawbacks of LDA [@hong2010empirical] 

