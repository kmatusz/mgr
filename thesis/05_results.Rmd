---
title: "Results analysis"
# output: 
  # bookdown::html_document2:
  #   toc: true
  #   toc_depth: 5
output:
  bookdown::word_document2:
    reference_docx: resources/styles_template.docx
    number_sections: true
# TODO: zamienić tabelke z tematami na output nie-html, pozniej usunac ten agrument
always_allow_html: true
params:
  echo: False
bibliography: resources/bib.bibtex
---

<!-- TODO: *W części 05 trzeba do każdej analizy dać dwa typy opisów: metodologiczne (co wyszło, co nie wyszło etc) i marketingowe (co to znaczy dla CRM).* -->


```{r include=FALSE}
knitr::opts_chunk$set(message = F, warning = F, echo=params$echo, fig.width = 4, fig.height = 2.5, cache = T)
```

```{r cache=F}
library(readr)
library(tidyverse)
library("leaflet")
library(psych)
library(lubridate)
library(cluster)
library(factoextra)
library(caret)
library(rpart)
library(DALEX)
library(stringr)
library(here)
```


```{r cache=T}
# Create functions for calculating AUC etc.
bootstrap_auc <- function(model, test_set, no_resamples){
  out_roc <- vector('numeric', no_resamples)
  
  len_test <- nrow(test_set)
  for (i in 1:no_resamples){
    set.seed(i)
    idxes <- sample(1:len_test, size = len_test, replace = T)
    temp_test <- test_set[idxes,]
    
    predictions_temp = predict(model, temp_test,type = 'prob')
    roc_temp  <- pROC::roc(as.numeric(temp_test$if_second_order == "yes"), 
                           predictions_temp[, 1])
    out_roc[i] <- roc_temp$auc
    i <- i+1
    
  }
  out_roc
}

bootstrap_summary<- function(out_roc){
  
  tibble(auc =out_roc) %>%
    summary %>%
    print
  
  mean_auc <- mean(out_roc)
  tibble(auc =out_roc, y=0) %>%
    ggplot(aes(x=auc)) +
    geom_density() +
    geom_jitter(aes(y=y), alpha=0.5) +
    geom_vline(xintercept = mean_auc, color = 'red')
  
}


calc_metrics <- function(model, to_model_test, run_confusion_matrix = F){
  predictions1 <- predict(model, to_model_test,type = 'prob')
  roc_test1  <- pROC::roc(as.numeric(to_model_test$if_second_order == "yes"), 
                          predictions1[, 1])
  
  plot(roc_test1) 
  title('ROC curve on test set \n')
  print('Calc AUC on test set:')
  print(roc_test1)
  
  if(run_confusion_matrix){
    confusionMatrix(data = as.factor(as.numeric(predictions1[, 2] > 0.5)), # probability of yes was more than
                    reference = as.factor(as.numeric(to_model_test$if_second_order == "yes")),
                    # definitions of the "success" label
                    positive = '1') 
  }
  # print('Quantiles of predicted responses:')
  # print('-----')
  # predictions1[,2] %>%
  #   quantile(seq(0.8,1, 0.01))
}

calc_roc <- function(model, to_model_test, run_confusion_matrix = F){
  predictions1 <- predict(model, to_model_test,type = 'prob')
  roc_test1  <- pROC::roc(as.numeric(to_model_test$if_second_order == "yes"), 
                          predictions1[, 1])
  return(roc_test1)
}

flextable_format <- function(x){
flextable::font(x, fontname = 'Times New Roman', part = 'all') %>%
  flextable::fontsize(size = 12, part = 'all') %>%
  flextable::autofit()
}

```


```{r }
# Load the models and training data - only XGB, LR is too big for now

files_cache <- list.files(here('run_all_models_cache/'))
files_names_models <- files_cache[str_starts(files_cache, 'saved_model')]
files_names_models <- files_names_models[!str_starts(files_names_models, 'saved_model_lr')]
files_names_models <- files_names_models[!str_starts(files_names_models, 'saved_model_boruta_all_with_pca')]
# files_names_models


load(here('run_all_models_cache/to_model_train.Rdata'))
load(here('run_all_models_cache/to_model_test.Rdata'))

models_list <- vector('list', length(files_names_models))

for (i in 1:length(files_names_models)){
  # print(i)
  name <- files_names_models[i]
  temp_env <- new.env()
  load(here(paste0('run_all_models_cache/',name)),envir = temp_env)
  models_list[[i]] <- temp_env$model_spec
}

models_names <- c()
for (i in models_list){
  models_names <- c(models_names, i$model_name)
}

names(models_list) <- models_names
```

## Results of the pre-modeling phase

### Boruta feature selection

The Boruta algorithm concluded that from all 47 variables only the 14 variables indicating topics are non-relevant. One should notice that even using automatic feature selection, the algorithm has dropped the whole category of variables, meaning that the approach of manually setting sets of variables to include in the model is also "recommended" by the algorithm. 

### Topic modeling (TODO)

TODO: that only one model produced meaningful results (Description of the table czy tabelka do appendixa??)

```{r}

topics_table_to_paper <- read_csv(here('topics_table_to_paper.csv')) %>%
  mutate(`Example reviews` = str_replace_all(`Example reviews`, '\\n', '||||'))

load(here('run_all_models_cache/topics_second_order_percentages.Rdata'))

topics_second_order_percentages %>%
  filter(topic != 'topic_nan') %>%
  separate(topic, c('a', 'b'), '_') %>%
  mutate(b = as.numeric(b)) %>%
  select(b, percent_second_order) %>%
  rename(`Topic no.` = b) %>%
  mutate(percent_second_order = round(percent_second_order,3)) -> topics_second_order_percentages2

topics_table_to_paper %>%
  left_join(topics_second_order_percentages2) %>%
  DT::datatable()


```



### DBSCAN for long/lat data

In the case of epsilon equal to 50 kilometers range, DBSCAN found 78 clusters and 18 thousand noise points. After a quick visual inspection of the points colored by cluster association, I have concluded that the boundaries of the clusters overlap with bigger cities' boundaries, which proves that the clustering has discovered valuable information from the data. 

I have also tried running the clustering with the epsilon obtained from using the elbow method, namely 2. Only 2 clusters were created, of which one of them contained 99.97% of observations. The probable reason for that is that 2 would roughly translate to 500 kilometers of cluster range, which is a way too large radius to model information about the population density on a country level.

To the final dataset, I have added only one variable indicating whether the point belongs to the cluster or is one of the noise points. 


### PCA for demographic data

Cumulative variance explained by each of the consecutive loadings is presented on the plot .. . I have decided to leave the 10 most informative PCA eigenvectors. These account for 97.3% of the explained variance. 

TODO: Zostawić wykres czy wywalić??

```{r fig.width = 3, fig.height = 2}
load(file=here('run_all_models_cache/pca_to_plot.Rdata'))

ggplot(pca_to_plot, aes(x=PC, y=value)) +
  geom_line() +
  ggrepel::geom_text_repel(data = tibble(value = 0.973, PC=10), aes(label=value))+
  geom_point(data = tibble(value = 0.973, PC=10), color='red') +
  theme_minimal()

```


## Performance analysis

### Introduction (TODO)


#### AUC performance tables (XGB, LR)

```{r }
# Create auc-roc tables
for (i in models_names){
  # print(i)
  models_list[[i]]$auc_test <- calc_roc(models_list[[i]]$model, to_model_test)$auc
  models_list[[i]]$auc_train <- calc_roc(models_list[[i]]$model, to_model_train)$auc
}

models_list %>% 
  purrr::map_df(~as.numeric(.x$auc_test)) %>%
  pivot_longer(cols=everything(),values_to = 'AUC_test') -> auc_test_table

models_list %>% 
  purrr::map_df(~as.numeric(.x$auc_train)) %>%
  pivot_longer(cols=everything(),values_to = 'AUC_train') -> auc_train_table
```



```{r }
auc_test_table %>%
  left_join(auc_train_table) %>%
  arrange(-AUC_test) %>%
  mutate(AUC_perc_performance_drop = (AUC_test-max(AUC_test))/max(AUC_test)) %>%
  mutate_at(vars(AUC_test, AUC_train), function(x) sprintf('%.4f', x)) %>%
  mutate_at(vars(AUC_perc_performance_drop), function(x) sprintf('%.2f%%', x*100)) %>%
  flextable::flextable() %>%
  flextable::set_caption("AUC values for XGBoost model") %>%
  flextable_format

```

Table .. shows the performance of the XGB models using various sets of variables. The best AUC score on the test set is obtained by the model containing basic features combined with dummies indicating product categories that the customer has bought during the first purchase. AUC is greater than 0.5, which means that the model has predictive power better than random guessing. 

The second-best model is the one containing all variables, with demographic variables transformed with PCA. It is worth noticing that this model also contains the features containing product categories information, so similar performance is not a surprise. The percentage drop in AUC is very small (0.6%). The model with only basic information is worse for about 2.5%. 

The score of the subset of features selected by the Boruta algorithm using AUC on the test set is 0.646 - less than the model including all variables. This means that using the Boruta algorithm did not bring additional predictive power to the model. At the same time, this means that the variables indicating reviews topics seem to be relevant for the model performance. 

Another thing worth noticing is the fact that AUC on the train set is almost 1 in every model. These values are worrying because this means that the models are highly over-fitted, and that generalization problems can be present. The XGBoost model has some built-in parameters that can be used as regularization strategies, like the maximum tree depth of a single tree trained and a number of iterations. In search of a less over-fitted model, I have tweaked these parameters in cross-validation. However, although in some cases I was able to make the model overfit less, the performance in 2-fold cross-validation was still the best with highly over-fitted models.

I have also created a table .. containing similar information, but this time for the Logistic Regression model. The main finding is that even the best LR model (containing product categories and basic features) is worse than the worst XGBoost model (0.586 vs. 0.625, respectively). This means that linear modeling is in general very poorly suited for this prediction task. 


```{r }
# Zapis LR tabeli do pliku
# auc_test_table %>%
#   left_join(auc_train_table) %>%
#   arrange(-AUC_test) %>%
#   mutate(AUC_perc_performance_drop = (AUC_test-max(AUC_test))/max(AUC_test)) -> auc_table_for_lr
#   
# save(auc_table_for_lr, file='run_all_models_cache/auc_table_for_lr.Rdata')

load(file=here('run_all_models_cache/auc_table_for_lr.Rdata'))

auc_table_for_lr %>%
  mutate_at(vars(AUC_test, AUC_train), function(x) sprintf('%.4f', x)) %>%
  mutate_at(vars(AUC_perc_performance_drop), function(x) sprintf('%.2f%%', x*100)) %>%
  flextable::flextable() %>%
  flextable_format
```

AUC values for the test set oscillating below 0.6 mean that the model is very poorly fitted to the data. For the worst model containing only the agglomeration feature, it is at the value of 0.546. It is that close to the level of random classifier (0.5), that one could wonder if this model has any predictive power at all.

An interesting remark is that judging my AUC values, both LR and XGBoost select the same 2 models as the best ones - namely the one with product categories and with all variables. From the fact that 2 such different models arrived at the same conclusion in terms of which variables should be included, this means that these variables simply provide the biggest predictive power, regardless of the model used.

Comparison of performance for *agglomeration* set of features is particularly interesting. In the XGBoost model, this feature is rated as the 3rd best one (after excluding Boruta set to compare meaningfully with LR table). In the LR case, it is scored as the worst one. One possible explanation is that it's because of the inherent ability of XGBoost to create interactions between variables, while these interactions should be included in the LR model manually. 


From the perspective of CRM, the most important result of the modeling procedure is that the created model has predictive power in the task of churn prediction. This means that using the model's predictions the marketing department can understand which of the customers are most likely to place the second order and can be encouraged further. And on the other hand, which customers have a very low probability to buy, and thus the company can restrain from losing money on targeting them.

From the comparison of Logistic Regression and Extreme Gradient Boosting results, the latter one was shown to be superior over the other. Logistic Regression interpretability is an important feature that XGB lacks. 
Knowing what are the factors that make the customer more likely to stay is important not only for increasing trust about model predictions. Also, such information can serve as an important source of knowledge that can be passed to non-technical marketing workers in the company and potentially help them in their daily work.


#### Bootstraping AUC values

The AUC test value in the above table is shown for the whole test set. However, these point estimates cannot tell whether the performance would still be the same for a slightly different test set. This is especially crucial in the case of the 2 best models - as the difference in AUC values is so small that it is not clear if it is reproducible, or just obtained by random chance. 

A standard way to compare the models' performance is using a bootstrap technique to obtain different test sets. I have sampled with replacement observations from the test set and calculated the AUC measure for the models of choice. Specifically, I have done 100 re-sample rounds, and the models chosen were the best one (product categories + basic information), second-best (all variables), and the one with only basic information. The picture .. shows density estimates of these 3 empirical AUC score distributions. 

```{r cache=T, fig.width = 5, fig.height = 3}
bootstrap_auc_best = bootstrap_auc(models_list$product_categories$model, to_model_test,no_resamples = 100)
bootstrap_auc_2nd_best = bootstrap_auc(models_list$all_with_pca$model, to_model_test,no_resamples = 100)
bootstrap_auc_basic = bootstrap_auc(models_list$basic_info$model, to_model_test,no_resamples = 100)

rbind(
  tibble(AUC = bootstrap_auc_best, model = '1st best - product categories', mean_auc = mean(bootstrap_auc_best)),
  tibble(AUC = bootstrap_auc_2nd_best, model = '2nd best - all features', mean_auc = mean(bootstrap_auc_2nd_best)),
  tibble(AUC = bootstrap_auc_basic, model = 'Baseline - basic information', mean_auc = mean(bootstrap_auc_2nd_best))
) %>%
  ggplot(aes(x = AUC, fill = model)) +
  geom_density(alpha=0.5) +
  scale_x_continuous(breaks = seq(0.5, 0.7, 0.01)) +
  theme_minimal() +
  scale_fill_brewer(palette = 'Set1')

```

The curve for the model with basic features is standing out of the others. However, the difference between 1st best and 2nd best models is not as clear - it looks like the better model has slightly better AUC, but this should be investigated more thoroughly. That is why I have used a Kolmorogov-Smirnov test to check if the empirical distributions can come from the same probability distribution. I have run the test twice using 2 alternative hypotheses. First one with `H1: auc_best =/= auc_2nd_best`, and the second one: `H1: auc_best > auc_2nd_best`.

```{r}
ks_1 <- ks.test(bootstrap_auc_best, bootstrap_auc_2nd_best)
ks_2 <- ks.test(bootstrap_auc_best, bootstrap_auc_2nd_best,alternative = 'less')

# ks_1
# ks_2
```

The p-value for the first hypothesis is `r round(ks_1$p.value,4)` This means that with the level of significance 0.05, 0.01 the performance of the models is distinguishable. At the same time, p-value with 'greater' hypothesis is `r round(ks_2$p.value,4)`. This means that at the levels of significance 0.05, 0.01 one can say that the performance of the first model (only product categories) is better than that of the second one (all variables).

Another reason to choose the model for usage in the production setting is Occam's razor heuristic. The model with product categories has 21 variables, while the one with all variables included - 47. If there is no important reason why the more complex approach should be used, the simpler is usually better. In this case, using a simpler approach has the following advantages for the usage in the CRM context:

- Faster inference about the new customers - especially in an online prediction setting when the predictions have to be done on the fly
- The predictions are easier to interpret. Not only the amount of factors taken into account is lower, but also the predictions are made only on the basis of the features that can be directly obtained from the purchases data. On the contrary - variables containing topic information or demographic features are somehow biased and uncertain. In the case of the review topic data, it is because of the model imperfections, and in the case of demographic data because of too big generalization
- Easier model retraining and serving the final model in the IT infrastructure of the company.


```{r}
# ECDF - wybrać czy lepsze niż density
# rbind(
#   tibble(AUC = bootstrap_auc_best, model = '1st best - product categories', mean_auc = mean(bootstrap_auc_best)),
#   tibble(AUC = bootstrap_auc_2nd_best, model = '2nd best - all features', mean_auc = mean(bootstrap_auc_2nd_best)),
#   tibble(AUC = bootstrap_auc_basic, model = 'Baseline - basic information', mean_auc = mean(bootstrap_auc_2nd_best))
# ) %>%
#   ggplot(aes(x = AUC, fill = model, color=model)) +
#   stat_ecdf()
```

## Analysis of potential revenue - lift etc. 

### Lift analysis - introduction

TODO: to że prawdopodobieństwa z modelu i prawdopodobieństwa empiryczne z posortowanego rankingu klientów się rozjeżdżają wynika z tego że model jest nieskalibrowany - przewidywane prawdopodobieństwa nie są prawdopodobieństwami [link](https://towardsdatascience.com/pythons-predict-proba-doesn-t-actually-predict-probabilities-and-how-to-fix-it-f582c21d63fc). Nie wiem jak to opisać (i czy opisać) ??

An ultimate goal of customer churn prediction is gaining information, which customers are most likely to buy for the second time. From these insights, the CRM experts can make an informed decision which customers are the most likely to respond positively to targeting efforts. 

As it was stated earlier in this study, a suitable class of Machine Learning models for churn prediction are the ones that are outputting probabilities of the good result. From these, one can create a ranking of customers, in which they are sorted by their likelihood to buy for the second time. 

For each cumulative *part* of the ranking (top 1% of customers, top 10%, etc.), one can compute, which percentage of this part is truly buying for the second time. This type of approach is called a lift analysis and is a go-to tool for measuring the performance of targeting campaigns. Such information is also very easily understandable by CRM experts without deep knowledge of statistics and machine learning.

Potential usage of customer ranking based on probability to churn and lift analysis include (but are not limited to):

- One strategy of marketing targeting based on lift information is targeting the customers that are the most susceptible to buying for the second time. This strategy can however generate an unnecessary cost of trying to encourage the customers that would buy anyway. In an improved strategy, one can focus on the customers from the middle of the ranking.
- One can also vary the targeting strategy based on different cutoffs from the ranking. For example, don't do anything with the top 1%, send encouraging emails to the top 10%, send discount coupons to the top 20%. This can serve as a way to optimize the costs of more expensive targeting measures.
- It can serve as a baseline for various marketing tools in the paradigm of A/B testing - instead of computing the performance of the test group vs. the control group, one can replace the performance of the control group with a more informed guess about the performance, namely the probability obtained from the ranking list.
- Another interesting insight that can be obtained from a well-designed A/B testing experiment is how the conversion rate of each of the customers' groups changes after performing some action, for example sending an encouraging email or giving a discount for the next purchase. 
- A typical approach for judging the effectiveness of a targeting campaign is comparing the cost of a particular targeting effort with potential revenue that can be gained. Only the approaches that still give some positive results should be continued. One could expect that the bigger the probability output by the model, the more susceptible is the customer to be affected by targeting. This can be tested by using some targeting tools (e.g. giving discount) on a sample of customers selected from each quantile and compare the performance with the control group per quantile. (TODO: przeformułować ten podpunkt bo chyba jest trochę niejasny)

The way to approach the customers is however in the hands of the company's marketing experts. Only information about the ranking of customers' probabilities to buy for the second time is not enough to add value for the company. To this, know-how about the effectiveness of various marketing tools and cost analysis of each channel that the company can influence customer's choices is needed. Such information is often already available in the marketing departments, if not based on previous market research, then coming from the intuition of the experienced marketing department employees.

### Lift analysis

Later, the churn rates of both groups should be compared. *Lift* is defined as test group performance divided by control group performance. 

1. Calculate a proportion of the customers that have bought for the second time (share_second)
1. Split the dataset into 100 sub-datasets. The split is done on the basis of the quantile of the response from the model y_hat.
2. Calculate true response rate for each subset i (share_second_model_i)
3. Calculate lift value for each subset i (share_second_model_i/share_second)




```{r}
preds <- to_model_test
preds['prob'] <- predict(models_list$product_categories$model, to_model_test%>% select(-if_second_order), type='prob')['yes']

preds %>% 
  mutate(if_second_order = ifelse(if_second_order=='yes', 1, 0)) %>%
  select(if_second_order, prob) -> preds2

```


```{r }
score_randomly <- mean(preds2$if_second_order)
# score_randomly
```


Suppose the marketing campaign is meant to target 5% of the customers that have already made the first order. This corresponds to 1598 customers from the test set. If one would choose the customers to target randomly, on average in this group would be 3.3% of customers that would do the second order anyway. Using the output of the churn prediction, this score would be 17%, which means a performance gain of 416%. One should bear in mind that targeting the customers that would place the second order anyway is useless. However, because the model is able to predict if the customer will buy for the second time with higher accuracy, it can be assumed that it can also correctly predict the customers that are likely to "almost buy".

TODO: tutaj przydałaby się argumentacja z discrete choice. Zakładamy że wynik modelu (albo rankingu) to latent variable, a decyzja czy klient kupuje jest bazowana na jakimś progu. Wtedy kiedy wiemy że model poprawnie przewiduje tych którzy kupili, to możemy zakładać że poprawnie przewiduje też tych którzy "prawie" kupili ??


```{r }
# preds2 %>%
#   filter(prob>quantile(prob, 0.99)) %>%
#   summarise(score=mean(if_second_order),
#             score_randomly = score_randomly,
#             lift_over_randomly = (score-score_randomly)/score_randomly
#   )

output_lifts <- data.frame()

for (quant in seq(0.99, 0.01, -0.01)){
  # print(1-quant)
  
  preds2 %>%
    filter(prob>quantile(prob, quant)) %>%
    summarise(
      fraction_of_customers = 1-quant,
      no_customers_in_bin = n(),
      prob_cutoff = min(prob),
      score=mean(if_second_order),
      score_randomly = score_randomly,
      performance_gain_over_randomly = (score-score_randomly)/score_randomly,
      lift_over_randomly = score/score_randomly
    ) -> tmp
  
  output_lifts <- rbind(output_lifts, tmp)
  
}
```

#### Lift curve

```{r fig.width = 6, fig.height = 4}

# output_lifts %>%
#   ggplot(aes(x = fraction_of_customers, y= lift_over_randomly)) +
#   geom_line() +
#   geom_hline(yintercept = 1, color = 'red') +
#   labs(title = 'Lift curve') +
#   theme_minimal() 
# elbow method
  # geom_point(data = output_lifts %>%
  #                                filter(fraction_of_customers>0.08 & fraction_of_customers<0.089), 
  #                              
  #                              color = 'red', size = 3
  # ) +
  # ggrepel::geom_text_repel(data = output_lifts %>%
  #              filter(fraction_of_customers>0.08 & fraction_of_customers<0.089) %>%
  #              mutate(label = paste0('Lift:', round(lift_over_randomly,2))),aes(label=label))

quantiles_to_show2 <- c(0.99, 0.98, 0.97, 0.96, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.25, 0.01)

output_lifts %>%
  ggplot(aes(x = fraction_of_customers, y= lift_over_randomly)) +
  geom_line() +
  # geom_hline(yintercept = 1, color = 'red') +
  labs(title = 'Lift curve') +
  theme_minimal() +
  geom_point(data = output_lifts %>% 
               filter(fraction_of_customers %in% (1-quantiles_to_show2))
  ) +
  ggrepel::geom_text_repel(data =output_lifts %>% 
                             filter(fraction_of_customers %in% (1-quantiles_to_show2)), 
                           aes(label=sprintf('(%s: %s)', (1-quantiles_to_show2), round(lift_over_randomly,1)))) +
  scale_y_continuous(breaks = c(1,5,10,15,20),limits = c(1, NA)) 
  


```

On the plot .., a lift curve is presented. On the x-axis, the fraction of the *top* customers judging by probability to buy for the second time is presented. On the y axis, a lift value for this quantile is shown. 
The shape of the plot resembles the one of the function 1/x. Values of lift are very big for the smallest percentage of the best customers to target, and they are getting smaller very quickly. This means that the more customers the company would like to target based on the model prediction, the less marginal effects it would get from the usage of the model. For example, for the top 1% of the customers, the model can predict retention 18.7 times better than the random targeting approach. For the top 5%, it is still very effective, being 4.2 times better. However, the improvement for half of the customers is only 0.3. Although this value is less impressive, it is still an improvement over random targeting. 

## Model's working explanations

<!-- Despite superior performance in lots of machine learning tasks, the biggest drawback of Extreme Gradient Boosting models is its black-box nature, meaning lack of interpretability of predictions and parameters. This problem can be mitigated by usage of Explainable AI tools. One of such tools is ceteris-paribus analysis. The idea is to set all variables except the variable(s) of interest as constants, and create a new set of observations by manipulating only one or to variables. Such analysis can give a similar information to the values of coefficients in Logistic Regression models.  -->

<!-- #### Introduction -->

In this study, I have analyzed Logistic Regression and XGBoost algorithms for the task of churn prediction. Logistic Regression is an interpretable model by design - one can simply look at the model coefficients and infer about strength and direction of a particular feature influenced on the final prediction. However, as shown in the previous sections it is inferior to XGBoost with regards to predictions quality. XGB is a *black-box model*, meaning that its structure is too complex to be directly inspected. To be able to test hypotheses about importances and direction of influence for model prediction, XAI tools have to be used.

In this study, I have used 2 techniques of XAI, namely Variable Importance (VI) and Partial Dependence Profile (PDP). The first one can answer the question "which variables (or categories of variables) influence the predictions the most?", while the second - "What is the direction and strength of this influence?" 

### Influence of features groups on customer churn

In this study for the feature importance assessment, I have used a Permutation method [@dalex]. At first, the performance score (in this case AUC) is computed on the whole dataset. Then, each of the variables is randomly permuted one-by-one, and predictions are made using such a "blinded" variable. Later, the drop in model performance for each of the blinded features serves as its importance.

I have examined variable importances for two of the XGB models from this study - one for the best set of variables (with included product categories), and another - with all variables included. The reason to check variable importances also for the model with all features is that it can answer the questions about the significance of the particular sets of variables.

<!-- ##### Best model with unbinned and binned product categories -->

To the left of the figure \@ref(fig:varimp-categories) the unbinned variables importances from the XGB model for the best variables (with included product categories) are presented.
The most important one is the value of the payment. Also, high importance scores are obtained by vanilla geolocation variables and the transportation cost for the value. 
The review score of the purchase was not a very important variable.

```{r varimp-categories, fig.width = 7, fig.height=5,  fig.cap="Variable importance plots for the model with included product categories. Left subplot shows single variables, while right one - binned product categories and geolocation features."}

range01 <- function(x){(x-min(x))/(max(x)-min(x))}

plot_varimp <- function(df){
  df %>% 
    ggplot(aes(x = reorder(var2,Overall), y = Overall, label = round(Overall,2))) +
    geom_point(stat='identity', fill="black", size=3) +
    geom_segment(aes(y = 0,
                     x = var2,
                     yend = Overall,
                     xend = var2),
                 color = "black") +
    # geom_text(color="white", size=3) +
    # ylim(-2.5, 2.5) +
    coord_flip() +
    theme_minimal() +
    labs(
      x = 'Variable',
      y = 'Importance'
    ) +
    scale_y_continuous(minor_breaks = NULL)
}

var_imp_cat <- varImp(models_list$product_categories$model, scale=F)


# Not binned at all
var_imp_cat$importance %>%
  as_tibble(rownames = 'var2') %>%
  # mutate(Overall = range01(Overall)*100) %>%
  plot_varimp() -> pl_varimp_raw

# binned by category and geolocation

var_imp_cat$importance %>%
  as_tibble(rownames = 'var') %>%
  mutate(var2 = ifelse(str_starts(var, 'prod_cat_'), 'prod_categories', var)) %>%
  mutate(var2 = ifelse(str_starts(var, 'geolocation'), 'geolocation', var2)) %>%
  group_by(var2) %>%
  summarise(Overall = sum(Overall)) %>%
  # mutate(Overall = range01(Overall)*100) %>%
  as.data.frame() -> temp_df_imp_cat

pl_varimp_binned <- plot_varimp(temp_df_imp_cat)

gridExtra::grid.arrange(pl_varimp_raw, pl_varimp_binned, ncol=2, widths=c(1.4,1))

```

All of the dummies indicating product categories are in the latter part of the ranking. One could wonder, why despite these features are relatively unimportant variables, they lead to a 2.5% gain in AUC compared to the model without them.
This is because conceptually all of the dummies indicating product categories encode one information, these variables' importances should be treated jointly. The same can be argued about geographic coordinates. To account for this, I have simply summarised variables' importances to bin them into 2 variables sets "geolocation" and "prod_categories". This information is presented in the right subfigure. 
After this operation, it can be seen that product categories gained in relative importance - now they are the 4th variable. Also, the geolocation variables set became more important than payment value on its own.

<!-- ##### Model with all variables -->

In the figure \@ref(fig:varimp-all) variables' categories importances for the model with all variables are presented. I have grouped the variables into 3 categories:

- variables describing the first transaction of the customer - payment value, product categories, etc.
- variables describing perception - namely 1-5 review and dummies for a topic of the textual review
- "geo" variables - with 3 subgroups:
  - variables describing demographics of the region that the customer is in 
  - raw location - simply longitude/latitude coordinates
  - density - variable indicating whether the customer is in a densely populated area.

```{r varimp-all, fig.cap = "Variable importance plots for the model with all variables."}

var_imp_all <- varImp(models_list$all_with_pca$model, scale=F)

range01 <- function(x){(x-min(x))/(max(x)-min(x))}

var_imp_all$importance %>%
  as_tibble(rownames = 'var') %>%
  mutate(var2 = case_when(
    str_starts(var, 'spatial_') ~ 'geo - demographic',
    str_starts(var, 'geolocation_') ~ 'geo - raw location',
    str_starts(var, 'agglomeration') ~ 'geo - density',
    str_starts(var, 'topic_') ~ 'perception',
    str_starts(var, 'review_') ~ 'perception',
    str_starts(var, 'prod_cat_') ~ 'behavioural (first transaction)',
    var %in% c('payment_value', 'sum_freight', 'no_items', 'prod_categories') ~ 'behavioural (first transaction)',
    TRUE ~ var
  )) %>%
  group_by(var2) %>%
  summarise(Overall = sum(Overall)) %>%
  # mutate(Overall = range01(Overall)*100) %>%
  as.data.frame() -> temp_df_imp

plot_varimp(temp_df_imp)

```

The best set of variables is the one containing behavioral features. The next 2 sets, namely geodemographic and spatial location have a similar influence. The lowest impact on the model predictions have the perception variables and the density population indicator.

From these values, one can validate the hypotheses stated earlier. **Customer's propensity to churn depends on**:

- Payment value for the first order, number of items bought, transport cost of the package
- Categories of the products bought 
- Demographic environment of the customer
- Customer's location.

At the same time, customer's propensity to churn is not (or is only very mildly) influenced by the following factors:

- population density in the customer's area
- 1-5-star review of the purchase
- topic of the customer's textual review

In the following section, I have tried to answer the questions about the direction of the influence of predictors of customer churn.

### Direction and strength of features influence on customer churn

Partial Dependence Profile is based on the Cateris Paribus technique. This technique is meant to perform a "what if" analysis for one observation and one feature. For this observation, the variable of interest is changed and the model predicts the response for each of these changes. Partial Dependence Profile is simply averaged value of such Cateris Paribus analysis for each of the observations from the dataset. 

```{r cache=T}
explain <- DALEX::explain(model = models_list$product_categories$model,  
                          data = to_model_test %>% select(-if_second_order),
                          y = to_model_test$if_second_order == "yes", 
                          label = "Product categories",verbose = F)


to_model_test %>%
  select_if(is.numeric) %>%
  summarise_all(mean) %>%
  mutate_if(is.numeric, round, 3) -> mean_values

preds %>%
  filter(prob > quantile(prob, 0.9)) %>%
  select(-if_second_order, -prob) %>%
  summarise_all(mean) %>%
  mutate_if(is.numeric, round, 3) -> mean_values_top_decile


mean_model_response <- predict(models_list$product_categories$model, to_model_test,type = 'prob') %>% .$yes %>% mean

```

```{r }
predict_profile(explainer = explain, 
                new_observation = mean_values) -> profile_all
```


<!-- ##### Payment value -->

In the figure \@ref(fig:pdp-payment) Partial Dependence Profile for payment value is presented. The black line is the profile itself. To facilitate drawing conclusions from the PDP plot for payment value I have included a smoothing line (blue). Also, I have added vertical lines indicating quantiles of this variable's values in the dataset (red).

```{r pdp-payment, fig.cap='PDP plot for payment value of the purchase'}

model_profile(explainer = explain, variables = "payment_value") %>% .$agr_profiles -> profile_payment_value

to_model_test$payment_value %>%
  quantile(c(0.1, 0.25, 0.5, 0.75, 0.9)) %>%
  as_tibble(rownames = 'a') -> quantiles_payment

profile_payment_value %>%
  ggplot(aes(x = `_x_`, y = `_yhat_`)) +
  geom_line(size = 0.7) +
  scale_x_continuous(n.breaks=10,limits = c(0,500), minor_breaks = NULL) +
  geom_smooth(aes(y = `_yhat_`), alpha=0, color = RColorBrewer::brewer.pal(9, 'Set1')[2]) +
  theme_minimal() +
  labs(
    title=NULL, 
    subtitle = NULL, 
    caption=NULL,
    x = 'Payment value',
    y = 'Model response'
  ) +
  geom_vline(data = quantiles_payment, aes(xintercept = value), color = RColorBrewer::brewer.pal(9, 'Set1')[1]) +
  geom_text(data = quantiles_payment, aes(label = a, x = value, y = 0),angle = 90, nudge_x = -10)

```

Model response for payment value is non-monotonous. From an analysis of smoothed model response, one can say that it is increasing to the point of around 100. This means that on average, until the payment value of 100, the bigger the payment value, the model is predicting a bigger probability of placing a second order by the customer. After this threshold of 100, the probability to buy for the second time is falling slowly. 

<!-- TODO: dlaczego takie wyniki, czy potwierdza tezę -->

<!-- ##### No. items -->

On the figure \@ref(fig:pdp-items) similar PDP plot but for the number of items is presented. The relationship between the number of items bought in the first purchase and the probability of the second purchase is negative. One has to remember that in 80% of the orders there is only one product, while in 10% - 2 items. At the same time, the drop in the model's response between 1 and 2 items is not very abrupt, meaning that this relationship is hard to use in modeling.

```{r pdp-items, fig.cap="PDP plot for number of items in the customer's purchase"}

model_profile(explainer = explain, variables = "no_items") %>% .$agr_profiles -> profile_no_items

profile_no_items %>%
  ggplot(aes(x = `_x_`, y = `_yhat_`)) +
  geom_line(size = 0.7) +
  theme_minimal() +
  labs(
    title=NULL, 
       subtitle = NULL, 
       caption=NULL,
       x = 'Number of items',
       y = 'Model response'
       ) +
  scale_x_continuous(breaks=seq(1,6,1),minor_breaks = NULL, limits=c(NA, 6)) +
  scale_y_continuous(breaks = seq(0, 0.18, 0.02),minor_breaks = NULL,limits = c(0, NA))


```

For CRM, information about such a relationship can lead to the following trade-off. The more the customer buys in the first purchase, the bigger are the chances that they will not make a second purchase. This can have implications in cross-selling campaigns. The company can try to maximize the revenue from the first transaction by making the customer buy more, but then there is a bigger possibility that the customer will not make the second purchase. 

<!-- ##### Location -->

```{r }
# Predictions for geo data

predict_profile(explainer = explain, 
                new_observation = mean_values,variables = c('geolocation_lng', 'geolocation_lat')) -> profile_geo

profile_geo %>%
  as_tibble() %>%
  select(geolocation_lat) %>%
  distinct() %>%
  crossing(
    profile_geo %>% 
      select(geolocation_lng) %>%
      distinct()) %>%
  crossing(mean_values %>% select(-geolocation_lat, -geolocation_lng)) -> profile_geo

profile_geo$yhat <- predict(models_list$all_with_pca$model, profile_geo,type='prob')[,2]
```

```{r cache=F}
library(sf)
library(tmap)
library(readxl)
```


```{r}
# 2-d map ceteris paribus
profile_geo %>%
  select(geolocation_lat, geolocation_lng, yhat) %>%
  # sample_frac(0.7) %>%
  mutate(a = as.factor(as.numeric(cut(yhat, breaks=9)))) -> profile_geo_to_plot

# Bounding box
profile_geo_to_plot = profile_geo_to_plot[profile_geo_to_plot$geolocation_lat <= 5.27438888,]
profile_geo_to_plot = profile_geo_to_plot[profile_geo_to_plot$geolocation_lng >= -73.98283055,]
profile_geo_to_plot = profile_geo_to_plot[profile_geo_to_plot$geolocation_lat >= -33.75116944,]
profile_geo_to_plot = profile_geo_to_plot[profile_geo_to_plot$geolocation_lng <=  -34.79314722,]

tmap_mode("plot")
brazil_map <- brazilmaps::get_brmap("Brazil")
brazil_map_city <- brazilmaps::get_brmap("City")
# 
df <- sf::st_as_sf(profile_geo_to_plot, coords = c(2,1))
st_crs(df) <- st_crs(brazil_map)


brasil_cities_coords <- read_excel(here("data/brasil_cities_coords.xlsx"))
brasil_cities_coords <- brasil_cities_coords %>%
  select(city, lat, lng, population)


brasil_cities_coords <- sf::st_as_sf(brasil_cities_coords, coords = c(3,2))
st_crs(brasil_cities_coords) <- st_crs(brazil_map)


```

In the case of geolocation data, I have created a 2-d partial dependence profile and visualized it on the figure \@ref(fig:pdp-location). It can be seen that the predictions are the highest in two distinct large spots - one having its center close to Brasilia (new capital of the country), and the other one on the same latitude, but closer to the western country border. 

```{r pdp-location, fig.width = 5, fig.height = 3, fig.cap = "PDP plot for customer's location"}

tm_shape(brazil_map) +
  tm_polygons(col = "white") +
  tm_shape(df[which(lengths(st_within(df, brazil_map)) != 0), ]%>%rename(`Model response`=yhat)) +
  tm_symbols(size = 0.4,
             col = "Model response",
             # palette = RColorBrewer::brewer.pal(9, 'Greens'),
             border.lwd = NA,
             n = 10,
             alpha = 0.8) +
  tm_shape(brasil_cities_coords %>% arrange(-population) %>% head(10)) +
  tm_symbols(size = 0.2,
               col = "black",
               border.lwd = NA,
               alpha = 0.8)  +
  tm_text(text='city', just='top',size = 0.8)

```

The predictions form a visible pattern in stripes. As was noticed by @behrens2018spatial, it comes from the limitation of the model underlying the XGBoost method, that is decision trees. The vanilla decision tree algorithm works by partitioning the feature space on a discrete basis, and a typical output of that model on 2-d space is in the form of visible rectangles. And as XGBoost is consisting of stacked decision trees, the resulting partition pattern is a bit more complex, but still, decision-tree-typical artifacts are visible. 

 <!-- They are mostly located along the coast, except for Brasilia (new capital of the country) and Manaus (the biggest city in the Amazonia region). -->

<!-- ##### Review score  -->

One the plot \@ref(fig:pdp-review) PDP for review score is presented. Analysis of model responses in case of this variable should be treated with caution, as variable importance assessment showed it to be relatively non-important. However, I have still tried to analyze PDP plots, to check the expectation of this relationship being positive. 

```{r pdp-review, fig.cap="PDP plot for 1-5 review score"}

set.seed(13)
model_profile(explainer = explain, variables = "review_score") %>% .$agr_profiles -> profile_review_score

profile_review_score %>%
  ggplot(aes(x = `_x_`, y = `_yhat_`)) +
  geom_line(size = 0.7) +
  theme_minimal() +
  labs(
    title=NULL, 
       subtitle = NULL, 
       caption=NULL,
       x = 'Review score',
       y = 'Model response'
       ) +
  scale_x_continuous(minor_breaks = NULL) +
  scale_y_continuous(breaks = seq(0, 0.18, 0.02),minor_breaks = NULL,limits = c(0, NA))

```

The model response is relatively flat in reaction to changes in review score. For reviews 1 and 2, the response is not changing at all - meaning that it doesn't matter "how bad" the review is. Rather, that unsatisfied customers will not buy again in general. From 2 to 5 the model response expectedly monotonically increases. 

From the analysis of both the variable's importance plot and the PDP plot, one can conclude that customer satisfaction positively influences customer's propensity to buy again. However, the strength of this relationship is very weak compared to other variables present in the model.

<!-- However, unexpected behavior can be noticed when the tested review changes from 1 to 2 - the model response drops. This means that not the customers with the most negative reviews have the biggest probability to buy for the second time but rather the ones with "moderately negative". -->


## Śmieci/do appendixa

#### ROC curve 

TODO: dokładnie ta sama infromacja jest w lift curve. Pytanie czy zostawić roc curve ??

```{r}
roc_best_model <- calc_roc(models_list$product_categories$model, to_model_test)

tibble(
thre= roc_best_model$thresholds,
sens = roc_best_model$sensitivities,
spec = roc_best_model$specificities
) -> roc_tibble

roc_tibble %>%
  mutate(thre= round(thre,1)) %>%
  filter(thre %in% round(seq(0,1,0.1),1)) %>%
  group_by(thre) %>%
  slice_head(n=1) %>%
  ungroup() -> thresholds

roc_tibble %>%
  sample_frac(0.1) %>%
  ggplot(aes(x = spec, y=sens))+ 
  geom_line() +
  geom_point(data=thresholds, aes(x = spec, y = sens), color='red')+
  ggrepel::geom_text_repel(data=thresholds, aes(x = spec, y = sens, label=thre),hjust=0)+
  scale_x_continuous(trans='reverse') +
  geom_abline(slope=1, intercept=1, linetype='longdash') +
  labs(x = 'Specifity', y = 'Sensitivity') +
  theme_minimal()

```


#### Table with selected quantiles

TODO: wykres i tabela pokazują prawie te same informacje, tabelę może dać do appendixa??

```{r }
quantiles_to_show <- c(0.99, 0.98, 0.97, 0.96, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5)
output_lifts %>% 
  select(-score_randomly, -performance_gain_over_randomly) %>%
  filter(fraction_of_customers %in% (1-quantiles_to_show)) %>%
  mutate_if(is.numeric, round, 2) %>%
  mutate(fraction_of_customers = scales::percent(fraction_of_customers,accuracy = 1)) %>%
  flextable::flextable() %>%
  flextable_format
```


#### Cateris paribus review score

The shape of the ceteris paribus profile for the review score variable for the mean observations has an expected shape. The better the review, the it is probable for the client to purchase for the second time. What is also worth noticing is the fact that the probability increase is non-linear with respect to review score. The biggest probability increase is visible from the score 4 to 5. It is often the case in the customer reviews setting, observed also in the area of recommendation engines [(TODO: znaleźć lepsze cytowanie niż xkcd, z wykładu o recommenders)](https://xkcd.com/1098/). 




#### Break down of average prediction 

(TODO: opisać bardziej ogólnie metodę we wstępie i po co robić, na tą chwilę też opisy wyników są trochę nieczytelne, do poprawy)

Variable Attributions via Sequential Variable Conditioning - algorithm [@dalex]:

For breakdown of one particular observation x_1:

1. Calculate prediction for all of the observations and calculate mean. This value is conceptually equivalent to intercept in Linear Regression
2. Change value of one variable in all observations and set it as the one from observation x_1
3. Calculate prediction and check how much it has changed from the value obtained from intercept
4. Repeat for the rest of the variables. 

Because the variables are set to a constant sequentially, the recommended algorithm is to perform a greedy search for the variable that once set constant results in the biggest change in prediction. This approach however does not facilitate comparisons between the plots as the order of the variables will change. That is why I have decided to have a fixed order of the variables, with variable importance as the main criterium. 

```{r cache=T, fig.width = 6, fig.height = 3}
varImp(models_list$product_categories$model)$importance %>%
  head(10) %>%
  rownames() -> most_important_vars



bd_rf <- predict_parts(explainer = explain,
                       new_observation = mean_values,
                       order = most_important_vars,
                       type = "break_down")
plot(bd_rf)
```

On the plot .. such break down additive plot is presented. The way to interpret it is as follows:

1. Obtain prediction for all of the observations. Calculate the mean prediction (*intercept*), in this case 0.156. 
2. For all observations in the dataset, set the value of the variable *payment_value* to 175. This is the value from the mean observation.
3. Obtain prediction for all of the observations. Calculate the mean prediction and substract it from the *intercept*. In this case the change is -0.016. This means that *payment_value* equal to the value from the mean observation lowers the average probability that the customer will buy second time.
4. Select next variable, (*geolocation_lat*) and repeat the process. In this case the predicted probability drops even further, to 0.101 (0.156-0.016-0.039)
5. Go through all variables. The last value on the plot, *prediction*, contains true prediction for the mean observation.

One interesting remark from this plot is that the low response of the model does not come from the fact that all variables influence it negatively. Rather, that for the variables that increase the response, there are other variables that lower the response for a similar amount and that the influences cancel out. 

The prediction for the average customer at the probability is very low (0.07). This is caused by the fact that there is big imbalance in the data, and in the test dataset used for analyzing the model results there is no imbalance correction applied. From the perspective of marketing targeting more important is gaining some knowledge not about an average customer, but the ones that are the most probable to buy next time. That is why I have also created a breakdown plot for the observation that is scored as the most probable to buy second time. 


```{r fig.width = 6, fig.height = 3}
preds[c(models_list$product_categories$vars_used, 'prob')] %>%
  mutate(if_second_order = to_model_test$if_second_order) %>%
  filter(if_second_order == 'yes') %>%
  arrange(-prob) %>%
  head(1) %>%
  predict_parts(explainer = explain,
                new_observation = .,
                order = most_important_vars,
                type = "break_down") %>%
  plot() 

```

In the case of the prediction for the best customer, almost all variables influence the prediction positively. The only variable doing otherwise is payment value. The influence is however minor compared to the other variables. This negative influence can be analyzed in connection with ceteris-paribus plot for that variable analyzed before. On the plot ... it can be seen that the model response has a small drop for the values around 500. 

The rest of the variables influence the score positively. The biggest gain is from the value of the transport, however number of items equal to 6 has also a large influence. 


### Hypotheses to check

- Main hypothesis - machine learning modeling can be helpful in ranking customers by their propensity to churn
- I have tested 2 hypotheses. (1) The amount of money spent on the first purchase positively influences the customer probability of buying for the second time. (2) categories bought by the customer can influence the customer's probability to stay with the company.
- A hypothesis worth checking is if a tendency to churn is dependent on whether the customer is living in a densely populated area.
- A hypothesis I would like to check is if the social structure of the customer’s environment can serve as a valuable predictor of churn tendency.
- My hypothesis is that both the numeric review, as well as topic of the textual review can be useful predictors of customer loyalty.

Jakie analizy:

- AUC comparison in table, bootstrapping AUC (if ML is able to discriminate non-churners, if model choice is justified)
- Lift curve (how good is ML model able to discriminate non-churners (better interpretation))
- Variable importances (if variables have predictive power)
- cateris paribus (what is the direction and strength of influence of variables on prediction)
- break down of prediction (???)


### Questions answered

2 parts of results, pre-modeling and after-modeling

Pre-modeling:

- Topic extraction from the reviews
  - CRM: use in daily dashboards - monitoring of new reviews, monitoring of sentiment based on the topic, customer segmentation, direct marketing (send message based on the review topic)
  - Methods: attention-based extraction shown superior to classic LDA approach in this domain
- DBSCAN on lon/lat, PCA on demographic
  - methodological: potential impact on model performance, shown by previous literature to be useful, reduction of dimensionality succesful (demographic+PCA better than demographic, lot of variance explained)
  - CRM: nie ma zbyt zastosowania
  - TODO przenieść dbscan i pca do methods description? bo bardzo krótkie i niewiele wnoszące rezultaty

After-modeling

- OK Model performance- AUC values for all models
  - methodological:
  - which ML model suits the task better (XGB)
  - which variables give the best score
  - CRM:
  - Most important: the model has predictive power
  - which parts of the dataset should be observed after putting model to production (drift analysis)
- OK Model performance- Bootstrap, K-S test, density plots
  - Methodological: Another tool for the best model search (complementing AUC on test set)
  - no importance for CRM
- Model performance- Lift curve, lift analysis
  - Methodological: a way to show how the model performs in a way that is understandable by non-technical adopters, a way of quantifying a potential revenue from using the model
  - OK CRM: a way to enhance targeting strategies (which customers should be more/less targeted based on apriori probability of buying second time), a more meaningful baseline to assessing A/B testing strategies (probability to buy from the model vs. share of customers that bought)
  - Customer Lifetime Value for new customers - probability to buy second time from the model \* average buy second time
  - OK Model performs better for almost all the cohorts of probabilities
  - OK segmentation+targeting - a way to reduce customer aquisition cost
  - OK Segmenation based on probability to buy second time
- XAI on model level (VI plots, cateris paribus plots)
- XAI on customer level (break-down plots)

- CRM:
  - a way to verify if the intuition and know-how present in the marketing department is compliant with the one that the model used to do predictions
  - in the case of complaints about model prediction a way to assess why the model did particular decision
  - increase of trust in the model from non-technical part of the company, better comunication between business and IT
- Methodology - że może poprawiać przewidywania modelu, że można testować hipotezy (TODO jak już wymyślę hipotezy do przetestowania)





```{r }
# RANDOM

# preds[c(models_list$product_categories$vars_used, 'prob')] %>%
#   arrange(-prob) %>%
#   select(prob, everything()) %>%
#   distinct() %>%
#   head(10)
```

```{r }
# break
# preds2 %>%
#   arrange(-prob) %>%
#   mutate(bin = floor(row_number()/1000)) %>%
#   group_by(bin) %>%
#   summarise(perc_second = mean(if_second_order)) %>%
#   View()
# 
# 
# preds2 %>%
#   filter(prob>quantile(prob, 0.7)) %>%
#   summarise(no_people_second=mean(if_second_order),
#             perc_second = (mean(if_second_order-0.0344)/0.0344)
#   )
# # przy "wyślij do 30% klientów" o 65% lepsze wyniki
# 
# preds2 %>%
#   filter(prob>quantile(prob, 0.9)) %>%
#   summarise(no_people_second=mean(if_second_order),
#             perc_second = (mean(if_second_order-0.0344)/0.0344)
#   )
# # a przy "wyślij do 10% klientów" o 144% 
# 
# 
# cp2d <- ingredients::ceteris_paribus_2d(explainer = explain_topic,observation = mean_values,
#                                         variables = c('geolocation_lng', 'geolocation_lat'))
# 
# cp2d %>%
#   ggplot(aes(x=geolocation_lat, y = geolocation_lng, fill=y_hat)) +
#   geom_raster()
# 
# 
# fifa_mp_gbm_deep <- model_parts(explain)
# plot(fifa_mp_gbm_deep, max_vars = 20, 
#      bar_width = 4, show_boxplots = FALSE) 
# 
```

