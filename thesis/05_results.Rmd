---
title: "Results analysis"
output: html_document
# output: bookdown::word_document2
# TODO: zamienić tabelke z tematami na output nie-html, pozniej usunac ten agrument
always_allow_html: true
params:
  echo: False
---

TODO: *W części 05 trzeba do każdej analizy dać dwa typy opisów: metodologiczne (co wyszło, co nie wyszło etc) i marketingowe (co to znaczy dla CRM).*


```{r include=FALSE}
knitr::opts_chunk$set(message = F, warning = F, echo=params$echo, fig.width = 6, fig.height = 4, cache = T)
```

```{r cache=F}
library(readr)
library(tidyverse)
library("leaflet")
library(psych)
library(lubridate)
library(cluster)
library(factoextra)
library(caret)
library(rpart)
library(DALEX)
library(stringr)
library(here)
```


```{r cache=T}
# Create functions for calculating AUC etc.
bootstrap_auc <- function(model, test_set, no_resamples){
  out_roc <- vector('numeric', no_resamples)
  
  len_test <- nrow(test_set)
  for (i in 1:no_resamples){
    set.seed(i)
    idxes <- sample(1:len_test, size = len_test, replace = T)
    temp_test <- test_set[idxes,]
    
    predictions_temp = predict(model, temp_test,type = 'prob')
    roc_temp  <- pROC::roc(as.numeric(temp_test$if_second_order == "yes"), 
                           predictions_temp[, 1])
    out_roc[i] <- roc_temp$auc
    i <- i+1
    
  }
  out_roc
}

bootstrap_summary<- function(out_roc){
  
  tibble(auc =out_roc) %>%
    summary %>%
    print
  
  mean_auc <- mean(out_roc)
  tibble(auc =out_roc, y=0) %>%
    ggplot(aes(x=auc)) +
    geom_density() +
    geom_jitter(aes(y=y), alpha=0.5) +
    geom_vline(xintercept = mean_auc, color = 'red')
  
}


calc_metrics <- function(model, to_model_test, run_confusion_matrix = F){
  predictions1 <- predict(model, to_model_test,type = 'prob')
  roc_test1  <- pROC::roc(as.numeric(to_model_test$if_second_order == "yes"), 
                          predictions1[, 1])
  
  plot(roc_test1) 
  title('ROC curve on test set \n')
  print('Calc AUC on test set:')
  print(roc_test1)
  
  if(run_confusion_matrix){
    confusionMatrix(data = as.factor(as.numeric(predictions1[, 2] > 0.5)), # probability of yes was more than
                    reference = as.factor(as.numeric(to_model_test$if_second_order == "yes")),
                    # definitions of the "success" label
                    positive = '1') 
  }
  # print('Quantiles of predicted responses:')
  # print('-----')
  # predictions1[,2] %>%
  #   quantile(seq(0.8,1, 0.01))
}

calc_roc <- function(model, to_model_test, run_confusion_matrix = F){
  predictions1 <- predict(model, to_model_test,type = 'prob')
  roc_test1  <- pROC::roc(as.numeric(to_model_test$if_second_order == "yes"), 
                          predictions1[, 1])
  return(roc_test1)
}
```


```{r }
# Load the models and training data - only XGB, LR is too big for now

files_cache <- list.files(here('run_all_models_cache/'))
files_names_models <- files_cache[str_starts(files_cache, 'saved_model')]
files_names_models <- files_names_models[!str_starts(files_names_models, 'saved_model_lr')]
files_names_models <- files_names_models[!str_starts(files_names_models, 'saved_model_boruta_all_with_pca')]
# files_names_models


load(here('run_all_models_cache/to_model_train.Rdata'))
load(here('run_all_models_cache/to_model_test.Rdata'))

models_list <- vector('list', length(files_names_models))

for (i in 1:length(files_names_models)){
  # print(i)
  name <- files_names_models[i]
  temp_env <- new.env()
  load(here(paste0('run_all_models_cache/',name)),envir = temp_env)
  models_list[[i]] <- temp_env$model_spec
}

models_names <- c()
for (i in models_list){
  models_names <- c(models_names, i$model_name)
}

names(models_list) <- models_names
```

## Questions answered

2 parts of results, pre-modeling and after-modeling

Pre-modeling:

- Topic extraction from the reviews
  - CRM: use in daily dashboards - monitoring of new reviews, monitoring of sentiment based on the topic, customer segmentation, direct marketing (send message based on the review topic)
  - Methods: attention-based extraction shown superior to classic LDA approach in this domain
- DBSCAN on lon/lat, PCA on demographic
  - methodological: potential impact on model performance, shown by previous literature to be useful, reduction of dimensionality succesful (demographic+PCA better than demographic, lot of variance explained)
  - CRM: nie ma zbyt zastosowania
  - TODO przenieść dbscan i pca do methods description? bo bardzo krótkie i niewiele wnoszące rezultaty

After-modeling

- OK Model performance- AUC values for all models
  - methodological:
  - which ML model suits the task better (XGB)
  - which variables give the best score
  - CRM:
  - Most important: the model has predictive power
  - which parts of the dataset should be observed after putting model to production (drift analysis)
- OK Model performance- Bootstrap, K-S test, density plots
  - Methodological: Another tool for the best model search (complementing AUC on test set)
  - no importance for CRM
- Model performance- Lift curve, lift analysis
  - Methodological: a way to show how the model performs in a way that is understandable by non-technical adopters, a way of quantifying a potential revenue from using the model
  - OK CRM: a way to enhance targeting strategies (which customers should be more/less targeted based on apriori probability of buying second time), a more meaningful baseline to assessing A/B testing strategies (probability to buy from the model vs. share of customers that bought)
  - Customer Lifetime Value for new customers - probability to buy second time from the model \* average buy second time 
  - OK Model performs better for almost all the cohorts of probabilities
  - OK segmentation+targeting - a way to reduce customer aquisition cost
  - OK Segmenation based on probability to buy second time
- XAI on model level (VI plots, cateris paribus plots)
- XAI on customer level (break-down plots)

- CRM:
  - a way to verify if the intuition and know-how present in the marketing department is compliant with the one that the model used to do predictions
  - in the case of complaints about model prediction a way to assess why the model did particular decision
  - increase of trust in the model from non-technical part of the company, better comunication between business and IT
- Methodology - że może poprawiać przewidywania modelu, że można testować hipotezy (TODO jak już wymyślę hipotezy do przetestowania)

## Results of the pre-modeling phase

### Boruta feature selection

The Boruta algorithm concluded that from all 47 variables only the 14 variables indicating topics are non-relevant. One should notice that even using automatic feature selection, the algorithm has dropped the whole category of variables, meaning that the approach of manually setting sets of variables to include in the model is also "recommended" by the algorithm. 

### Topic modeling (TODO)

TODO: that only one model produced meaningful results (Description of the table czy tabelka do appendixa??)

```{r}

topics_table_to_paper <- read_csv(here('topics_table_to_paper.csv')) %>%
  mutate(`Example reviews` = str_replace_all(`Example reviews`, '\\n', '||||'))

load(here('run_all_models_cache/topics_second_order_percentages.Rdata'))

topics_second_order_percentages %>%
  filter(topic != 'topic_nan') %>%
  separate(topic, c('a', 'b'), '_') %>%
  mutate(b = as.numeric(b)) %>%
  select(b, percent_second_order) %>%
  rename(`Topic no.` = b) %>%
  mutate(percent_second_order = round(percent_second_order,3)) -> topics_second_order_percentages2

topics_table_to_paper %>%
  left_join(topics_second_order_percentages2) %>%
  DT::datatable()


```



### DBSCAN for long/lat data

In the case of epsilon equal to 50 kilometers range, DBSCAN found 78 clusters, and 18 thousand noise points. After a quick visual inspection of the points colored by cluster association, I have concluded that the clusters boundaries overlap with bigger cities boundaries, which proves that the clustering has discovered a valuable information from the data. 

I have also tried running the clustering with the epsilon obtained from using the elbow method, namely 2. Only 2 clusters were created, of which one of them contaied 99.97% of observations. The probable reason for that is that 2 would roughly translate to 500 kilometers of cluster range, which is way to large radius to model information about the population density on a country-level.

To the final dataset I have added only one variable indicating whether the point belongs to the cluster or is one of the noise points. 


### PCA for demographic data

Cumulative variance explained by each of the consecutive loadings is presented on the plot .. . I have decided to leave the 10 most informative PCA eigenvectors. These account for 97.3% of the explained variance. 

TODO: Zostawić wykres czy wywalić??

```{r}
load(file=here('run_all_models_cache/pca_to_plot.Rdata'))

ggplot(pca_to_plot, aes(x=PC, y=value)) +
  geom_line() +
  ggrepel::geom_text_repel(data = tibble(value = 0.973, PC=10), aes(label=value))+
  geom_point(data = tibble(value = 0.973, PC=10), color='red')

```


## Performance analysis

### Introduction (TODO)


### AUC performance tables (XGB, LR)

```{r }
# Create auc-roc tables
for (i in models_names){
  # print(i)
  models_list[[i]]$auc_test <- calc_roc(models_list[[i]]$model, to_model_test)$auc
  models_list[[i]]$auc_train <- calc_roc(models_list[[i]]$model, to_model_train)$auc
}

models_list %>% 
  purrr::map_df(~as.numeric(.x$auc_test)) %>%
  pivot_longer(cols=everything(),values_to = 'AUC_test') -> auc_test_table

models_list %>% 
  purrr::map_df(~as.numeric(.x$auc_train)) %>%
  pivot_longer(cols=everything(),values_to = 'AUC_train') -> auc_train_table
```



```{r }
auc_test_table %>%
  left_join(auc_train_table) %>%
  arrange(-AUC_test) %>%
  mutate(AUC_perc_performance_drop = (AUC_test-max(AUC_test))/max(AUC_test)) %>%
  flextable::flextable()

```

Table .. shows the performance of the XGB models using various sets of variables. The best AUC score on the test set is obtained by the model containing basic features combined with dummies indicating product categories that the customer has bought during the first purchase. AUC is greater than 0.5, which means that the model has predictive power better than random guessing. 

Second best model is the one containing all variables, with demographic variables transformed with PCA. It is worth noticing that this model also contains the features containing product categories information, so similar performance is not a surprise. The percentage drop in AUC is very small (0.6%). The model with only basic information is worse for about 2.5%. 

The score of the subset of features selected by Boruta algorithm using AUC on the test set is 0.646 - less than the model including all variables. This means that using Boruta algorithm did not bring an additional predictive power to the model. At the same time, this means that the variables indicating reviews topics seem to be relevant for the model performance. 


Another thing worth noticing is the fact that AUC on the train set is almost 1 in every model. These values are worrying because this means that the models are highly over-fitted, and that generalization problems can be present. The XGBoost model has some built-in parameters that can be used as a regularization strategies, like the maximum tree depth of single tree trained and number of iterations. In search for a less over-fitted model I have tweaked these parameters in cross-validation. However, although in some cases I was able to make the model overfit less, the performance in 2-fold cross-validation was still the best with a highly over-fitted models.

I have also created a table .. containing similar information, but this time for Logistic Regression model. Main finding is that even the best LR model (containing product categories and basic features) is worse than the worst XGBoost model (0.586 vs. 0.625, respectively). This means that linear modeling is in general very poorly suited for this prediction task. 


```{r }
# Zapis LR tabeli do pliku
# auc_test_table %>%
#   left_join(auc_train_table) %>%
#   arrange(-AUC_test) %>%
#   mutate(AUC_perc_performance_drop = (AUC_test-max(AUC_test))/max(AUC_test)) -> auc_table_for_lr
#   
# save(auc_table_for_lr, file='run_all_models_cache/auc_table_for_lr.Rdata')

load(file=here('run_all_models_cache/auc_table_for_lr.Rdata'))

auc_table_for_lr %>%
  flextable::flextable()
```

AUC values for the test set oscillating below 0.6 mean that the model is very poorly fitting to the data. For the worst model containing only agglomeration feature it is at the value of 0.546. It is that close to the level of random classifier (0.5), that one could wonder if this model has any predictive power at all.

Interesting remark is that judging my AUC values, both LR and XGBoost select the same 2 models as the best ones - namely the one with product categories and with all variables. From the fact that 2 such different models arrived to the same conclusion in terms of which variables should be included, this means that these variables simply provide the biggest predictive power, regardless of the model used.

Comparison of performance for *agglomeration* set of features is particularly interesting. In XGBoost model this feature is rated as the 3rd best one (after excluding Boruta set to compare meaningfully with LR table). In LR case it is scored as the worst one. One possible explanation is that it's because of inherent ability of XGBoost to create interactions between variables, while these interactions should be included in LR model manually. 


From the perspective of CRM, the most important result of the modeling procedure is that the created model has predictive power in the task of churn prediction. This means that using model's predictions the marketing department can understand which of the customers are most likely to place the second order and can be encouraged further. And on the other hand, which customers have a very low probability to buy, and thus the company can restrain from losing money on targeting them.

From the comparison of Logistic Regression and Extreme Gradient Boosting results, the latter one was shown to be superior over the other. Logistic Regression interpretability is an important feature that XGB lacks. 
Knowing what are the factors that make the customer more likely to stay is important not only for increasing trust about model predictions. Also, such information can serve as an important source of knowledge that can be passed to non-technical marketing workers in the company and potentially help them in their daily work.


- TODO: which parts of the dataset should be observed after putting model to production (drift analysis)


#### Bootstraping AUC values

The AUC test value in the above table is shown for the whole test set. However, these point estimates cannot tell whether the performance would still be the same for sightly different test set. This is especially crucial in the case of the 2 best models - as the difference in AUC values is so small that it is not clear if it is reproducible, or just obtained by random chance. 

A standard way to compare the models performance is using bootstrap technique to obtain different test sets. I have sampled with replacement observations from the test set and calculated AUC measure for the models of choice. Specifically, I have done 100 re-sample rounds, and the models chosen were the best one (product categories + basic information), second best (all variables), and the one with only basic information. The picture .. shows density estimates of these 3 empirical AUC score distributions. 

```{r cache=T}
bootstrap_auc_best = bootstrap_auc(models_list$product_categories$model, to_model_test,no_resamples = 100)
bootstrap_auc_2nd_best = bootstrap_auc(models_list$all_with_pca$model, to_model_test,no_resamples = 100)
bootstrap_auc_basic = bootstrap_auc(models_list$basic_info$model, to_model_test,no_resamples = 100)

rbind(
  tibble(AUC = bootstrap_auc_best, model = '1st best - product categories', mean_auc = mean(bootstrap_auc_best)),
  tibble(AUC = bootstrap_auc_2nd_best, model = '2nd best - all features', mean_auc = mean(bootstrap_auc_2nd_best)),
  tibble(AUC = bootstrap_auc_basic, model = 'Baseline - basic information', mean_auc = mean(bootstrap_auc_2nd_best))
) %>%
  ggplot(aes(x = AUC, fill = model)) +
  geom_density(alpha=0.5) +
  scale_x_continuous(breaks = seq(0.5, 0.7, 0.01)) +
  theme_minimal()

```

The curve for the model with basic features is standing out of the others. However, the difference between 1st best and 2nd best models is not as clear - it looks like the better model has slightly better AUC, but this should be investigated more thoroughly. That is why I have used Kolmorogov-Smirnov test to check if the empirical distributions can come from the same probability distribution. I have run the test twice using 2 alternative hypotheses. First one with `H1: auc_best =/= auc_2nd_best`, and the second one: `H1: auc_best > auc_2nd_best`.

```{r}
ks_1 <- ks.test(bootstrap_auc_best, bootstrap_auc_2nd_best)
ks_2 <- ks.test(bootstrap_auc_best, bootstrap_auc_2nd_best,alternative = 'less')

ks_1
ks_2
```

The p-value for the first hypothesis is `r round(ks_1$p.value,4)` This means that with the level of significance 0.05, 0.01 the performance of the models is distinguishable. At the same time, p-value with 'greater' hypothesis is `r round(ks_2$p.value,4)`. This means that at the levels of significance 0.05, 0.01 one can say that the performance of the first model (only product categories) is better than of the second one (all variables).

Another reason to choose the model for usage in in production setting is Occam's razor heuristic. The model with product categories has 21 variables, while the one with all variables included - 47. If there is no an important reason why the more complex approach should be used, the simpler is usually better. In this case, using simpler approach has the following advantages for the usage in CRM context:

- Faster inference about the new customers - especially in an online prediction setting when the predictions have to be done on the fly
- The predictions are easier to interpret. Not only the amount of factors taken into account is lower, but also the predictions are made only on the basis of the features that can be directly obtained from the purchase data. On the contrary - variables containing topic information or demographic features are somehow biased and uncertain. In the case of the review topic data it is because of the model imperfections, and in the case of demographic data because of too big generalization
- Easier model retraining and serving the final model in the IT infrastructure of the company.


```{r}
# ECDF - wybrać czy lepsze niż density
# rbind(
#   tibble(AUC = bootstrap_auc_best, model = '1st best - product categories', mean_auc = mean(bootstrap_auc_best)),
#   tibble(AUC = bootstrap_auc_2nd_best, model = '2nd best - all features', mean_auc = mean(bootstrap_auc_2nd_best)),
#   tibble(AUC = bootstrap_auc_basic, model = 'Baseline - basic information', mean_auc = mean(bootstrap_auc_2nd_best))
# ) %>%
#   ggplot(aes(x = AUC, fill = model, color=model)) +
#   stat_ecdf()
```

#### ROC curve 

TODO: dokładnie ta sama infromacja jest w lift curve. Pytanie czy zostawić roc curve ??

```{r}
roc_best_model <- calc_roc(models_list$product_categories$model, to_model_test)

tibble(
thre= roc_best_model$thresholds,
sens = roc_best_model$sensitivities,
spec = roc_best_model$specificities
) -> roc_tibble

roc_tibble %>%
  mutate(thre= round(thre,1)) %>%
  filter(thre %in% round(seq(0,1,0.1),1)) %>%
  group_by(thre) %>%
  slice_head(n=1) %>%
  ungroup() -> thresholds

roc_tibble %>%
  sample_frac(0.1) %>%
  ggplot(aes(x = spec, y=sens))+ 
  geom_line() +
  geom_point(data=thresholds, aes(x = spec, y = sens), color='red')+
  ggrepel::geom_text_repel(data=thresholds, aes(x = spec, y = sens, label=thre),hjust=0)+
  scale_x_continuous(trans='reverse') +
  geom_abline(slope=1, intercept=1, linetype='longdash') +
  labs(x = 'Specifity', y = 'Sensitivity') +
  theme_minimal()

```



## Analysis of potential revenue - lift etc. 

### Lift analysis - introduction

TODO: to że prawdopodobieństwa z modelu i prawdopodobieństwa empiryczne z posortowanego rankingu klientów się rozjeżdżają wynika z tego że model jest nieskalibrowany - przewidywane prawdopodobieństwa nie są prawdopodobieństwami [link](https://towardsdatascience.com/pythons-predict-proba-doesn-t-actually-predict-probabilities-and-how-to-fix-it-f582c21d63fc). Nie wiem jak to opisać (i czy opisać) ??

An ultimate goal of customer churn prediction is gaining information, which customers are most likely to buy for the second time. From these insights, the CRM experts can make an informed decision which customers are the most likely to respond positively to targeting efforts. 

As it was stated earlier in this study, a suitable class of Machine Learning models for churn prediction are the ones that are outputing probabilities of the good result. From these, one can create a ranking of customers, in which they are sorted by their likelihood to buy second time. 

For each cumulative *part* of the ranking (top 1% of customers, top 10% etc.), one can compute, which percentage of this part is truly buying for the second time. This type of approach is called lift analysis, and is a go-to tool for measuring the performance of targeting campaigns. Such information is also very easily understandable by the CRM experts without deep knowledge of statistics and machine learning.

A potential usages of customer ranking based on probability to churn and lift analysis include (but are not limited to):

- One strategy of marketing targeting based on lift information is targeting the customers that are the most susceptible to buying second time. This strategy can however generate unnecessary cost of trying to encourage the customers that would buy anyway. In an improved strategy, one can focus on the customers from the middle of the ranking.
- One can also vary the targeting strategy based on different cutoffs from the ranking. For example don't do anything with top 1%, send encouraging email to top 10%, send discount to top 20%. This can serve as a way to optimize costs of more expensive targeting measures.
- It can serve as a baseline for various marketing tools in the paradigm of A/B testing - instead of computing the performance of the test group vs. control group, one can replace the performance of control group with more informed guess about the performance, namely the probability obtained from the ranking list.
- Another interesting insight that can be obtained from a well designed A/B testing experiment is the how the conversion rate of each of the customers group changes after performing some action, for example sending an encouraging email or giving a discount for the next purchase. 
- A typical approach for judging the effectivity of a targeting campaign is comparing cost of particular targeting effort with potential revenue that can be gained. Only the approaches that still give some positive results should be continued. One could expect that the bigger probability outputed by the model, the more susceptible is the customer to be affected by targeting. This can be tested by using some targeting tools (e.g. gving discount) on a sample of customers selected from each quantile, and compare the performance with the control group per quantile. (TODO: przeformułować ten podpunkt bo chyba jest trochę niejasny)

The way to approach the customers is however in the hands of company's marketing experts. Only information about the ranking of customers probabilities to buy second time is not enough to add value for the company. To this, know-how about effectiveness of various marketing tools and cost analysis of each channels that the company can influence customer's choices is needed. Such information is often already available in the marketing departments, if not based on previous market research, then coming from the intuition of the experienced marketing department employees.

TODO: - Customer Lifetime Value for new customers - probability to buy second time from the model \* average buy second time 

### Lift analysis

Later, churn rates of both groups should be compared. *Lift* is defined as test group performance divided by control group performance. 

1. Calculate proportion of the customers that have bought for the second time (share_second)
1. Split the dataset into 100 sub-datasets. The split is done on the basis of quantile of the response from the model y_hat.
2. Calculate true response rate for each subset i (share_second_model_i)
3. Calculate lift value for each subset i (share_second_model_i/share_second)




```{r}
preds <- to_model_test
preds['prob'] <- predict(models_list$product_categories$model, to_model_test%>% select(-if_second_order), type='prob')['yes']

preds %>% 
  mutate(if_second_order = ifelse(if_second_order=='yes', 1, 0)) %>%
  select(if_second_order, prob) -> preds2

```


```{r }
score_randomly <- mean(preds2$if_second_order)
# score_randomly
```


Suppose the marketing campaign is meant to target 5% of the customers that has already made the first order. This equals to 1598 customers from the test set. If one would choose the customers to target randomly, on average in this group would be 3.3% of customers that would do the second order anyway. Using the output of the churn prediction, this score would be 17%, which means a performance gain of 416%. One should bear in mind that targeting the customers that would place the second order anyway is useless. However, because the model is able to predict if the customer will buy second time with higher accuracy, it can be assumed that it can also correctly predict the customers that are likely to "almost buy".

TODO: tutaj przydałaby się argumentacja z discrete choice. Zakładamy że wynik modelu (albo rankingu) to latent variable, a decyzja czy klient kupuje jest bazowana na jakimś progu. Wtedy kiedy wiemy że model poprawnie przewiduje tych którzy kupili, to możemy zakładać że poprawnie przewiduje też tych którzy "prawie" kupili ??


```{r }
# preds2 %>%
#   filter(prob>quantile(prob, 0.99)) %>%
#   summarise(score=mean(if_second_order),
#             score_randomly = score_randomly,
#             lift_over_randomly = (score-score_randomly)/score_randomly
#   )

output_lifts <- data.frame()

for (quant in seq(0.99, 0.01, -0.01)){
  # print(1-quant)
  
  preds2 %>%
    filter(prob>quantile(prob, quant)) %>%
    summarise(
      fraction_of_customers = 1-quant,
      no_customers_in_bin = n(),
      prob_cutoff = min(prob),
      score=mean(if_second_order),
      score_randomly = score_randomly,
      performance_gain_over_randomly = (score-score_randomly)/score_randomly,
      lift_over_randomly = score/score_randomly
    ) -> tmp
  
  output_lifts <- rbind(output_lifts, tmp)
  
}
```

#### Table with selected quantiles

TODO: wykres i tabela pokazują prawie te same informacje, tabelę może dać do appendixa??

```{r }
quantiles_to_show <- c(0.99, 0.98, 0.97, 0.96, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5)
output_lifts %>% 
  filter(fraction_of_customers %in% (1-quantiles_to_show)) %>%
  mutate_if(is.numeric, round, 2) %>%
  flextable::flextable()
```

#### Lift curve

```{r }

# output_lifts %>%
#   ggplot(aes(x = fraction_of_customers, y= lift_over_randomly)) +
#   geom_line() +
#   geom_hline(yintercept = 1, color = 'red') +
#   labs(title = 'Lift curve') +
#   theme_minimal() 
# elbow method
  # geom_point(data = output_lifts %>%
  #                                filter(fraction_of_customers>0.08 & fraction_of_customers<0.089), 
  #                              
  #                              color = 'red', size = 3
  # ) +
  # ggrepel::geom_text_repel(data = output_lifts %>%
  #              filter(fraction_of_customers>0.08 & fraction_of_customers<0.089) %>%
  #              mutate(label = paste0('Lift:', round(lift_over_randomly,2))),aes(label=label))

quantiles_to_show2 <- c(0.99, 0.98, 0.97, 0.96, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.25, 0.01)

output_lifts %>%
  ggplot(aes(x = fraction_of_customers, y= lift_over_randomly)) +
  geom_line() +
  # geom_hline(yintercept = 1, color = 'red') +
  labs(title = 'Lift curve') +
  theme_minimal() +
  geom_point(data = output_lifts %>% 
               filter(fraction_of_customers %in% (1-quantiles_to_show2))
  ) +
  ggrepel::geom_text_repel(data =output_lifts %>% 
                             filter(fraction_of_customers %in% (1-quantiles_to_show2)), 
                           aes(label=sprintf('(%s: %s)', (1-quantiles_to_show2), round(lift_over_randomly,1)))) +
  scale_y_continuous(breaks = c(1,5,10,15,20),limits = c(1, NA)) 
  


```

On the plot .., lift curve is presented. On the x axis, the fraction of the *top* customers judging by probability to buy second time is presented. On the y axis, lift value for this quantile is shown. 
The shape of the plot resembles the one of the function 1/x. Values of lift are very big for the smallest percentage of the best customers to target, and they are getting smaller very quickly. This means that the more customers the company would like to target based on the model prediction, the less marginal effects it would get from the usage of the model. For example, for top 1% of the customers, the model can predict retention 18.7 times better than the random targeting approach. For top 5%, it is still very effective, being 4.2 times better. However, the improvement for half of the customers is only 0.3. Although this value is less impressive, it is still an improvement over random targeting. 

## Model explanations

### Introduction 

Despite superior performance in lots of machine learning tasks, the biggest drawback of Extreme Gradient Boosting models is its black-box nature, meaning lack of interpretability of predictions and parameters. This problem can be mitigated by usage of Explainable AI tools. One of such tools is ceteris-paribus analysis. The idea is to set all variables except the variable(s) of interest as constants, and create a new set of observations by manipulating only one or to variables. Such analysis can give a similar information to the values of coefficients in Logistic Regression models. 

(TODO: opisać trochę więcej)

#### Variable importances

```{r }
plot(varImp(models_list$basic_info$model))
```

On the plot .. the variables importances from XGB model for basic variables are presented. The best one is the value of the payment. Also, high importance scores are obtained by vanilla geolocation variables and the transportation cost for the value. Surprisingly, the review score of the purchase is not very important variable. Number of items bought has the smallest variable importance. One should remember that although the shown variable importance is equal to 0, this doesn't mean that the variable was not used in growing trees in XGB model. Rather, the variable importances are simply scaled to the range 0-100 on the plot.

```{r }
plot(varImp(models_list$product_categories$model))
```


```{r}
var_imp_cat <- varImp(models_list$product_categories$model, scale=F)

range01 <- function(x){(x-min(x))/(max(x)-min(x))}


var_imp_cat$importance %>%
  as_tibble(rownames = 'var') %>%
  mutate(var2 = ifelse(str_starts(var, 'prod_cat_'), 'prod_categories', var)) %>%
  group_by(var2) %>%
  summarise(Overall = sum(Overall)) %>%
  mutate(Overall = range01(Overall)*100) %>%
  as.data.frame() -> temp_df_imp


rownames(temp_df_imp) <- temp_df_imp$var2
temp_df_imp$var2 <- NULL
var_imp_cat$importance <- temp_df_imp
plot(var_imp_cat)

```

Second variable importance plot contains importances for the best model - with included product categories. Top 6 best predictors are the basic information, while the variables indicating product categories are the least important ones. Just from this information one could wonder, why despite product categories are relatively unimportant variables, they lead to 2.5% gain in AUC compared to the model without them. The reason is that all product categories variables in reality encode one variable. That is why variable importances of these variables should be treated jointly, which can be done simply by summing up the importances and scaling all importances to 0-100 range. This is presented on the plot .. . Now, product categories has become 5th most important variable, with importance bigger than review score. 


#### Cateris paribus plots

In my analysis I have decided to obtain the ceteris paribus profiles for mean values of each variable [@dalex]. Unfortunately this method is not able to meaningfully analyse dummy variables, that is why I have excluded the variables indicating product categories. Another variables requiring special treatment are the ones indicating geographic location. For them, I have created a 2-way ceteris paribus estimates and plotted them over the Brazil map. This way it is possible to see if it is correlated with presence of bigger cities or small population density (as in the Amazonia area).

```{r cache=T}
  explain <- DALEX::explain(model = models_list$product_categories$model,  
                          data = to_model_test %>% select(-if_second_order),
                          y = to_model_test$if_second_order == "yes", 
                          label = "Product categories",verbose = F)


to_model_test %>%
  select_if(is.numeric) %>%
  summarise_all(mean) -> mean_values

preds %>%
  filter(prob > quantile(prob, 0.9)) %>%
  select(-if_second_order, -prob) %>%
  summarise_all(mean) -> mean_values_top_decile

```

```{r }
# plot(varImp(models_list$product_categories$model))
predict_profile(explainer = explain, 
                new_observation = mean_values) -> profile_all

```

```{r }
plot(profile_all, variables = c("review_score")) + ggtitle("Ceteris-paribus profile", "") 
```

The shape of the ceteris paribus profile for the review score variable for the mean observations has an expected shape. The better the review, the it is probable for the client to purchase for the second time. What is also worth noticing is the fact that the probability increase is non-linear with respect to review score. The biggest probability increase is visible from the score 4 to 5. It is often the case in the customer reviews setting, observed also in the area of recommendation engines [(TODO: znaleźć lepsze cytowanie niż xkcd, z wykładu o recommenders)](https://xkcd.com/1098/). 

```{r }
ingredients::ceteris_paribus(explain,
                             new_observation = mean_values,variables = c('payment_value')) -> profile_payment

plot(profile_payment, variables = c("payment_value")) + ggtitle("Ceteris-paribus profile", "") +
  scale_x_continuous(n.breaks=10,limits = c(0,500)) + geom_smooth(alpha=0)

```

Model response for ceteris paribus profile is non-monotonous. To facilitate drawing conclusions I have included smoothing line used *loess* technique. From analysis of this smoothed model response one can say that the model response is increasing to the point of around 100. This value is a median payment value in the whole dataset. After this threshold the probability to buy second time is falling up to the level of around 175, and then rise a little bit again. 


```{r }
plot(profile_all, variables = c("no_items")) + ggtitle("Ceteris-paribus profile", "") +
  scale_x_continuous(breaks=1:20)

```

Number of items bought in the first purchase negatively influences the probability of the second purchase. This effect is visible only for the smallest numbers of items, while the model response for no_items>4 stays roughly the same. 

```{r }
# Predictions for geo data

predict_profile(explainer = explain, 
                new_observation = mean_values,variables = c('geolocation_lng', 'geolocation_lat')) -> profile_geo

profile_geo %>%
  as_tibble() %>%
  select(geolocation_lat) %>%
  distinct() %>%
  crossing(
    profile_geo %>% 
      select(geolocation_lng) %>%
      distinct()) %>%
  crossing(mean_values %>% select(-geolocation_lat, -geolocation_lng)) -> profile_geo

profile_geo$yhat <- predict(models_list$all_with_pca$model, profile_geo,type='prob')[,2]
```


```{r cache=F}
library(sf)
library(tmap)
library(readxl)
```


```{r }
# 2-d map ceteris paribus
profile_geo %>%
  select(geolocation_lat, geolocation_lng, yhat) %>%
  # sample_frac(0.7) %>%
  mutate(a = as.factor(as.numeric(cut(yhat, breaks=9)))) -> profile_geo_to_plot

# Bounding box
profile_geo_to_plot = profile_geo_to_plot[profile_geo_to_plot$geolocation_lat <= 5.27438888,]
profile_geo_to_plot = profile_geo_to_plot[profile_geo_to_plot$geolocation_lng >= -73.98283055,]
profile_geo_to_plot = profile_geo_to_plot[profile_geo_to_plot$geolocation_lat >= -33.75116944,]
profile_geo_to_plot = profile_geo_to_plot[profile_geo_to_plot$geolocation_lng <=  -34.79314722,]


# pal <- colorFactor(RColorBrewer::brewer.pal(9, 'Greens'), domain = levels(profile_geo_to_plot$a))
# 
# profile_geo_to_plot %>%
#   rename(lat = geolocation_lat, lng = geolocation_lng) %>%
#   leaflet() %>%  
#   addTiles() %>%
#   # addProviderTiles('Stamen.Toner') %>%
#   addCircleMarkers(
#     color = ~pal(a),
#     stroke = FALSE, fillOpacity = 0.5,
#     radius = 4
#   )


tmap_mode("plot")
brazil_map <- brazilmaps::get_brmap("Brazil")
brazil_map_city <- brazilmaps::get_brmap("City")
# 
df <- sf::st_as_sf(profile_geo_to_plot, coords = c(2,1))
st_crs(df) <- st_crs(brazil_map)


brasil_cities_coords <- read_excel(here("data/brasil_cities_coords.xlsx"))
brasil_cities_coords <- brasil_cities_coords %>%
  select(city, lat, lng, population)


brasil_cities_coords <- sf::st_as_sf(brasil_cities_coords, coords = c(3,2))
st_crs(brasil_cities_coords) <- st_crs(brazil_map)


tm_shape(brazil_map) +
  tm_polygons(col = "white") +
  tm_shape(df[which(lengths(st_within(df, brazil_map)) != 0), ]%>%rename(`Prediction quantile`=a)) +
  tm_symbols(size = 0.4,
             col = "Prediction quantile",
             palette = RColorBrewer::brewer.pal(9, 'Greens'),
             # style = "fixed",
             # breaks = c(45, 60, 75, 90),
             border.lwd = NA,
             alpha = 0.8) +
  tm_shape(brasil_cities_coords %>% arrange(-population) %>% head(20)) +
  tm_symbols(size = 0.4,
             col = "red",
             # style = "fixed",
             # breaks = c(45, 60, 75, 90),
             border.lwd = NA,
             alpha = 0.8) +
  tm_text(text='city', just='top')

```

In the case of geolocation data, I have created 2-d ceteris paribus estimates .. and visualized it on the map. To facilitate the analysis I have marked 10 most populated Brazilian cities. They are mostly located along the coast, with the exception of Brasilia (new capital of the country) and Manaus (the biggest city in Amazonia region). It can be seen that the predictions are the highest in two distinct large spots - one having center close to Brasilia, and the other one same latitude, but closer to the western country border. 

It can be seen that the predictions form a very visible pattern in stripes. As was noticed by @behrens2018spatial, it comes from the limitation of the model underlying XGBoost method, that is decision trees. The vanilla decision tree algorithm works by partitioning the feature space on a discrete basis, and a typical output of that model on 2-d space are visible rectangles. And as XGBoost is simply stacked decision trees, the resulting partition pattern is a bit more complex, but still decision-tree-typical artifacts are visible. 


#### Break down of average prediction 

(TODO: opisać bardziej ogólnie metodę we wstępie i po co robić, na tą chwilę też opisy wyników są trochę nieczytelne, do poprawy)

Variable Attributions via Sequential Variable Conditioning - algorithm [@dalex]:

For breakdown of one particular observation x_1:

1. Calculate prediction for all of the observations and calculate mean. This value is conceptually equivalent to intercept in Linear Regression
2. Change value of one variable in all observations and set it as the one from observation x_1
3. Calculate prediction and check how much it has changed from the value obtained from intercept
4. Repeat for the rest of the variables. 

Because the variables are set to a constant sequentially, the recommended algorithm is to perform a greedy search for the variable that once set constant results in the biggest change in prediction. This approach however does not facilitate comparisons between the plots as the order of the variables will change. That is why I have decided to have a fixed order of the variables, with variable importance as the main criterium. 

```{r cache=T}
varImp(models_list$product_categories$model)$importance %>%
  head(10) %>%
  rownames() -> most_important_vars



bd_rf <- predict_parts(explainer = explain,
                       new_observation = mean_values,
                       order = most_important_vars,
                       type = "break_down")
plot(bd_rf)
```

On the plot .. such break down additive plot is presented. The way to interpret it is as follows:

1. Obtain prediction for all of the observations. Calculate the mean prediction (*intercept*), in this case 0.156. 
2. For all observations in the dataset, set the value of the variable *payment_value* to 175. This is the value from the mean observation.
3. Obtain prediction for all of the observations. Calculate the mean prediction and substract it from the *intercept*. In this case the change is -0.016. This means that *payment_value* equal to the value from the mean observation lowers the average probability that the customer will buy second time.
4. Select next variable, (*geolocation_lat*) and repeat the process. In this case the predicted probability drops even further, to 0.101 (0.156-0.016-0.039)
5. Go through all variables. The last value on the plot, *prediction*, contains true prediction for the mean observation.

One interesting remark from this plot is that the low response of the model does not come from the fact that all variables influence it negatively. Rather, that for the variables that increase the response, there are other variables that lower the response for a similar amount and that the influences cancel out. 

The prediction for the average customer at the probability is very low (0.07). This is caused by the fact that there is big imbalance in the data, and in the test dataset used for analyzing the model results there is no imbalance correction applied. From the perspective of marketing targeting more important is gaining some knowledge not about an average customer, but the ones that are the most probable to buy next time. That is why I have also created a breakdown plot for the observation that is scored as the most probable to buy second time. 


```{r}
preds[c(models_list$product_categories$vars_used, 'prob')] %>%
  mutate(if_second_order = to_model_test$if_second_order) %>%
  filter(if_second_order == 'yes') %>%
  arrange(-prob) %>%
  head(1) %>%
  predict_parts(explainer = explain,
                new_observation = .,
                order = most_important_vars,
                type = "break_down") %>%
  plot() 

```

In the case of the prediction for the best customer, almost all variables influence the prediction positively. The only variable doing otherwise is payment value. The influence is however minor compared to the other variables. This negative influence can be analyzed in connection with ceteris-paribus plot for that variable analyzed before. On the plot ... it can be seen that the model response has a small drop for the values around 500. 

The rest of the variables influence the score positively. The biggest gain is from the value of the transport, however number of items equal to 6 has also a large influence. 


```{r }
# RANDOM

# preds[c(models_list$product_categories$vars_used, 'prob')] %>%
#   arrange(-prob) %>%
#   select(prob, everything()) %>%
#   distinct() %>%
#   head(10)
```

```{r }
# break
# preds2 %>%
#   arrange(-prob) %>%
#   mutate(bin = floor(row_number()/1000)) %>%
#   group_by(bin) %>%
#   summarise(perc_second = mean(if_second_order)) %>%
#   View()
# 
# 
# preds2 %>%
#   filter(prob>quantile(prob, 0.7)) %>%
#   summarise(no_people_second=mean(if_second_order),
#             perc_second = (mean(if_second_order-0.0344)/0.0344)
#   )
# # przy "wyślij do 30% klientów" o 65% lepsze wyniki
# 
# preds2 %>%
#   filter(prob>quantile(prob, 0.9)) %>%
#   summarise(no_people_second=mean(if_second_order),
#             perc_second = (mean(if_second_order-0.0344)/0.0344)
#   )
# # a przy "wyślij do 10% klientów" o 144% 
# 
# 
# cp2d <- ingredients::ceteris_paribus_2d(explainer = explain_topic,observation = mean_values,
#                                         variables = c('geolocation_lng', 'geolocation_lat'))
# 
# cp2d %>%
#   ggplot(aes(x=geolocation_lat, y = geolocation_lng, fill=y_hat)) +
#   geom_raster()
# 
# 
# fifa_mp_gbm_deep <- model_parts(explain)
# plot(fifa_mp_gbm_deep, max_vars = 20, 
#      bar_width = 4, show_boxplots = FALSE) 
# 
```

