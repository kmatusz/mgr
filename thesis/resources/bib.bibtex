@InProceedings{he2017aspect,
  author    = {He, Ruidan  and  Lee, Wee Sun  and  Ng, Hwee Tou  and  Dahlmeier, Daniel},
  title     = {An Unsupervised Neural Attention Model for Aspect Extraction},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={the Journal of machine Learning research},
  volume={3},
  pages={993--1022},
  year={2003},
  publisher={JMLR. org}
}


@inproceedings{10.1145/2623330.2623715,
author = {Yin, Jianhua and Wang, Jianyong},
title = {A Dirichlet Multinomial Mixture Model-Based Approach for Short Text Clustering},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623715},
doi = {10.1145/2623330.2623715},
abstract = {Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {233â€“242},
numpages = {10},
keywords = {dirichlet multinomial mixture, short text clustering, gibbs sampling},
location = {New York, New York, USA},
series = {KDD '14}
}


@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{chorowski2015attention,
  title={Attention-based models for speech recognition},
  author={Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1506.07503},
  year={2015}
}


@inproceedings{ijcai2019-712,
  title     = {Unsupervised Neural Aspect Extraction with Sememes},
  author    = {Luo, Ling and Ao, Xiang and Song, Yan and Li, Jinyao and Yang, Xiaopeng and He, Qing and Yu, Dong},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {5123--5129},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/712},
  url       = {https://doi.org/10.24963/ijcai.2019/712},
}

@inproceedings{tulkens-van-cranenburgh-2020-embarrassingly,
    title = "Embarrassingly Simple Unsupervised Aspect Extraction",
    author = "Tulkens, St{\'e}phan  and
      van Cranenburgh, Andreas",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.290",
    doi = "10.18653/v1/2020.acl-main.290",
    pages = "3182--3187",
    abstract = "We present a simple but effective method for aspect identification in sentiment analysis. Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages. We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable. Previous work relied on syntactic features and complex neural models. We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed. The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.",
}


@inproceedings{hong2010empirical,
  title={Empirical study of topic modeling in twitter},
  author={Hong, Liangjie and Davison, Brian D},
  booktitle={Proceedings of the first workshop on social media analytics},
  pages={80--88},
  year={2010}
}


@article{ganesh2000understanding,
  title={Understanding the customer base of service providers: an examination of the differences between switchers and stayers},
  author={Ganesh, Jaishankar and Arnold, Mark J and Reynolds, Kristy E},
  journal={Journal of marketing},
  volume={64},
  number={3},
  pages={65--87},
  year={2000},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}


@article{yu2011extended,
  title={An extended support vector machine forecasting framework for customer churn in e-commerce},
  author={Yu, Xiaobing and Guo, Shunsheng and Guo, Jun and Huang, Xiaorong},
  journal={Expert Systems with Applications},
  volume={38},
  number={3},
  pages={1425--1430},
  year={2011},
  publisher={Elsevier}
}



@ARTICLE{8627369,
  author={P. {Berger} and M. {Kompan}},
  journal={IEEE Intelligent Systems}, 
  title={User Modeling for Churn Prediction in E-Commerce}, 
  year={2019},
  volume={34},
  number={2},
  pages={44-52},
  doi={10.1109/MIS.2019.2895788}}


@inproceedings{zhao2005customer,
  title={Customer churn prediction using improved one-class support vector machine},
  author={Zhao, Yu and Li, Bing and Li, Xiu and Liu, Wenhuang and Ren, Shouju},
  booktitle={International Conference on Advanced Data Mining and Applications},
  pages={300--306},
  year={2005},
  organization={Springer}
}


@inproceedings{aleksandrova2018application,
  title={Application of Machine Learning for Churn Prediction Based on Transactional Data (RFM Analysis)},
  author={Aleksandrova, Yanka},
  booktitle={18 International Multidisciplinary Scientific Geoconference SGEM 2018: Conference Proceedings},
  volume={18},
  number={2.1},
  pages={125--132},
  year={2018}
}


@article{DECAIGNY20201563,
title = {Incorporating textual information in customer churn prediction models based on a convolutional neural network},
journal = {International Journal of Forecasting},
volume = {36},
number = {4},
pages = {1563-1578},
year = {2020},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301499},
author = {Arno {De Caigny} and Kristof Coussement and Koen W. {De Bock} and Stefan Lessmann},
keywords = {Customer relationship management, Text mining, Predictive modeling, Deep learning, Financial services industry},
abstract = {This study investigates the value added by incorporating textual data into customer churn prediction (CCP) models. It extends the previous literature by benchmarking convolutional neural networks (CNNs) against current best practices for analyzing textual data in CCP, and, using real life data from a European financial services provider, validates a framework that explains how textual data can be incorporated in a predictive model. First, the results confirm previous research showing that the inclusion of textual data in a CCP model improves its predictive performance. Second, CNNs outperform current best practices for text mining in CCP. Third, textual data are an important source of data for CCP, but unstructured textual data alone cannot create churn prediction models that are competitive with models that use traditional structured data. A calculation of the additional profit obtained from a customer retention campaign through the inclusion of textual information can be used by practitioners directly to help them make more informed decisions on whether to invest in text mining.}
}


@INPROCEEDINGS{8284914,
  author={Q. {Yanfang} and L. {Chen}},
  booktitle={2017 IEEE 2nd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)}, 
  title={Research on E-commerce user churn prediction based on logistic regression}, 
  year={2017},
  volume={},
  number={},
  pages={87-91},
  doi={10.1109/ITNEC.2017.8284914}}
  
@ARTICLE{1255389,
  author={ {Wai-Ho Au} and K. C. C. {Chan} and  {Xin Yao}},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A novel evolutionary data mining algorithm with applications to churn prediction}, 
  year={2003},
  volume={7},
  number={6},
  pages={532-545},
  doi={10.1109/TEVC.2003.819264}}

@INPROCEEDINGS{9325646,
  author={D. {Suryadi}},
  booktitle={2020 International Conference on Data Analytics for Business and Industry: Way Towards a Sustainable Economy (ICDABI)}, 
  title={Predicting Repurchase Intention Using Textual Features of Online Customer Reviews}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICDABI51230.2020.9325646}}



@inproceedings{jheng2019customer,
  title={Customer Retention Prediction with CNN},
  author={Jheng, Yang Ruei and Luo, Zhi Chao},
  booktitle={Data Mining and Big Data: 4th International Conference, DMBD 2019, Chiang Mai, Thailand, July 26--30, 2019, Proceedings},
  volume={1071},
  pages={104},
  year={2019},
  organization={Springer}
}



@Book{wordnet,
  title = {WordNet: An Electronic Lexical Database},
  author = {Christiane Fellbaum},
  year = {1998},
  publisher = {Bradford Books},
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@software{spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}


@book{van1995python, 
  title={Python tutorial}, 
  author={Van Rossum, Guido and Drake Jr, Fred L}, 
  year={1995}, 
  publisher={Centrum voor Wiskunde en Informatica Amsterdam, The Netherlands} 
}


@article{behrens2018spatial,
  title={Spatial modelling with Euclidean distance fields and machine learning},
  author={Behrens, Thorsten and Schmidt, Karsten and Viscarra Rossel, Raphael A and Gries, Philipp and Scholten, Thomas and MacMillan, Robert A},
  journal={European journal of soil science},
  volume={69},
  number={5},
  pages={757--770},
  year={2018},
  publisher={Wiley Online Library}
}
