{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I received it well before the deadline.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Congratulations lannister stores loved shoppin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>efficient device. on the website the brand of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>But a little, braking ... for the value ta Boa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Reliable seller, ok product and delivery befor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           message_en\n",
       "3             I received it well before the deadline.\n",
       "4   Congratulations lannister stores loved shoppin...\n",
       "9   efficient device. on the website the brand of ...\n",
       "12    But a little, braking ... for the value ta Boa.\n",
       "15  Reliable seller, ok product and delivery befor..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.read_csv('data/reviews_with_en.csv').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "df = df_full[['message_en']].dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.message_en = df.message_en.fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing - countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "a = df['message_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.int64'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Convert a collection of text documents to a matrix of token counts\n",
       "\n",
       "This implementation produces a sparse representation of the counts using\n",
       "scipy.sparse.csr_matrix.\n",
       "\n",
       "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
       "that does some kind of feature selection then the number of features will\n",
       "be equal to the vocabulary size found by analyzing the data.\n",
       "\n",
       "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "input : string {'filename', 'file', 'content'}, default='content'\n",
       "    If 'filename', the sequence passed as an argument to fit is\n",
       "    expected to be a list of filenames that need reading to fetch\n",
       "    the raw content to analyze.\n",
       "\n",
       "    If 'file', the sequence items must have a 'read' method (file-like\n",
       "    object) that is called to fetch the bytes in memory.\n",
       "\n",
       "    Otherwise the input is expected to be a sequence of items that\n",
       "    can be of type string or byte.\n",
       "\n",
       "encoding : string, default='utf-8'\n",
       "    If bytes or files are given to analyze, this encoding is used to\n",
       "    decode.\n",
       "\n",
       "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
       "    Instruction on what to do if a byte sequence is given to analyze that\n",
       "    contains characters not of the given `encoding`. By default, it is\n",
       "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
       "    values are 'ignore' and 'replace'.\n",
       "\n",
       "strip_accents : {'ascii', 'unicode'}, default=None\n",
       "    Remove accents and perform other character normalization\n",
       "    during the preprocessing step.\n",
       "    'ascii' is a fast method that only works on characters that have\n",
       "    an direct ASCII mapping.\n",
       "    'unicode' is a slightly slower method that works on any characters.\n",
       "    None (default) does nothing.\n",
       "\n",
       "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
       "    :func:`unicodedata.normalize`.\n",
       "\n",
       "lowercase : bool, default=True\n",
       "    Convert all characters to lowercase before tokenizing.\n",
       "\n",
       "preprocessor : callable, default=None\n",
       "    Override the preprocessing (string transformation) stage while\n",
       "    preserving the tokenizing and n-grams generation steps.\n",
       "    Only applies if ``analyzer is not callable``.\n",
       "\n",
       "tokenizer : callable, default=None\n",
       "    Override the string tokenization step while preserving the\n",
       "    preprocessing and n-grams generation steps.\n",
       "    Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "stop_words : string {'english'}, list, default=None\n",
       "    If 'english', a built-in stop word list for English is used.\n",
       "    There are several known issues with 'english' and you should\n",
       "    consider an alternative (see :ref:`stop_words`).\n",
       "\n",
       "    If a list, that list is assumed to contain stop words, all of which\n",
       "    will be removed from the resulting tokens.\n",
       "    Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "    If None, no stop words will be used. max_df can be set to a value\n",
       "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
       "    words based on intra corpus document frequency of terms.\n",
       "\n",
       "token_pattern : string\n",
       "    Regular expression denoting what constitutes a \"token\", only used\n",
       "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
       "    or more alphanumeric characters (punctuation is completely ignored\n",
       "    and always treated as a token separator).\n",
       "\n",
       "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
       "    The lower and upper boundary of the range of n-values for different\n",
       "    word n-grams or char n-grams to be extracted. All values of n such\n",
       "    such that min_n <= n <= max_n will be used. For example an\n",
       "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
       "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
       "    Only applies if ``analyzer is not callable``.\n",
       "\n",
       "analyzer : string, {'word', 'char', 'char_wb'} or callable,             default='word'\n",
       "    Whether the feature should be made of word n-gram or character\n",
       "    n-grams.\n",
       "    Option 'char_wb' creates character n-grams only from text inside\n",
       "    word boundaries; n-grams at the edges of words are padded with space.\n",
       "\n",
       "    If a callable is passed it is used to extract the sequence of features\n",
       "    out of the raw, unprocessed input.\n",
       "\n",
       "    .. versionchanged:: 0.21\n",
       "\n",
       "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
       "    first read from the file and then passed to the given callable\n",
       "    analyzer.\n",
       "\n",
       "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
       "    When building the vocabulary ignore terms that have a document\n",
       "    frequency strictly higher than the given threshold (corpus-specific\n",
       "    stop words).\n",
       "    If float, the parameter represents a proportion of documents, integer\n",
       "    absolute counts.\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "min_df : float in range [0.0, 1.0] or int, default=1\n",
       "    When building the vocabulary ignore terms that have a document\n",
       "    frequency strictly lower than the given threshold. This value is also\n",
       "    called cut-off in the literature.\n",
       "    If float, the parameter represents a proportion of documents, integer\n",
       "    absolute counts.\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "max_features : int, default=None\n",
       "    If not None, build a vocabulary that only consider the top\n",
       "    max_features ordered by term frequency across the corpus.\n",
       "\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "vocabulary : Mapping or iterable, default=None\n",
       "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
       "    indices in the feature matrix, or an iterable over terms. If not\n",
       "    given, a vocabulary is determined from the input documents. Indices\n",
       "    in the mapping should not be repeated and should not have any gap\n",
       "    between 0 and the largest index.\n",
       "\n",
       "binary : bool, default=False\n",
       "    If True, all non zero counts are set to 1. This is useful for discrete\n",
       "    probabilistic models that model binary events rather than integer\n",
       "    counts.\n",
       "\n",
       "dtype : type, default=np.int64\n",
       "    Type of the matrix returned by fit_transform() or transform().\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "vocabulary_ : dict\n",
       "    A mapping of terms to feature indices.\n",
       "\n",
       "fixed_vocabulary_: boolean\n",
       "    True if a fixed vocabulary of term to indices mapping\n",
       "    is provided by the user\n",
       "\n",
       "stop_words_ : set\n",
       "    Terms that were ignored because they either:\n",
       "\n",
       "      - occurred in too many documents (`max_df`)\n",
       "      - occurred in too few documents (`min_df`)\n",
       "      - were cut off by feature selection (`max_features`).\n",
       "\n",
       "    This is only available if no vocabulary was given.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
       ">>> corpus = [\n",
       "...     'This is the first document.',\n",
       "...     'This document is the second document.',\n",
       "...     'And this is the third one.',\n",
       "...     'Is this the first document?',\n",
       "... ]\n",
       ">>> vectorizer = CountVectorizer()\n",
       ">>> X = vectorizer.fit_transform(corpus)\n",
       ">>> print(vectorizer.get_feature_names())\n",
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
       ">>> print(X.toarray())\n",
       "[[0 1 1 1 0 0 1 0 1]\n",
       " [0 2 0 1 0 1 1 0 1]\n",
       " [1 0 0 1 1 0 1 1 1]\n",
       " [0 1 1 1 0 0 1 0 1]]\n",
       ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
       ">>> X2 = vectorizer2.fit_transform(corpus)\n",
       ">>> print(vectorizer2.get_feature_names())\n",
       "['and this', 'document is', 'first document', 'is the', 'is this',\n",
       "'second document', 'the first', 'the second', 'the third', 'third one',\n",
       " 'this document', 'this is', 'this the']\n",
       " >>> print(X2.toarray())\n",
       " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
       " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
       " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
       " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
       "\n",
       "See Also\n",
       "--------\n",
       "HashingVectorizer, TfidfVectorizer\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The ``stop_words_`` attribute can get large and increase the model size\n",
       "when pickling. This attribute is provided only for introspection and can\n",
       "be safely removed using delattr or set to None before pickling.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     TfidfVectorizer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(min_df=0.001, stop_words=stop_words)\n",
    "doc_term_matrix = count_vect.fit_transform(df['message_en'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=88)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_liked = LatentDirichletAllocation(n_components=5, random_state=88)\n",
    "LDA_liked.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['quality', 'time', 'well', 'delivered', 'deadline', 'recommend', 'arrived', 'great', 'good', 'product']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['much', 'came', 'arrived', 'ok', 'office', 'right', 'post', 'expected', 'product', 'everything']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['sent', 'return', 'two', 'delivered', '39', 'received', 'came', 'one', 'bought', 'product']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['still', 'delivered', 'days', 'time', 'receive', 'yet', 'delivery', 'received', 'product', '39']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['congratulations', 'always', 'satisfied', 'buy', 'recommend', 'time', 'super', 'product', 'fast', 'delivery']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,topic in enumerate(LDA_liked.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "# LDA is shit - nothing interesting here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Trying kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl = pd.DataFrame({'message_en':df['message_en'], 'cluster':km.predict(doc_term_matrix)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_en</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I&amp;#39;m completely in love, super responsible ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>The product did not arrive within the stipulat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>I bought the watch, unisex and sent a women&amp;#3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>I always shop at lannister. For me it&amp;#39;s th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>I bought the product on the 25th of February a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99522</th>\n",
       "      <td>I bought 3 units of this curtain, and only 1 a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99531</th>\n",
       "      <td>I haven&amp;#39;t received the blouse on delivery yet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99537</th>\n",
       "      <td>I&amp;#39;m waiting for delivery.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99546</th>\n",
       "      <td>The bracelet I bought is not cool, the bath wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99814</th>\n",
       "      <td>I didn&amp;#39;t receive the product</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4956 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              message_en  cluster\n",
       "38     I&#39;m completely in love, super responsible ...        1\n",
       "68     The product did not arrive within the stipulat...        1\n",
       "150    I bought the watch, unisex and sent a women&#3...        1\n",
       "151    I always shop at lannister. For me it&#39;s th...        1\n",
       "168    I bought the product on the 25th of February a...        1\n",
       "...                                                  ...      ...\n",
       "99522  I bought 3 units of this curtain, and only 1 a...        1\n",
       "99531  I haven&#39;t received the blouse on delivery yet        1\n",
       "99537                      I&#39;m waiting for delivery.        1\n",
       "99546  The bracelet I bought is not cool, the bath wa...        1\n",
       "99814                   I didn&#39;t receive the product        1\n",
       "\n",
       "[4956 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl.query('cluster ==1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biterm topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bigrams\n",
    "from biterm.utility import vec_to_biterms\n",
    "\n",
    "vocab = np.array(count_vect.get_feature_names())\n",
    "biterms = vec_to_biterms(doc_term_matrix[:1000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:57<00:00,  1.18s/it]\n",
      "<ipython-input-15-0a48d6f33ee1>:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  topics = btm.fit_transform(biterms, iterations=100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from biterm.cbtm import oBTM\n",
    "\n",
    "btm = oBTM(num_topics=3, V=vocab)\n",
    "topics = btm.fit_transform(biterms, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 9.78033498e-11, 9.78033498e-11],\n",
       "       [1.00000000e+00, 9.78033498e-11, 9.78033498e-11],\n",
       "       [1.00000000e+00, 9.78033498e-11, 9.78033498e-11],\n",
       "       ...,\n",
       "       [1.00000000e+00, 9.78033498e-11, 9.78033498e-11],\n",
       "       [1.00000000e+00, 9.78033498e-11, 9.78033498e-11],\n",
       "       [1.00000000e+00, 9.78033498e-11, 9.78033498e-11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 9.78033498e-11, 9.78033498e-11])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmin(topics, axis=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 9.78033498e-11, 9.78033498e-11])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmax(topics, axis=0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same predicted for each of the entries - bad model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie group process - Gibbs sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"bats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
      "The striped bat are hanging on their foot for best\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'feet'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "#> ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_one_sentence(sentence):\n",
    "    \n",
    "    a = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence.lower())]\n",
    "    return [i for i in a if i not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['strip', 'bat', 'hang', 'foot', 'best']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_one_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41751/41751 [01:30<00:00, 463.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "in_sent = list(df.message_en)\n",
    "out_sent = [None]*len(in_sent)\n",
    "for i in tqdm(range(len(out_sent))):\n",
    "    out_sent[i] = preprocess_one_sentence(in_sent[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(out_sent)):\n",
    "    out_sent[i] = [sent for sent in out_sent[i] if sent not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['congratulation',\n",
       " 'lannister',\n",
       " 'store',\n",
       " 'love',\n",
       " 'shopping',\n",
       " 'online',\n",
       " 'safe',\n",
       " 'practical',\n",
       " 'congratulation',\n",
       " 'happy',\n",
       " 'easter']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sent[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flatten = itertools.chain.from_iterable\n",
    "temp_vocab = list(flatten(out_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10505"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(temp_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_sent = list(df.message_en)#[:1000] \n",
    "out_sent = []\n",
    "temp_vocab_size= []\n",
    "\n",
    "for i in in_sent:\n",
    "    temp = []\n",
    "    # print(i)\n",
    "    doc = nlp(i)\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['VERB', 'ADJ', 'NOUN', 'ADV']:\n",
    "            # print(token.lemma_, token.pos_)\n",
    "            temp.append(token.lemma_)\n",
    "            temp_vocab_size.append(token.lemma_)\n",
    "\n",
    "        # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        #     token.shape_, token.is_alpha, token.is_stop)\n",
    "    out_sent.append(set(temp))\n",
    "    # print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(temp_vocab_size))\n",
    "out_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I received it well before the deadline.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Congratulations lannister stores loved shoppin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>efficient device. on the website the brand of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>But a little, braking ... for the value ta Boa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Reliable seller, ok product and delivery befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>Entregou dentro do prazo. O produto chegou em ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>O produto não foi enviado com NF, não existe v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>Excelente mochila, entrega super rápida. Super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>Solicitei a compra de uma capa de retrovisor c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>meu produto chegou e ja tenho que devolver, po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41751 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               message_en\n",
       "3                 I received it well before the deadline.\n",
       "4       Congratulations lannister stores loved shoppin...\n",
       "9       efficient device. on the website the brand of ...\n",
       "12        But a little, braking ... for the value ta Boa.\n",
       "15      Reliable seller, ok product and delivery befor...\n",
       "...                                                   ...\n",
       "99985   Entregou dentro do prazo. O produto chegou em ...\n",
       "99992   O produto não foi enviado com NF, não existe v...\n",
       "99998   Excelente mochila, entrega super rápida. Super...\n",
       "100000  Solicitei a compra de uma capa de retrovisor c...\n",
       "100001  meu produto chegou e ja tenho que devolver, po...\n",
       "\n",
       "[41751 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.message_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in_sent = list(df.message_en)#[:1000] \n",
    "# out_sent = []\n",
    "# temp_vocab_size= []\n",
    "\n",
    "# for i in in_sent:\n",
    "#     temp = []\n",
    "#     # print(i)\n",
    "#     doc = nlp(i)\n",
    "#     for token in doc:\n",
    "#         if token.pos_ in ['VERB', 'ADJ', 'NOUN', 'ADV']:\n",
    "#             # print(token.lemma_, token.pos_)\n",
    "#             temp.append(token.lemma_)\n",
    "#             temp_vocab_size.append(token.lemma_)\n",
    "\n",
    "#         # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#         #     token.shape_, token.is_alpha, token.is_stop)\n",
    "#     out_sent.append(set(temp))\n",
    "#     # print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm.gsdmm.mgp import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMovieGroupProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "A MovieGroupProcess is a conceptual model introduced by Yin and Wang 2014 to\n",
       "describe their Gibbs sampling algorithm for a Dirichlet Mixture Model for the\n",
       "clustering short text documents.\n",
       "Reference: http://dbgroup.cs.tsinghua.edu.cn/wangjy/papers/KDD14-GSDMM.pdf\n",
       "\n",
       "Imagine a professor is leading a film class. At the start of the class, the students\n",
       "are randomly assigned to K tables. Before class begins, the students make lists of\n",
       "their favorite films. The teacher reads the role n_iters times. When\n",
       "a student is called, the student must select a new table satisfying either:\n",
       "    1) The new table has more students than the current table.\n",
       "OR\n",
       "    2) The new table has students with similar lists of favorite movies.\n",
       "\n",
       ":param K: int\n",
       "    Upper bound on the number of possible clusters. Typically many fewer\n",
       ":param alpha: float between 0 and 1\n",
       "    Alpha controls the probability that a student will join a table that is currently empty\n",
       "    When alpha is 0, no one will join an empty table.\n",
       ":param beta: float between 0 and 1\n",
       "    Beta controls the student's affinity for other students with similar interests. A low beta means\n",
       "    that students desire to sit with students of similar interests. A high beta means they are less\n",
       "    concerned with affinity and are more influenced by the popularity of a table\n",
       ":param n_iters:\n",
       "\u001b[0;31mFile:\u001b[0m           /mnt/d/DS/mgr/gsdmm/gsdmm/mgp.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MovieGroupProcess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgp = MovieGroupProcess(K=30, alpha=0.1, beta=0.9, n_iters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 40082 clusters with 30 clusters populated\n",
      "In stage 1: transferred 30819 clusters with 30 clusters populated\n",
      "In stage 2: transferred 13890 clusters with 18 clusters populated\n",
      "In stage 3: transferred 10801 clusters with 7 clusters populated\n",
      "In stage 4: transferred 8700 clusters with 6 clusters populated\n",
      "In stage 5: transferred 6823 clusters with 7 clusters populated\n",
      "In stage 6: transferred 5738 clusters with 6 clusters populated\n",
      "In stage 7: transferred 5130 clusters with 6 clusters populated\n",
      "In stage 8: transferred 4522 clusters with 6 clusters populated\n",
      "In stage 9: transferred 4381 clusters with 7 clusters populated\n",
      "In stage 10: transferred 4358 clusters with 7 clusters populated\n",
      "In stage 11: transferred 4486 clusters with 7 clusters populated\n",
      "In stage 12: transferred 4501 clusters with 5 clusters populated\n",
      "In stage 13: transferred 4486 clusters with 6 clusters populated\n",
      "In stage 14: transferred 4441 clusters with 5 clusters populated\n",
      "In stage 15: transferred 4430 clusters with 5 clusters populated\n",
      "In stage 16: transferred 4495 clusters with 5 clusters populated\n",
      "In stage 17: transferred 4480 clusters with 4 clusters populated\n",
      "In stage 18: transferred 4521 clusters with 5 clusters populated\n",
      "In stage 19: transferred 4528 clusters with 5 clusters populated\n",
      "In stage 20: transferred 4452 clusters with 4 clusters populated\n",
      "In stage 21: transferred 4486 clusters with 4 clusters populated\n",
      "In stage 22: transferred 4517 clusters with 6 clusters populated\n",
      "In stage 23: transferred 4476 clusters with 4 clusters populated\n",
      "In stage 24: transferred 4567 clusters with 4 clusters populated\n",
      "In stage 25: transferred 4506 clusters with 5 clusters populated\n",
      "In stage 26: transferred 4429 clusters with 6 clusters populated\n",
      "In stage 27: transferred 4367 clusters with 5 clusters populated\n",
      "In stage 28: transferred 4337 clusters with 5 clusters populated\n",
      "In stage 29: transferred 4422 clusters with 5 clusters populated\n"
     ]
    }
   ],
   "source": [
    "y = mgp.fit(out_sent,vocab_size=len(set(temp_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tuple = list(zip(y, in_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "Very good. very fragrant.\n",
      "I hope it lasts because it is made of fur.\n",
      "Delivery ahead of time. Very practical and easy to handle product.\n",
      "Nor did the invoice\n",
      "the product arrived before the deadline, and I really liked it !! now my 3 year old boy will make the most of it !!\n",
      "The brushes came crumpled, not consistent with some photos, but consistent with the description. They are weak, do not fit in the case and a cool shaft\n",
      "I loved it! Small, easy to use, does not scratch, appeals and cleans the skin .. recommend\n",
      "N/a\n",
      "Product easy to assemble and perfectly serves the purpose to optimize space.\n",
      "\n",
      "Topic 1\n",
      "Congratulations lannister stores loved shopping online safe and practical Congratulations to all happy Easter\n",
      "I always shop at lannister. For me it&#39;s the best online store\n",
      "nothing to declare\n",
      "congratulations to store and dealer although it took a while\n",
      "Very good pre workout, one of the best on the market, if not the best.\n",
      "I recommend the shop I always buy at lannister.\n",
      "Good night the lannister stores are great I love the site today I make very little purchase for abusive shipping unfortunately does not pay as before but we need to buy thanks\n",
      "I always buy and always receive the right product on time or before the deadline\n",
      "I love shopping at lannister stores\n",
      "\n",
      "Topic 2\n",
      "I bought two units and only received one and now what do I do?\n",
      "Only one piece arrived, but the note and guarantee contain the two jewels.\n",
      "I bought two pendant chandeliers, with the targaryen partner and only one chandelier was sent to me. I filed a complaint, but I still haven&#39;t received a response. I await solution from Lojas lannister.\n",
      "here it is describing as delivered only that until now I have not received\n",
      "I bought two products and received only one. I sent two emails to the store and got no feedback. I am dissatisfied.\n",
      "Of the two products purchased, only one was delivered!\n",
      "receive only 01 piece of product! I need the remaining 2 pieces!\n",
      "Only one unit was delivered, instead of two.\n",
      "I bought 02 only sent 01\n",
      "\n",
      "Topic 3\n",
      "Reliable seller, ok product and delivery before the deadline.\n",
      "good\n",
      "Everything happened as contracted and the delivery was made before the deadline I am satisfied\n",
      "OK I RECOMMEND\n",
      "OK\n",
      "All right\n",
      "Delivered on time, with good validity period.\n",
      "A\n",
      "Right merchandise delivered on time. Thankful\n",
      "\n",
      "Topic 4\n",
      "I haven&#39;t had the opportunity to test it yet\n",
      "so far I haven&#39;t received the product.\n",
      "I received the product within the stated period, I haven&#39;t used it yet.\n",
      "So far so good I&#39;m waiting for the delivery of the product\n",
      "I haven&#39;t received it yet\n",
      "I would like to know when my product will arrive? Since the delivery date has passed, I would like an answer, I am waiting!\n",
      "I&#39;m not even able to ratrear :(\n",
      "I bought a product to be delivered by that store and I still haven&#39;t received it because the delivery date has passed\n",
      "I have not received the product\n",
      "\n",
      "Topic 5\n",
      "On-time delivery\n",
      "Super fast.\n",
      "Very fast delivery Congratulations\n",
      "Excellent quality product\n",
      "Very good high quality!\n",
      "Super fast delivery\n",
      "The product arrived very fast and of good quality I recommend the product\n",
      "I recommend\n",
      "Excellent product.\n",
      "\n",
      "Topic 6\n",
      "I received it well before the deadline.\n",
      "GREAT\n",
      "The product arrived very well\n",
      "Delivery well before the perfect deadline !!\n",
      "great arrived well before the deadline.\n",
      "To this day I am satisfied with sbmarino\n",
      "I recommend it to everyone. Great quality product, delivered without problems.\n",
      "Product arrived on time. It came as described and is excellent.\n",
      "It came well packaged, fast shipping!\n",
      "\n",
      "Topic 7\n",
      "I started using now\n",
      "very good and fast when i saw it had already arrived\n",
      "very good and fast and the product came in good condition now and even do the tests!\n",
      "Good product, good deadline ...\n",
      "The goods have not been delivered yet ..\n",
      "I haven&#39;t installed it yet but it arrived on time!\n",
      "Product arrived on time\n",
      "Very beautiful, looks like quality. I haven&#39;t applied it yet, so I can&#39;t talk about its adherence. For the cost benefit I recommend.\n",
      "Fantastic\n",
      "\n",
      "Topic 8\n",
      "I recommend ,\n",
      "great seller arrived before the deadline, I loved the product\n",
      "I recommend the seller ...\n",
      "Product delivered as requested, and very soon. Congratulations!\n",
      "Very good quality product, arrived before the promised deadline\n",
      "Very good\n",
      "It met my expectation.\n",
      "super recommend professional quality product\n",
      "quality product and efficient delivery. I received it long before the arrangement. I recommend\n",
      "\n",
      "Topic 9\n",
      "I WOULD LIKE TO KNOW WHAT HAS BEEN, I ALWAYS RECEIVED AND THIS PURCHASE NOW HAS DISCUSSED\n",
      "The purchase was made easily. The delivery was made well before the given deadline. The product has already started to be used and to date, without problems.\n",
      "I always buy over the Internet and delivery takes place before the agreed deadline, which I believe is the maximum period. At stark, the maximum term has expired and I have not yet received the product.\n",
      "I got exactly what I expected. Other orders from other sellers were delayed, but this one arrived on time.\n",
      "The product did not arrive within the stipulated time and caused inconvenience, because I scheduled my son&#39;s vacation trip, based on the deadline. I live in Bahia and he in Cuiabá alone. Now, the house is empty.\n",
      "Wonderful\n",
      "It took too long for delivery\n",
      "I liked the attention with the delivery\n",
      "I bought the product on the 25th of February and today the 29th of March it had not been delivered to my residence. I don&#39;t know if the post office in Brazil is terrible or it was the store itself that took a long time to post.\n",
      "\n",
      "Topic 10\n",
      "Store note 10\n",
      "STORE 10\n",
      "Store Note 10\n",
      "note 10\n",
      "Great at TDS aspects\n",
      "My grade is 10\n",
      "good\n",
      "Note 1000\n",
      "Thanks...\n",
      "\n",
      "Topic 11\n",
      "Good product, but what came to me does not match the photo in the ad.\n",
      "The store advertises one product and delivers another.\n",
      "I bought the product, paid at the boleto and only received half the product, they announced one thing and sent another. Very dissatisfied 😡😡😡\n",
      "The product came different from the one shown in the photo, it is not luxurious, nor quilted as I said.\n",
      "I did not receive the product.and now?\n",
      "supplier did not ship the correct product. I ordered the chocolate flavor and was sent another flavor.\n",
      "Product matches the ad\n",
      "I bought it in the lannister, because it is a trustworthy store, but when the product arrived I found that what was sold by the Targus brand was actually a counterfeit product,\n",
      "product delivered totally different from the one purchased\n",
      "\n",
      "Topic 12\n",
      "This was the order Bucket With 128 Pieces - Assembling Blocks 2 units - R $ 25.00 each (NOT DELIVERED) Sold and delivered targaryen Eva Rug Nº Letters 36 Pieces Children 1 unit - R $ 35.90 (THIS WAS DELIVERED)\n",
      "I bought three packs of five sheets each of transfer paper for dark tissue and received only two\n",
      "1 product was missing and the ones I received 1 came broken\n",
      "The two leather covers purchased were not delivered. Only one of them. I am waiting for the money back or the delivery of the second cover.\n",
      "I did not receive the game in tobacco color. Only red.\n",
      "I received only 1 box with 100 units, and ordered 2 boxes.\n",
      "*******************\n",
      "I made a purchase of two but both have the same code and only one came, I bought a bedspread that arrived just right but the skirt of the bed didn&#39;t come\n",
      "The cover is still missing\n",
      "\n",
      "Topic 13\n",
      "Very inferior product, badly finished.\n",
      "I really liked the bath game. The flower print, the embroidery, everything was very beautiful. Great product.\n",
      "One of the pieces did not fit, it was of a different diameter. I had to saw and hammer in order to fit and fit. Otherwise, everything is ok.\n",
      "product was delivered with one of the handles with problems (1 fixing pin is missing)\n",
      "the finish that was in the photo was beautiful the one that arrived was an ugly synthetic with an all smudged finish that I didn&#39;t like\n",
      "Product do not recommend not reading on any device\n",
      "The bag is very beautiful on the outside But inside it has no lining And it is very fragile\n",
      "I was happy with. The fabric, but I think the top sheet was missing to be complete, I hope they will soon be released.\n",
      "On May 19th I used it and the strap broke without much weight\n",
      "\n",
      "Topic 14\n",
      "I bought it one day and received it the next and the merchandise is excellent !!!\n",
      "I didn&#39;t receive another order from that store. I&#39;m waiting for them to send me the two 2kg proteins ...\n",
      "I received only 2 of the 3 units planned. Requeiro verification and adjustment of the situation.\n",
      "I received the order correctly, but it did not come with the tax note\n",
      "Overcame !!!\n",
      "Only one of the products I bought was delivered, with one more to be delivered and the tracking code says it has already been closed\n",
      "I bought a crystal ceiling light and two pendants, it appears in the email they sent me that was delivered, but the pendants I didn&#39;t receive, only the ceiling light.\n",
      "I received the essences, but the Hydrolyzed Collagen Kit is missing. On the website and by email he is saying that he was received together. Only I still await the collagens.\n",
      "Typically carioca. Received in 24 hours and delivered in 168 hours, a package with 330 grams.\n",
      "\n",
      "Topic 15\n",
      "The canine patrol backpack kit is beautiful !! My grandchild will love it !! Thanks!!\n",
      "I bought the watch, unisex and sent a women&#39;s watch, much smaller than the specifications of the ad.\n",
      "arrived on time and the product is of excellent quality! No more back pain, I&#39;m sleeping very well!\n",
      "I really liked the product, the delivery came before the deadline. I recommend it, it fits well in the cell phone, it does not take away the elegance of the device, like other covers that make it monstrous with its size.\n",
      "The product does not match what I bought, the film is much smaller than the cell phone screen and does not cover the entire screen.\n",
      "I did not receive the shower arm with diverter, only the 20x20 shower with a 30cm arm.\n",
      "Good\n",
      "I haven&#39;t received my sofa cover yet\n",
      "Today on the market we find low power router. And this device is very good and powerful\n",
      "\n",
      "Topic 16\n",
      "Did not like ! I bought a pig in a poke\n",
      "I received only 1 Midea Split STYLE control. Missed Remote Control for Air Conditioning Consul\n",
      "Buy a correct product on the cover but internal be another. ???????\n",
      "When buying the kit I understood that I would receive 2 bottles of 60 capsules (each) and not 2 bottles with 60 capsules in total. I just won&#39;t return it because I need the product.\n",
      "came as per the order plus the after sales did not answer any of my questions\n",
      "I received the product that is fake I want my money back\n",
      "The watch I bought was a replica of the original.\n",
      "I received the product with open box, unsealed, without invoice, without purchase order and with a different sender than the announcement. I want money back. Discontent\n",
      "Material inferior to that shown in the image.\n",
      "\n",
      "Topic 17\n",
      "If it were glass, it would have broken; it came in the box with no protection inside, the box was very dented. Sorry .. but I like to buy from you.\n",
      "great product not only came manual, but without problems.\n",
      "I received my product correctly and within the deadline, checking in the box everything is correct, but not yet assembled. Beautiful product, perfect color, we love it. SUPER RECOMMEND.\n",
      "The packaging left something to be desired, the product was almost undamaged, the box was all wrinkled.\n",
      "Missed cables and manual\n",
      "The product was only packed with brown paper on the packaging and sent by post. Of course, the packaging arrived crumpled and god knows if the product is working or not, I haven&#39;t tested it yet.\n",
      "It arrived crumpled ...\n",
      "Exactly what I needed\n",
      "I bought a chair, received it on time, but the chair came broken at the bottom. They could be more careful in the packaging.\n",
      "\n",
      "Topic 18\n",
      "efficient device. on the website the brand of the device is printed as 3desinfector and when it arrives with another name ... update with the correct brand since it is the same device\n",
      "Product arrived, but my PC was unable to recognize the USB ports.\n",
      "Does not work does not sync\n",
      "The play did not fit\n",
      "I placed an order for 4 bottles of olive oil. 2 arrived in one day and the other 2 almost 48 hours later. Contact with the store is very difficult, both via email and by phone. The suspense is over but I don’t buy anymore\n",
      "product failed in 1 time of use.\n",
      "Despite being delivered quickly, they replaced the METEORO box with the BORNE box. When I tried to complain on lannister the system was down. The guitar and accessories were correct.\n",
      "Good night is unfortunate, this store that I have already made so many purchases and today I am not satisfied even with a watch in the amount of 50.00 reais ... a defective product and so far has not given me back\n",
      "Beautiful ... Just missing instructions for exchanging bracelets.\n",
      "\n",
      "Topic 19\n",
      "super agil na entraga\n",
      "ta otima\n",
      "Com prei nau chego nuprazo\n",
      "lannister e otima\n",
      "TD DE BOM\n",
      "LINDA!!!\n",
      "Complimentable\n",
      "Cancelei\n",
      "Executioner\n",
      "\n",
      "Topic 20\n",
      "But a little, braking ... for the value ta Boa.\n",
      "Great store for partnership: very fast, well packaged and quality products! Only the cost of shipping that was a little sour.\n",
      "Pleased\n",
      "Very good! What I expected!\n",
      "The protective cover is not exactly what I expected leaves something to be desired in terms of material and quality.\n",
      "On the website the product seems to have better quality of material and finish.\n",
      "I didn&#39;t like the product, poor quality material.\n",
      "as for the site I have nothing to question! always super meet the agreed ... always recommend! but the product is weak ..\n",
      "Liked it!\n",
      "\n",
      "Topic 21\n",
      "FERÃO DO TIMÃO MUG, ONE MORE FOR THE COLLECTION\n",
      "I bought Iso X and received Nitro Hard\n",
      "I honestly bought a razor and a razor scissors kit from the list and it went wrong the product trace indicates that the scissors kit came and instead I come to the razor I wanted an explanation\n",
      "I PURCHASED 01 CONDITIONER AUSSIE MOIST + 01 SHAMPOO AUSSIE MOIST + THE 3 MINUTES OF TREATMENT AUSSIE STRONG. THE SEND ME THE AUSSIE MOIST TREATMENT WITHOUT KNOWING MY OPINION.\n",
      "FBFHJJKLI \\ ZXDAd\n",
      "The carpet did not arrive on schedule. and the size I ordered was wrong.\n",
      "It was bought for my brother-in-law to use it on a day-to-day basis. I know the brand ALCATEL and I think this device will be ideal.\n",
      "the box was half open and the guarantee date I don&#39;t know if the expiration date is very close 06/2018\n",
      "I received the product on the scheduled date. It is easy to assemble and very practical for hanging clothes and shoes. Only he can handle a small amount of clothes.\n",
      "\n",
      "Topic 22\n",
      "i bought a rubber mat for mitsubishi tr4 the store delivered a horrible pvc mat in the title ta rubber mat tr4, they deceived me and now what to do being that there is an expense for return\n",
      "Printing is coming out with weak ink ...\n",
      "I received the wrong product. I bought original and received parallel products. Recycled. Pickaxe\n",
      "I bought with the voltage 220 and came 110v\n",
      "My order arrived before the stipulated deadline, but the product came in disagreement with the order, instead of a black pg 145 cartridge and a black pg 146 color cartridge, two black pg 145 came. And now\n",
      "The product arrived on time and intact, but does not work. I put the car vacuum with plug for the cigarette lighter in the device already plugged in and nothing, does not work. Although 12v\n",
      "Unfortunately the cartridge did not work on the canon mg3510 printer\n",
      "My bucket bag came from the Ford ranger instead of the frontier\n",
      "Good afternoon, I bought product for 110V, but came 220V. I would like to exchange the same.\n",
      "\n",
      "Topic 23\n",
      "disappointed\n",
      "Delay in delivery, I hated the service and NEVER buy more. baratheon when selling by themselves was much better!\n",
      ", too bad !\n",
      "Targaryen is not to be trusted did not deliver my purchase and placed it in the order tracking that was delivered on 12/14/17 fake company I want to receive my products that were paid with boleto bancari\n",
      "I need a forecast.\n",
      "I think there should be a way to communicate with the company if the order happened not to be delivered\n",
      "I am extremely disappointed. This had never happened\n",
      "Shameless shop. They did not deliver the order and they had the impudence that I canceled it. Crap seller. stark chose his mistress very badly.\n",
      "You are leaving something to be desired I bought products with you and you delay delivery and still cancel the order without authorization\n",
      "\n",
      "Topic 24\n",
      "Only one product was missing, I received it today, thank you very much! All right! Att Elenice.\n",
      "Great value for money\n",
      "I received it very quickly, but another product came, and not the one I ordered, the model came different, the color, came a cape that has no zipper, anyway, I don&#39;t recommend it.\n",
      "I bought the product in black and arrived in red.\n",
      "Sdds \n",
      "I am waiting for an urgent return.\n",
      "The curtain is not the model I asked for. |\n",
      "I bought Rosa with White Arms and deliver all Black?\n",
      "My request for number 03-492551892 came in parts. a Manchester Polo Match was missing. That each day shows that it is in one place and I enter the site, it appears that it has already been delivered!\n",
      "\n",
      "Topic 25\n",
      "very beautiful and cheap watch.\n",
      "Very beautiful\n",
      "I loved it, beautiful, very delicate\n",
      "The thermal bag besides being beautiful is super spacious. Product as advertised and super fast delivery, delivered well before the deadline.\n",
      "super fast delivery .... arrived before the date ...\n",
      "Beautiful watch I loved it\n",
      "Beautiful product, delivered well worth and on time. I recommend!\n",
      "The set of towels is beautiful more beautiful than in the photo, I loved it.\n",
      "Wonderful cream and super fast delivery\n",
      "\n",
      "Topic 26\n",
      "thank you for your kind attention\n",
      "very good\n",
      "I&#39;m completely in love, super responsible and reliable store!\n",
      "Smooth and efficient purchasing process.\n",
      "Fulfilled what was promised in the purchase and with a satisfactory delivery time\n",
      "congratulations station ... always arrives with a lot of antecedence .. Thank you very much ....\n",
      "Recommend to all customers\n",
      "Liked it\n",
      "Delivered on time, I was happy with the purchase I made\n",
      "\n",
      "Topic 27\n",
      "Terrible\n",
      "Nothing to get my order.\n",
      "I asked for a refund and no response so far\n",
      "The delivery was split in two. There was no statement from the store. I came to think that they had only shipped part of the product.\n",
      "I hope to receive this week, which would not be so extravagant, however ... more delay than that would make me give up baratheon.\n",
      "I can not! I am waiting for the arrival of the product I bought! As soon as that happens, I will have my opinion formed!\n",
      "They canceled my purchase the day before delivery, I called lannister and the attendants didn&#39;t know what was going on, a mess, now I&#39;m waiting to see if they&#39;ll be charged\n",
      "I sent 2 requests through the site to change the address. I got the message that they were going to call me in 48h but they didn&#39;t.\n",
      "The days are past.\n",
      "\n",
      "Topic 28\n",
      "I DID NOT RECEIVE THE PRODUCT AND IS IN THE SYSTEM I RECEIVED BEYOND PAYING EXPENSIVE SHIPPING\n",
      "my product did not arrive ... according to the screening is stopped in Ribeirão Preto sp ...\n",
      "The store posted the product and the mail took a while\n",
      "Post office did not deliver\n",
      "It wasn&#39;t delivered to my house, so I didn&#39;t like it, I don&#39;t have time to go to the post office.\n",
      "The zip code was misspelled, I had to pick up the product at the post office in the neighboring city.\n",
      "Hello only problem was that we did not receive the product at the requested address, we had to go after the product at the post office in the city center, and we paid even more to receive it at home.\n",
      "The order was not delivered to my address, I paid up front and had to go to the distribution center to pick up my order. Lack of respect to pay to receive at home and not receive.\n",
      "The product was sent to the destination city on 03/31 and so far I have not received it.\n",
      "\n",
      "Topic 29\n",
      "And very cluttered I bought three products but it didn&#39;t come in the backlands it came separately and on a separate date\n",
      "Excelent reception\n",
      "great purchase but freight does not pay, weighs in the budget. should have free grates\n",
      "I just think I could have bought more cartridges, but I didn&#39;t do them because each cartridge came with a different freight, something that could be placed in a single package, which would make shipping cheaper.\n",
      "I bought 2 (two) identical products and had to pay 2 (two) freight. The Company could have put it in a single package and charge only one freight. Product delivered in perfect condition. Great product.\n",
      "Only one cover came, I bought 3 then I paid. More than 100 reais for a cover\n",
      "When buying 11 dumpsters, I had to pay the freight individually, as if I had bought only 1 dumpster. The stark could have assembled 1 bigger box with the 11 bins inside which would be cheaper\n",
      "THE PRODUCT HAS BEEN DELIVERED IN A WRONG LOCATION. HOWEVER I GOT TO RECEIVE, WAITING FOR THE WASHING MACHINE IN THE CORRECT ADDRESS: RUA GENERAL IVAN RAPOSO, 44 AP 101- BARRA DA TIJUCA, RIO DE JANEIRO\n",
      "I just don&#39;t give 5 stars because the freight was too expensive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for topic_no in set(y):\n",
    "    print(f'Topic {topic_no}')\n",
    "    counter = 1\n",
    "    for i in out_tuple:\n",
    "        if i[0]==topic_no and counter < 10:\n",
    "            print(i[1])\n",
    "            counter += 1\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# frequency per topic \n",
    "# frequency overall\n",
    "\n",
    "out_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "count_vect2 = CountVectorizer(min_df=0.001, stop_words=stop_words)\n",
    "doc_term_matrix2 = count_vect2.fit_transform([' '.join(i) for i in out_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'a':doc_term_matrix2.todense().sum(axis=0).transpose()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect2.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frequencies = pd.DataFrame({'word':count_vect2.get_feature_names(), 'occurence_total':doc_term_matrix2.todense().sum(axis=0).tolist()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frequencies.sort_values('occurence_total',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(y):\n",
    "    print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_no = 0\n",
    "\n",
    "indexes_for_topic = []\n",
    "for idx, i in enumerate(y):\n",
    "    if i == topic_no:\n",
    "        indexes_for_topic.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frequencies_topic = pd.DataFrame({'word':count_vect2.get_feature_names(), 'occurence_total':doc_term_matrix2[indexes_for_topic,:].todense().sum(axis=0).tolist()[0]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for topic_no in set(y):\n",
    "    indexes_for_topic = []\n",
    "\n",
    "    for idx, i in enumerate(y):\n",
    "        if i == topic_no:\n",
    "            indexes_for_topic.append(idx)\n",
    "\n",
    "\n",
    "    frequencies_topic = pd.DataFrame({'word':count_vect2.get_feature_names(), 'occurence_total':doc_term_matrix2[indexes_for_topic,:].todense().sum(axis=0).tolist()[0]})\n",
    "    frequencies['occurences_topic_'+str(topic_no)] = doc_term_matrix2[indexes_for_topic,:].todense().sum(axis=0).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc(x):\n",
    "    return x/sum(x)\n",
    "\n",
    "frequencies2 = frequencies.select_dtypes(include=['int64']).apply(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies2['word'] = frequencies.word\n",
    "frequencies2['occurence_total_num'] = frequencies.occurence_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies3=frequencies2.melt(value_vars=frequencies2.select_dtypes(['float']).columns, id_vars = 'word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "words_to_show = list(frequencies.sort_values('occurence_total',ascending=False).head(20).word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies4=frequencies3[frequencies3.word.isin(words_to_show)]#.query('word in !words_to_show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=frequencies4, kind=\"bar\",\n",
    "    y=\"word\", x=\"value\", hue=\"variable\",\n",
    "    palette=\"dark\", alpha=.6, height=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    data=frequencies4, kind=\"bar\",\n",
    "    y=\"word\", x=\"value\", hue=\"variable\",\n",
    "    palette=\"dark\", alpha=.6, height=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "count_vect_small = CountVectorizer(min_df=0.01, stop_words=stop_words)\n",
    "doc_term_matrix_small = count_vect_small.fit_transform(df['message_en'].values)\n",
    "doc_term_matrix_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PCA(n_components=2)\n",
    "p.fit(doc_term_matrix_small.todense())\n",
    "p_data = p.transform(doc_term_matrix_small.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_data[:,0], p_data[:,1], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "out = pd.DataFrame({\"message_en\":df['message_en'], \n",
    "'x1':p_data[:,0], 'x2':p_data[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out.to_csv('review_reduction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Very basic EDA of words in the corpus\n",
    "df_occurences = df.copy()\n",
    "df['message_en']\n",
    "df['message_en'].str.contains('deadline')\n",
    "# df['message_en'].str.contains('delivery')\n",
    "# df['message_en'].str.contains('product')\n",
    "\n",
    "# list(df['message_en'].str.split(' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['message_en'][100:120])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = en_nlp(df['message_en'][110])\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occurences.assign(a = lambda x: x['message_en'].str.contains('deadline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find subjects of sentences\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
    "\n",
    "print(sub_toks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = nlp(list(df.message_en)[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in list(df.message_en)[:30]:\n",
    "    print(i)\n",
    "    doc = nlp(i)\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['VERB', 'ADJ', 'NOUN', 'ADV']:\n",
    "            print(token.lemma_, token.pos_)\n",
    "        # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        #     token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "    print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
