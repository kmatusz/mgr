---
title: "Methods description"
output: html_document
bibliography: bib.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Methods - order items

For each purchase of the customer there is avaliable information about the items that particular customer bought. All items in the shop are assigned to one of .. categories. There is a couple of challenges that should be adressed before adding this information to the dataset:

1. For each purchase, the customer can buy products from different categories
2. Most of the purchases (..%) contain only one item - this means that one-hot-encoding matrix would be extremely sparse
3. Some of the categories are underrepresented - there are ... categories for which only ... purchases were made. This can pose generalization problems.
4. The number of categories is very big, and because of that model training would be extremely slow

To mitigate the problem of too many categories, I have decided to bin the least popular ones as a distinct category "other". I have chosen 13 (??) most popular ones, which account for .. of all observations. Then, I have used one-hot-encoding approach to create numeric representation. Category "other" was set as a base level. 

### Methods - geographic data

### Methods - demographic data

This part of the dataset is comprised of ... variables. Because all of these 

### Methods - text reviews

Olist e-commerce store is operating only in Brazil. That is why most of the reviews are written in Portuguese. I have used Google Translate API to change the language of them to English. This is to facilitate not only understanding the reviews, but also the NLP tools avaliable for english language are more up-to-date and advanced.

3 algorithms for topic modeling that I have tested were:

- Latent Dirchlet Allocation - because it is a go-to standard for topic recognition
- Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture - as this method is an improvement over LDA, meant especially for short texts. This is the case from this study, as most of the reviews are just couple of words long
- Attention-Based Aspect Extraction - this method is also meant for short texts, and at the same time it uses the most modern, state-of-the-art NLP techniques.

To improve the results in all 3 methods I have removed stopwords and punctuation from the text. Also, to limit the number of words in the vocabulary I have used lemmatization technique. This was done by WordNet lemmatizer. To further improve the results, I have used Part-of-Speech tagger, and passed the tags of words to the lemmatizer. This way the algorithm can change the form of the word on a more informed basis, and thus apply lemmatization to more words. 

Second step of the preprocessing was converting lemmatized reviews into vector format. In case of LDA, count-vectorizing approach was applied. the words which appeared in less than 0.1% of reviews were dropped from the dataset. In the case of Gibbs Sampling the same preprocessing is done internally by the function. 

In both of these cases after vectorization one should obtain a matrix with n rows and k columns, where n is number of observations in the original dataset, while k - size of the vocabulary. 

Very different preprocessing was required in the case of Attention-Based Aspect Extraction. The neural network requires simply lemmatized reviews in textual format as the output. Then, one of the layers of the network is meant to embed the currently preprocessed word. These embeddings are not learnt during the network training, they should be trained beforehand instead. The authors of the paper propose Word2vec technique for learning embeddings. Following their guidelines I have used this method, setting the dimensionality of the vector space to 200. I have also applied the word window of 10. 

After applying word2vec on this dataset, I have obtained the matrix with m rows and 200 columns, where m stands for number of words in the dataset, and 200 is the dimensionality of the vector space chosen as a hyperparameter. 
One should bear in mind that count-vectorization works on review level, while word2vec - on words level. This means that after applying word2vec model to 1 review, one would obtain the same number of outputs as the number of words in the review. This is why it is impossible to use word2vec to preprocess the dataset and then use LDA or Gibbs sampling without some way to convert couple of vectors into one. 

Unfortunately, the evaluation of topic extraction is a hard task. The only reasonable one is human inspection. That is why after running every model I have verified the obtained topic for coherency and distinctiveness.

TODO: Opis wyboru parametrów modeli z poniższej listy

- LDA
  - remove stopwords and punctuation
  - use wordnet lemmatizer [@wordnet] with POS tagging (NLTK)
  - use count-vectorize approach with removing words which appear in less than 0.1% reviews
  - use LDA with varying number of components 3, 5, 10, 15
  - manually inspect inferred topics
  
- Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture (movie group process)
  - same as in LDA but this one does not require vectorizing the dataset
  - 30 topics
  - Citing python gsdmm package documentation:
  
    "Alpha controls the probability that a student will join a table that is currently empty.
    When alpha is 0, no one will join an empty table.
    Beta controls the student's affinity for other students with similar interests. A low beta means
    that students desire to sit with students of similar interests. A high beta means they are less
    concerned with affinity and are more influenced by the popularity of a table"
  - checking every combination of alpha, beta chosen from the values: [0.01, 0.1, 0.5, 0.9]
  - manual inspection
  
- Attention-Based Aspect Extraction [@he2017aspect]
  - Based on the implementation by authors of the article 
  - remove stopwords and punctuation
  - use wordnet lemmatizer [@wordnet]
  - converting to vectorized form using Word2Vect model with final dimensionality of 200 and window of 10
  - Training attention model
  - manual assessment of reviews topics
  
For the analysis of textual reviews I have used python programming language. 

- python [@van1995python]
  - NLTK [@bird2009natural], spacy [@spacy] itd
  
  
  
```{r}
library(tidyverse)
library(DT)
library(stringr)

DT::datatable(read_csv('topics_table_to_paper.csv') %>% mutate(`Example reviews` = str_replace(`Example reviews`, '\\n', '||||')))

```

